"text_id","text"
"0","Autonomous resource provisioning for multi-service web applications Dynamic resource provisioning aims at maintaining the end-to-end response time of a web application within a pre-defined SLA . Although the topic has been well studied for monolithic applications , provisioning resources for applications composed of multiple services remains a challenge . When the SLA is violated , one must decide which service ( s ) should be reprovisioned for optimal effect . We propose to assign an SLA only to the front-end service . Other services are not given any particular response time objectives . Services are autonomously responsible for their own provisioning operations and collaboratively negotiate performance objectives with each other to decide the provisioning service ( s ) . We demonstrate through extensive experiments that our system can add\/remove\/shift both servers and caches within an entire multi-service application under varying workloads to meet the SLA target and improve resource utilization ."
"1","Collaborative filtering for orkut communities : discovery of user latent behavior Users of social networking services can connect with each other by forming communities for online interaction . Yet as the number of communities hosted by such websites grows over time , users have even greater need for effective community recommendations in order to meet more users . In this paper , we investigate two algorithms from very different domains and evaluate their effectiveness for personalized community recommendation . First is association rule mining ( ARM ) , which discovers associations between sets of communities that are shared across many users . Second is latent Dirichlet allocation ( LDA ) , which models user-community co-occurrences using latent aspects . In comparing LDA with ARM , we are interested in discovering whether modeling low-rank latent structure is more effective for recommendations than directly mining rules from the observed data . We experiment on an Orkut data set consisting of 492,104 users and 118,002 communities . Our empirical comparisons using the top-k recommendations metric show that LDA performs consistently better than ARM for the community recommendation task when recommending a list of 4 or more communities . However , for recommendation lists of up to 3 communities , ARM is still a bit better . We analyze examples of the latent information learned by LDA to explain this finding . To efficiently handle the large-scale data set , we parallelize LDA on distributed computers and demonstrate our parallel implementation 's scalability with varying numbers of machines ."
"2","A trust management framework for service-oriented environments Many reputation management systems have been developed under the assumption that each entity in the system will use a variant of the same scoring function . Much of the previous work in reputation management has focused on providing robustness and improving performance for a given reputation scheme . In this paper , we present a reputation-based trust management framework that supports the synthesis of trust-related feedback from many different entities while also providing each entity with the flexibility to apply different scoring functions over the same feedback data for customized trust evaluations . We also propose a novel scheme to cache trust values based on recent client activity . To evaluate our approach , we implemented our trust management service and tested it on a realistic application scenario in both LAN and WAN distributed environments . Our results indicate that our trust management service can effectively support multiple scoring functions with low overhead and high availability ."
"3","A client-server architecture for state-dependent dynamic visualizations on the web As sophisticated enterprise applications move to the Web , some advanced user experiences become difficult to migrate due to prohibitively high computation , memory , and bandwidth requirements . State-dependent visualizations of large-scale data sets are particularly difficult since a change in the client 's context necessitates a change in the displayed results . This paper describes a Web architecture where clients are served a session-specific image of the data , with this image divided into tiles dynamically generated by the server . This set of tiles is supplemented with a corpus of metadata describing the immediate vicinity of interest ; additional metadata is delivered as needed in a progressive fashion in support and anticipation of the user 's actions . We discuss how the design of this architecture was motivated by the goal of delivering a highly responsive user experience . As an example of a complete application built upon this architecture , we present OrgMaps , an interactive system for navigating hierarchical data , enabling fluid , low-latency navigation of trees of hundreds of thousands of nodes on standard Web browsers using only HTML and JavaScript ."
"4","Rapid development of spreadsheet-based web mashups The rapid growth of social networking sites and web communities have motivated web sites to expose their APIs to external developers who create mashups by assembling existing functionalities . Current APIs , however , aim toward developers with programming expertise ; they are not directly usable by wider class of users who do not have programming background , but would nevertheless like to build their own mashups . To address this need , we propose a spreadsheet-based Web mashups development framework , which enables users to develop mashups in the popular spreadsheet environment . First , we provide a mechanism that makes structured data first class values of spreadsheet cells . Second , we propose a new component model that can be used to develop fairly sophisticated mashups , involving joining data sources and keeping spreadsheet data up to date . Third , to simplify mashup development , we provide a collection of spreadsheet-based mashup patterns that captures common Web data access and spreadsheet presentation functionalities . Users can reuse and customize these patterns to build spreadsheet-based Web mashups instead of developing them from scratch . Fourth , we enable users to manipulate structured data presented on spreadsheet in a drag-and-drop fashion . Finally , we have developed and tested a proof-of-concept prototype to demonstrate the utility of the proposed framework ."
"5","Information diffusion through blogspace We study the dynamics of information propagation in environments of low-overhead personal publishing , using a large collection of weblogs over time as our example domain . We characterize and model this collection at two levels . First , we present a macroscopic characterization of topic propagation through our corpus , formalizing the notion of long-running `` chatter '' topics consisting recursively of `` spike '' topics generated by outside world events , or more rarely , by resonances within the community . Second , we present a microscopic characterization of propagation from individual to individual , drawing on the theory of infectious diseases to model the flow . We propose , validate , and employ an algorithm to induce the underlying propagation network from a sequence of posts , and report on the results ."
"6","BizCQ : using continual queries to cope with changes in business information exchange In this poster , we propose the framework of BizCQ , a system to apply Continual Queries ( 7 ) ( 8 ) on Web-based content to manage information exchanges between two business partners . In this poster , we describe ways to leverage previous research in Web monitoring techniques applied to the everyday problem of managing change within a business environment , and focus on the difficulties of managing changes that are caused by external parties in business-to-business ( B2B ) information exchanges ."
"7","Combining global optimization with local selection for efficient QoS-aware service composition The run-time binding of web services has been recently put forward in order to support rapid and dynamic web service compositions . With the growing number of alternative web services that provide the same functionality but differ in quality parameters , the service composition becomes a decision problem on which component services should be selected such that user 's end-to-end QoS requirements ( e.g. availability , response time ) and preferences ( e.g. price ) are satisfied . Although very efficient , local selection strategy fails short in handling global QoS requirements . Solutions based on global optimization , on the other hand , can handle global constraints , but their poor performance renders them inappropriate for applications with dynamic and real-time requirements . In this paper we address this problem and propose a solution that combines global optimization with local selection techniques to benefit from the advantages of both worlds . The proposed solution consists of two steps : first , we use mixed integer programming ( MIP ) to find the optimal decomposition of global QoS constraints into local constraints . Second , we use distributed local selection to find the best web services that satisfy these local constraints . The results of experimental evaluation indicate that our approach significantly outperforms existing solutions in terms of computation time while achieving close-to-optimal results ."
"8","Advertising keyword generation using active learning This paper proposes an efficient relevance feedback based interactive model for keyword generation in sponsored search advertising . We formulate the ranking of relevant terms as a supervised learning problem and suggest new terms for the seed by leveraging user relevance feedback information . Active learning is employed to select the most informative samples from a set of candidate terms for user labeling . Experiments show our approach improves the relevance of generated terms significantly with little user effort required ."
"9","Small world peer networks in distributed web search In ongoing research , a collaborative peer network application is being proposed to address the scalability limitations of centralized search engines . Here we introduce a local adaptive routing algorithm used to dynamically change the topology of the peer network based on a simple learning scheme driven by query response interactions among neighbors . We test the algorithm via simulations with 70 model users based on actual Web crawls . We find that the network topology rapidly converges from a random network to a small world network , with emerging clusters that match the user communities with shared interests ."
"10","Simulation , verification and automated composition of web services Web services -- Web-accessible programs and devices - are a key application area for the Semantic Web . With the proliferation of Web services and the evolution towards the Semantic Web comes the opportunity to automate various Web services tasks . Our objective is to enable markup and automated reasoning technology to describe , simulate , compose , test , and verify compositions of Web services . We take as our starting point the DAML-S DAML+OIL ontology for describing the capabilities of Web services . We define the semantics for a relevant subset of DAML-S in terms of a first-order logical language . With the semantics in hand , we encode our service descriptions in a Petri Net formalism and provide decision procedures for Web service simulation , verification and composition . We also provide an analysis of the complexity of these tasks under different restrictions to the DAML-S composite services we can describe . Finally , we present an implementation of our analysis techniques . This implementation takes as input a DAML-S description of a Web service , automatically generates a Petri Net and performs the desired analysis . Such a tool has broad applicability both as a back end to existing manual Web service composition tools , and as a stand-alone tool for Web service developers ."
"11","The interoperability of learning object repositories and services : standards , implementations and lessons learned Interoperability is one of the main issues in creating a networked system of repositories . The eduSource project in its holisticapproach to building a network of learning object repositories in Canada is implementing an open network for learning services . Itsopenness is supported by a communication protocol called theeduSource Communications Layer ( ECL ) which closely implements the IMS Digital Repository Interoperability ( DRI ) specification and architecture . The ECL in conjunction withconnection middleware enables any service providers to join thenetwork . EduSource is open to external initiatives as it explicitlysupports an extensible bridging mechanism between eduSource and other major initiatives . This paper discusses interoperability in general and then focuses on the design of ECL as animplementation of IMS DRI with supporting infrastructure andmiddleware . The eduSource implementation is in the mature stateof its development as being deployed in different settings withdifferent partners . Two applications used in evaluating ourapproach are described : a gateway for connecting betweeneduSource and the NSDL initiative , and a federated searchconnecting eduSource , EdNA and SMETE ."
"12","Social recommender systems The goal of this tutorial is to expose participants to the current research on social recommender systems ( i.e. , recommender systems for the social web ) . Participants will become familiar with state-of-the-art recommendation methods , their classifications according to various criteria , common evaluation methodologies , and potential applications that can utilize social recommender systems . Additionally , open issues and challenges in the field will be discussed ."
"13","A search-based method for forecasting ad impression in contextual advertising Contextual advertising ( also called content match ) refers to the placement of small textual ads within the content of a generic web page . It has become a significant source of revenue for publishers ranging from individual bloggers to major newspapers . At the same time it is an important way for advertisers to reach their intended audience . This reach depends on the total number of exposures of the ad ( impressions ) and its click-through-rate ( CTR ) that can be viewed as the probability of an end-user clicking on the ad when shown . These two orthogonal , critical factors are both difficult to estimate and even individually can still be very informative and useful in planning and budgeting advertising campaigns . In this paper , we address the problem of forecasting the number of impressions for new or changed ads in the system . Producing such forecasts , even within large margins of error , is quite challenging : 1 ) ad selection in contextual advertising is a complicated process based on tens or even hundreds of page and ad features ; 2 ) the publishers ' content and traffic vary over time ; and 3 ) the scale of the problem is daunting : over a course of a week it involves billions of impressions , hundreds of millions of distinct pages , hundreds of millions of ads , and varying bids of other competing advertisers . We tackle these complexities by simulating the presence of a given ad with its associated bid over weeks of historical data . We obtain an impression estimate by counting how many times the ad would have been displayed if it were in the system over that period of time . We estimate this count by an efficient two-level search algorithm over the distinct pages in the data set . Experimental results show that our approach can accurately forecast the expected number of impressions of contextual ads in real time . We also show how this method can be used in tools for bid selection and ad evaluation ."
"14","Latent space domain transfer between high dimensional overlapping distributions Transferring knowledge from one domain to another is challenging due to a number of reasons . Since both conditional and marginal distribution of the training data and test data are non-identical , model trained in one domain , when directly applied to a different domain , is usually low in accuracy . For many applications with large feature sets , such as text document , sequence data , medical data , image data of different resolutions , etc. two domains usually do not contain exactly the same features , thus introducing large numbers of `` missing values '' when considered over the union of features from both domains . In other words , its marginal distributions are at most overlapping . In the same time , these problems are usually high dimensional , such as , several thousands of features . Thus , the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model . To address these challenges , we propose a framework that first brings the marginal distributions of two domains closer by `` filling up '' those missing values of disjoint features . Afterwards , it looks for those comparable sub-structures in the `` latent-space '' as mapped from the expanded feature vector , where both marginal and conditional distribution are similar . With these sub-structures in latent space , the proposed approach then find common concepts that are transferable across domains with high probability . During prediction , unlabeled instances are treated as `` queries '' , the mostly related labeled instances from out-domain are retrieved , and the classification is made by weighted voting using retrieved out-domain examples . We formally show that importing feature values across domains and latent semantic index can jointly make the distributions of two related domains easier to measure than in original feature space , the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in-domain examples . Software and datasets are available for download ."
"15","Investigating behavioral variability in web search Understanding the extent to which people 's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively . In this paper we describe a longitudinal log-based study that investigated variability in people . s interaction behavior when engaged in search-related activities on the Web . allWe analyze the search interactions of more than two thousand volunteer users over a five-month period , with the aim of characterizing differences in their interaction styles . allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users , and within and between the search queries they submit . allOur findings also suggest two classes of extreme user . navigators and explorers . whose search interaction is highly consistent or highly variable . Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone ."
"16","Efficient application placement in a dynamic hosting platform Web hosting providers are increasingly looking into dynamic hosting to reduce costs and improve the performance of their platforms . Instead of provisioning fixed resources to each customer , dynamic hosting maintains a variable number of application instances to satisfy current demand . While existing research in this area has mostly focused on the algorithms that decide on the number and location of application instances , we address the problem of efficient enactment of these decisions once they are made . We propose a new approach to application placement and experimentally show that it dramatically reduces the cost of application placement , which in turn improves the end-to-end agility of the hosting platform in reacting to demand changes ."
"17","Query-driven indexing for peer-to-peer text retrieval We describe a query-driven indexing framework for scalable text retrieval over structured P2P networks . To cope with the bandwidth consumption problem that has been identified as the major obstacle for full-text retrieval in P2P networks , we truncate posting lists associated with indexing features to a constant size storing only top-k ranked document references . To compensate for the loss of information caused by the truncation , we extend the set of indexing features with carefully chosen term sets . Indexing term sets are selected based on the query statistics extracted from query logs , thus we index only such combinations that are a ) frequently present in user queries and b ) non-redundant w.r.t the rest of the index . The distributed index is compact and efficient as it constantly evolves adapting to the current query popularity distribution . Moreover , it is possible to control the tradeoff between the storage\/bandwidth requirements and the quality of query answering by tuning the indexing parameters . Our theoretical analysis and experimental results indicate that we can indeed achieve scalable P2P text retrieval for very large document collections and deliver good retrieval performance ."
"18","Making RDF presentable : integrated global and local semantic Web browsing This paper discusses generating document structure from annotated media repositories in a domain-independent manner . This approaches the vision of a universal RDF browser . We start by applying the search-and-browse paradigm established for the WWW to RDF presentation . Furthermore , this paper adds to this paradigm the clustering-based derivation of document structure from search returns , providing simple but domain-independent hypermedia generation from RDF stores . While such generated presentations hardly meet the standards of those written by humans , they provide quick access to media repositories when the required document has not yet been written . The resulting system allows a user to specify a topic for which it generates a hypermedia document providing guided navigation through virtually any RDF repository . The impact for content providers is that as soon as one adds new media items and their annotations to a repository , they become immediately available for automatic integration into subsequently requested presentations ."
"19","Network-aware forward caching This paper proposes and evaluates a Network Aware Forward Caching approach for determining the optimal deployment strategy of forward caches to a network . A key advantage of this approach is that we can reduce the network costs associated with forward caching to maximize the benefit obtained from their deployment . We show in our simulation that a 37 % increase to net benefits could be achieved over the standard method of full cache deployment to cache all POPs traffic . In addition , we show that this maximal point occurs when only 68 % of the total traffic is cached . Another contribution of this paper is the analysis we use to motivate and evaluate this problem . We characterize the Internet traffic of 100K subscribers of a US residential broadband provider . We use both layer 4 and layer 7 analysis to investigate the traffic volumes of the flows as well as study the general characteristics of the applications used . We show that HTTP is a dominant protocol and account for 68 % of the total downstream traffic and that 34 % of that traffic is multimedia . In addition , we show that multimedia content using HTTP exhibits a 83 % annualized growth rate and other HTTP traffic has a 53 % growth rate versus the 26 % over all annual growth rate of broadband traffic . This shows that HTTP traffic will become ever more dominent and increase the potential caching opportunities . Furthermore , we characterize the core backbone traffic of this broadband provider to measure the distance traveled by content and traffic . We find that CDN traffic is much more efficient than P2P content and that there is large skew in the Air Miles between POP in a typical network . Our findings show that there are many opportunties in broadband provider networks to optimize how traffic is delivered and cached ."
"20","Dynamic assembly of learning objects This paper describes one solution to the problem of how to select sequence , and link Web resources into a coherent , focused organization for instruction that addresses a user 's immediate and focused learning need . A system is described that automatically generates individualized learning paths from a repository of XML Web resources . Each Web resource has an XML Learning Object Metadata ( LOM ) description consisting of General , Educational , and Classification metadata . Dynamic assembly of these learning objects is based on the relative match of the learning object content and metadata to the learner 's needs , preferences , context , and constraints . Learning objects are connected into coherent paths based on their LOM topic classifications and the proximity of these topics in a Resource Description Framework ( RDF ) graph . An instructional sequencing policy specifies how to arrange the objects on the path into a particular learning sequence . The system has been deployed and evaluated within a corporate setting ."
"21","Learning consensus opinion : mining data from a labeling game We consider the problem of identifying the consensus ranking for the results of a query , given preferences among those results from a set of individual users . Once consensus rankings are identified for a set of queries , these rankings can serve for both evaluation and training of retrieval and learning systems . We present a novel approach to collecting the individual user preferences over image-search results : we use a collaborative game in which players are rewarded for agreeing on which image result is best for a query . Our approach is distinct from other labeling games because we are able to elicit directly the preferences of interest with respect to image queries extracted from query logs . As a source of relevance judgments , this data provides a useful complement to click data . Furthermore , the data is free of positional biases and is collected by the game without the risk of frustrating users with non-relevant results ; this risk is prevalent in standard mechanisms for debiasing clicks . We describe data collected over 34 days from a deployed version of this game that amounts to about 18 million expressed preferences between pairs . Finally , we present several approaches to modeling this data in order to extract the consensus rankings from the preferences and better sort the search results for targeted queries ."
"22","Incorporating site-level knowledge to extract structured data from web forums Web forums have become an important data resource for many web applications , but extracting structured data from unstructured web forum pages is still a challenging task due to both complex page layout designs and unrestricted user created posts . In this paper , we study the problem of structured data extraction from various web forum sites . Our target is to find a solution as general as possible to extract structured data , such as post title , post author , post time , and post content from any forum site . In contrast to most existing information extraction methods , which only leverage the knowledge inside an individual page , we incorporate both page-level and site-level knowledge and employ Markov logic networks ( MLNs ) to effectively integrate all useful evidence by learning their importance automatically . Site-level knowledge includes ( 1 ) the linkages among different object pages , such as list pages and post pages , and ( 2 ) the interrelationships of pages belonging to the same object . The experimental results on 20 forums show a very encouraging information extraction performance , and demonstrate the ability of the proposed approach on various forums . We also show that the performance is limited if only page-level knowledge is used , while when incorporating the site-level knowledge both precision and recall can be significantly improved ."
"23","Towards context-aware search by learning a very large variable length hidden markov model from search logs Capturing the context of a user 's query from the previous queries and clicks in the same session may help understand the user 's information need . A context-aware approach to document re-ranking , query suggestion , and URL recommendation may improve users ' search experience substantially . In this paper , we propose a general approach to context-aware search . To capture contexts of queries , we learn a variable length Hidden Markov Model ( vlHMM ) from search sessions extracted from log data . Although the mathematical model is intuitive , how to learn a large vlHMM with millions of states from hundreds of millions of search sessions poses a grand challenge . We develop a strategy for parameter initialization in vlHMM learning which can greatly reduce the number of parameters to be estimated in practice . We also devise a method for distributed vlHMM learning under the map-reduce model . We test our approach on a real data set consisting of 1.8 billion queries , 2.6 billion clicks , and 840 million search sessions , and evaluate the effectiveness of the vlHMM learned from the real data on three search applications : document re-ranking , query suggestion , and URL recommendation . The experimental results show that our approach is both effective and efficient ."
"24","Object views : fine-grained sharing in browsers Browsers do not currently support the secure sharing of JavaScript objects between principals . We present this problem as the need for object views , which are consistent and controllable versions of objects . Multiple views can be made for the same object and customized for the recipients . We implement object views with a JavaScript library that wraps shared objects and interposes on all access attempts . The security challenge is to fully mediate access to objects shared through a view and prevent privilege escalation . We discuss how object views can be deployed in two settings : same-origin sharing with rewriting-based JavaScript isolation systems like Google Caja , and inter-origin sharing between browser frames over a message-passing channel . To facilitate simple document sharing , we build a policy system for declaratively defining policies for document object views . Notably , our document policy system makes it possible to hide elements without breaking document structure invariants . Developers can control the fine-grained behavior of object views with an aspect system that accepts programmatic policies ."
"25","Searching with numbers A large fraction of the useful web comprises of specification documents that largely consist of hattribute name , numeric valuei pairs embedded in text . Examples include product information , classified advertisements , resumes , etc. . The approach taken in the past to search these documents by first establishing correspondences between values and their names has achieved limited success because of the difficulty of extracting this information from free text . We propose a new approach that does not require this correspondence to be accurately established . Provided the data has `` low reflectivity '' , we can do effective search even if the values in the data have not been assigned attribute names and the user has omitted attribute names in the query . We give algorithms and indexing structures for implementing the search . We also show how hints ( i. e , imprecise , partial correspondences ) from automatic data extraction techniques can be incorporated into our approach for better accuracy on high reflectivity datasets . Finally , we validate our approach by showing that we get high precision in our answers on real datasets from a variety of domains ."
"26","Automatic web service composition with abstraction and refinement The behavioral description based Web Service Composition ( WSC ) problem aims at the automatic construction of a coordinator web service that controls a set of web services to reach a goal state . However , solving the WSC problem exactly with a realistic model is doubly-exponential in the number of variables in web service descriptions . In this paper , we propose a novel efficient approximation-based algorithm using automatic abstraction and refinement to dramatically reduce the number of variables needed to solve the problem ."
"27","A fault model and mutation testing of access control policies To increase confidence in the correctness of specified policies , policy developers can conduct policy testing by supplying typical test inputs ( requests ) and subsequently checking test outputs ( responses ) against expected ones . Unfortunately , manual testing is tedious and few tools exist for automated testing of access control policies . We present a fault model for access control policies and a framework to explore it . The framework includes mutation operators used to implement the fault model , mutant generation , equivalent-mutant detection , and mutant-killing determination . This framework allows us to investigate our fault model , evaluate coverage criteria for test generation and selection , and determine a relationship between structural coverage and fault-detection effectiveness . We have implemented the framework and applied it to various policies written in XACML . Our experimental results offer valuable insights into choosing mutation operators in mutation testing and choosing coverage criteria in test generation and selection ."
"28","Preserving XML queries during schema evolution In XML databases , new schema versions may be released as frequently as once every two weeks . This poster describes a taxonomy of changes for XML schema evolution . It examines the impact of those changes on schema validation and query evaluation . Based on that study , it proposes guidelines for XML schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas ."
"29","Open user profiles for adaptive news systems : help or harm ? Over the last five years , a range of projects have focused on progressively more elaborated techniques for adaptive news delivery . However , the adaptation process in these systems has become more complicated and thus less transparent to the users . In this paper , we concentrate on the application of open user models in adding transparency and controllability to adaptive news systems . We present a personalized news system , YourNews , which allows users to view and edit their interest profiles , and report a user study on the system . Our results confirm that users prefer transparency and control in their systems , and generate more trust to such systems . However , similar to previous studies , our study demonstrate that this ability to edit user profiles may also harm the system . s performance and has to be used with caution ."
"30","Homepage live : automatic block tracing for web personalization The emergence of personalized homepage services , e.g. personalized Google Homepage and Microsoft Windows Live , has enabled Web users to select Web contents of interest and to aggregate them in a single Web page . The web contents are often predefined content blocks provided by the service providers . However , it involves intensive manual efforts to define the content blocks and maintain the information in it . In this paper , we propose a novel personalized homepage system , called . Homepage Live. , to allow end users to use drag-and-drop actions to collect their favorite Web content blocks from existing Web pages and organize them in a single page . Moreover , Homepage Live automatically traces the changes of blocks with the evolvement of the container pages by measuring the tree edit distance of the selected blocks . By exploiting the immutable elements of Web pages , the tracing algorithm performance is significantly improved . The experimental results demonstrate the effectiveness and efficiency of our algorithm ."
"31","SRing : a structured non dht p2p overlay supporting string range queries This paper presents SRing , a structured non DHT P2P overlay that efficiently supports exact and range queries on multiple attribute values . In SRing , all attribute values are interpreted as strings formed by a base alphabet and are published in the lexicographical order . Two virtual rings are built : N-ring is built in a skip-list way for range partition and queries ; D-ring is built in a small-world way for the construction of N-ring . A leave-and-join based load balancing method is used to balance range overload in the network with heterogeneous nodes ."
"32","Acquiring ontological knowledge from query logs We present a method for acquiring ontological knowledge using search query logs . We first use query logs to identify important contexts associated with terms belonging to a semantic category ; we then use these contexts to harvest new words belonging to this category . Our evaluation on selected categories indicates that the method works very well to help harvesting terms , achieving 85 % to 95 % accuracy in categorizing newly acquired terms ."
"33","Sync kit : a persistent client-side database caching toolkit for data intensive websites We introduce a client-server toolkit called Sync Kit that demonstrates how client-side database storage can improve the performance of data intensive websites . Sync Kit is designed to make use of the embedded relational database defined in the upcoming HTML5 standard to offload some data storage and processing from a web server onto the web browsers to which it serves content . Our toolkit provides various strategies for synchronizing relational database tables between the browser and the web server , along with a client-side template library so that portions web applications may be executed client-side . Unlike prior work in this area , Sync Kit persists both templates and data in the browser across web sessions , increasing the number of concurrent connections a server can handle by up to a factor of four versus that of a traditional server-only web stack and a factor of three versus a recent template caching approach ."
"34","Social search and discovery using a unified approach We explore new ways of improving a search engine using data from Web 2.0 applications such as blogs and social bookmarks . This data contains entities such as documents , people and tags , and relationships between them . We propose a simple yet effective method , based on faceted search , that treats all entities in a unified manner : returning all of them ( documents , people and tags ) on every search , and allowing all of them to be used as search terms . We describe an implementation of such a social search engine on the intranet of a large enterprise , and present large-scale experiments which verify the validity of our approach ."
"35","Where to adapt dynamic service compositions Peer services depend on one another to accomplish their tasks , and their structures may evolve . A service composition may be designed to replace its member services whenever the quality of the composite service fails to meet certain quality-of-service ( QoS ) requirements . Finding services and service invocation endpoints having the greatest impact on the quality are important to guide subsequent service adaptations . This paper proposes a technique that samples the QoS of composite services and continually analyzes them to identify artifacts for service adaptation . The preliminary results show that our technique has the potential to effectively find such artifacts in services ."
"36","Constructing travel itineraries from tagged geo-temporal breadcrumbs Vacation planning is a frequent laborious task which requires skilled interaction with a multitude of resources . This paper develops an end-to-end approach for constructing intra-city travel itineraries automatically by tapping a latent source reflecting geo-temporal breadcrumbs left by millions of tourists . In particular , the popular rich media sharing site , Flickr , allows photos to be stamped by the date and time of when they were taken , and be mapped to Points Of Interest ( POIs ) by latitude-longitude information as well as semantic metadata ( e.g. , tags ) that describe them . Our extensive user study on a `` crowd-sourcing '' marketplace ( Amazon Mechanical Turk ) , indicates that high quality itineraries can be automatically constructed from Flickr data , when compared against popular professionally generated bus tours ."
"37","Deducing trip related information from flickr Uploading tourist photos is a popular activity on photo sharing platforms . These photographs and their associated metadata ( tags , geo-tags , and temporal information ) should be useful for mining information about the sites visited . However , user-supplied metadata are often noisy and efficient filtering methods are needed before extracting useful knowledge . We focus here on exploiting temporal information , associated with tourist sites that appear in Flickr . From automatically filtered sets of geo-tagged photos , we deduce answers to questions like `` how long does it take to visit a tourist attraction ? '' or `` what can I visit in one day in this city ? '' Our method is evaluated and validated by comparing the automatically obtained visit duration times to manual estimations ."
"38","Visualizing differences in web search algorithms using the expected weighted hoeffding distance We introduce a new dissimilarity function for ranked lists , the expected weighted Hoeffding distance , that has several advantages over current dissimilarity measures for ranked search results . First , it is easily customized for users who pay varying degrees of attention to websites at different ranks . Second , unlike existing measures such as generalized Kendall 's tau , it is based on a true metric , preserving meaningful embeddings when visualization techniques like multi-dimensional scaling are applied . Third , our measure can effectively handle partial or missing rank information while retaining a probabilistic interpretation . Finally , the measure can be made computationally tractable and we give a highly efficient algorithm for computing it . We then apply our new metric with multi-dimensional scaling to visualize and explore relationships between the result sets from different search engines , showing how the weighted Hoeffding distance can distinguish important differences in search engine behavior that are not apparent with other rank-distance metrics . Such visualizations are highly effective at summarizing and analyzing insights on which search engines to use , what search strategies users can employ , and how search results evolve over time . We demonstrate our techniques using a collection of popular search engines , a representative set of queries , and frequently used query manipulation methods ."
"39","Dtwiki : a disconnection and intermittency tolerant wiki Wikis have proven to be a valuable tool for collaboration and content generation on the web . Simple semantics and ease-of-use make wiki systems well suited for meeting many emerging region needs in the areas of education , collaboration and local content generation . Despite their usefulness , current wiki software does not work well in the network environments found in emerging regions . For example , it is common to have long-lasting network partitions due to cost , power and poor connectivity . Network partitions make a traditional centralized wiki architecture unusable due to the unavailability of the central server . Existing solutions towards addressing connectivity problems include web-caching proxies and snapshot distribution . While proxies and snapshots allow wiki data to be read while disconnected , they prevent users from contributing updates back to the wiki . In this paper we detail the design and implementation of DTWiki , a wiki system which explicitly addresses the problem of operating a wiki system in an intermittent environment . The DTWiki system is able to cope with long-lasting partitions and bad connectivity while providing the functionality of popular wiki software such as MediaWiki and TWiki ."
"40","Externalities in online advertising Most models for online advertising assume that an advertiser 's value from winning an ad auction , which depends on the clickthrough rate or conversion rate of the advertisement , is independent of other advertisements served alongside it in the same session . This ignores an important ` externality effect ' : as the advertising audience has a limited attention span , a high-quality ad on a page can detract attention from other ads on the same page . That is , the utility to a winner in such an auction also depends on the set of other winners . In this paper , we introduce the problem of modeling externalities in online advertising , and study the winner determination problem in these models . Our models are based on choice models on the audience side . We show that in the most general case , the winner determination problem is hard even to approximate . However , we give an approximation algorithm for this problem with an approximation factor that is logarithmic in the ratio of the maximum to the minimum bid . Furthermore , we show that there are some interesting special cases , such as the case where the audience preferences are single peaked , where the problem can be solved exactly in polynomial time . For all these algorithms , we prove that the winner determination algorithm can be combined with VCG-style payments to yield truthful mechanisms ."
"41","Impact of search engines on page popularity Recent studies show that a majority of Web page accesses are referred by search engines . In this paper we study the widespread use of Web search engines and its impact on the ecology of the Web . In particular , we study how much impact search engines have on the popularity evolution of Web pages . For example , given that search engines return currently popular '' pages at the top of search results , are we somehow penalizing newly created pages that are not very well known yet ? Are popular pages getting even more popular and new pages completely ignored ? We first show that this unfortunate trend indeed exists on the Web through an experimental study based on real Web data . We then analytically estimate how much longer it takes for a new page to attract a large number of Web users when search engines return only popular pages at the top of search results . Our result shows that search engines can have an immensely worrisome impact on the discovery of new Web pages ."
"42","A machine learning based approach for table detection on the web Table is a commonly used presentation scheme , especially for describing relational information . However , table understanding remains an open problem . In this paper , we consider the problem of table detection in web documents . Its potential applications include web mining , knowledge management , and web content summarization and delivery to narrow-bandwidth devices . We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine . Various features reflecting the layout as well as content characteristics of tables are studied . In order to facilitate the training and evaluation of our table classifier , we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database . The database consists of 1,393 HTML files collected from hundreds of different web sites and contains 11,477 leaf TABLE elements , out of which 1,740 are genuine tables . Experiments were conducted using the cross validation method and an F-measure of 95.89 % was achieved ."
"43","Detecting Wikipedia vandalism with active learning and statistical language models This paper proposes an active learning approach using language model statistics to detect Wikipedia vandalism . Wikipedia is a popular and influential collaborative information system . The collaborative nature of authoring , as well as the high visibility of its content , have exposed Wikipedia articles to vandalism . Vandalism is defined as malicious editing intended to compromise the integrity of the content of articles . Extensive manual efforts are being made to combat vandalism and an automated approach to alleviate the laborious process is needed . This paper builds statistical language models , constructing distributions of words from the revision history of Wikipedia articles . As vandalism often involves the use of unexpected words to draw attention , the fitness ( or lack thereof ) of a new edit when compared with language models built from previous versions may well indicate that an edit is a vandalism instance . In addition , the paper adopts an active learning model to solve the problem of noisy and incomplete labeling of Wikipedia vandalism . The Wikipedia domain with its revision histories offers a novel context in which to explore the potential of language models in characterizing author intention . As the experimental results presented in the paper demonstrate , these models hold promise for vandalism detection ."
"44","sMash : semantic-based mashup navigation for data API network With the proliferation of data APIs , it is not uncommon that users who have no clear ideas about data APIs will encounter difficulties to build Mashups to satisfy their requirements . In this paper , we present a semantic-based mashup navigation system , sMash that makes mashup building easy by constructing and visualizing a real-life data API network . We build a sample network by gathering more than 300 popular APIs and find that the relationships between them are so complex that our system will play an important role in navigating users and give them inspiration to build interesting mashups easily . The system is accessible at : http:\/\/www.dart.zju.edu.cn\/mashup ."
"45","Mashroom : end-user mashup programming using nested tables This paper presents an end-user-oriented programming environment called Mashroom . Major contributions herein include an end-user programming model with an expressive data structure as well as a set of formally-defined mashup operators . The data structure takes advantage of nested table , and maintains the intuitiveness while allowing users to express complex data objects . The mashup operators are visualized with contextual menu and formula bar and can be directly applied on the data . Experiments and case studies reveal that end users have little difficulty in effectively and efficiently using Mashroom to build mashup applications ."
"46","PAKE-based mutual HTTP authentication for preventing phishing attacks We developed a new Web authentication protocol with password-based mutual authentication which prevents various kinds of phishing attacks . This protocol provides a protection of user 's passwords against any phishers even if a dictionary attack is employed , and prevents phishers from imitating a false sense of successful authentication to users . The protocol is designed considering interoperability with many recent Web applications which requires many features which current HTTP authentication does not provide . The protocol is proposed as an Internet Draft submitted to IETF , and implemented in both server side ( as an Apache extension ) and client side ( as a Mozilla-based browser and an IE-based one ) ."
"47","Earthquake shakes Twitter users : real-time event detection by social sensors Twitter , a popular microblogging service , has received much attention recently . An important characteristic of Twitter is its real-time nature . For example , when an earthquake occurs , people make many Twitter posts ( tweets ) related to the earthquake , which enables detection of earthquake occurrence promptly , simply by observing the tweets . As described in this paper , we investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event . To detect a target event , we devise a classifier of tweets based on features such as the keywords in a tweet , the number of words , and their context . Subsequently , we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location . We consider each Twitter user as a sensor and apply Kalman filtering and particle filtering , which are widely used for location estimation in ubiquitous\/pervasive computing . The particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons . As an application , we construct an earthquake reporting system in Japan . Because of the numerous earthquakes and the large number of Twitter users throughout the country , we can detect an earthquake with high probability ( 96 % of earthquakes of Japan Meteorological Agency ( JMA ) seismic intensity scale 3 or more are detected ) merely by monitoring tweets . Our system detects earthquakes promptly and sends e-mails to registered users . Notification is delivered much faster than the announcements that are broadcast by the JMA ."
"48","Mind the data skew : distributed inferencing by speeddating in elastic regions Semantic Web data exhibits very skewed frequency distributions among terms . Efficient large-scale distributed reasoning methods should maintain load-balance in the face of such highly skewed distribution of input data . We show that term-based partitioning , used by most distributed reasoning approaches , has limited scalability due to load-balancing problems . We address this problem with a method for data distribution based on clustering in elastic regions . Instead of as - signing data to fixed peers , data flows semi-randomly in the network . Data items `` speed-date '' while being temporarily collocated in the same peer . We introduce a bias in the routing to allow semantically clustered neighborhoods to emerge . Our approach is self-organising , efficient and does not require any central coordination . We have implemented this method on the MaRVIN platform and have performed experiments on large real-world datasets , using a cluster of up to 64 nodes . We compute the RDFS closure over different datasets and show that our clustering algorithm drastically reduces computation time , calculating the RDFS closure of 200 million triples in 7.2 minutes ."
"49","Tv2web : generating and browsing web with multiple lod from video streams and their metadata We propose a method of automatically constructing Web content from video streams with metadata that we call TV2Web . The Web content includes thumbnails of video units and caption data generated from metadata . Users can watch TV ona normal Web browser . They can also manipulate Web content with zooming metaphors to seamlessly alter the level of detail ( LOD ) of the content being viewed . They can search for favorite scenes faster than with analog video equipment , and experience a new cross-media environment . We also developed a prototype of the TV2Web system and discuss its implementation ."
"50","Empirical comparison of algorithms for network community detection Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest . In practice , one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity , and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that `` look like '' good communities for the application of interest . In this paper , we explore a range of network community detection methods in order to compare them and to understand their relative performance and the systematic biases in the clusters they identify . We evaluate several common objective functions that are used to formalize the notion of a network community , and we examine several different classes of approximation algorithms that aim to optimize such objective functions . In addition , rather than simply fixing an objective and asking for an approximation to the best cluster of any size , we consider a size-resolved version of the optimization problem . Considering community quality as a function of its size provides a much finer lens with which to examine community detection algorithms , since objective functions and approximation algorithms often have non-obvious size-dependent behavior ."
"51","What is disputed on the web ? We present a method for automatically acquiring of a corpus of disputed claims from the web . We consider a factual claim to be disputed if a page on the web suggests both that the claim is false and also that other people say it is true . Our tool extracts disputed claims by searching the web for patterns such as `` falsely claimed that X '' and then using a statistical classifier to select text that appears to be making a disputed claim . We argue that such a corpus of disputed claims is useful for a wide range of applications related to information credibility on the web , and we report what our current corpus reveals about what is being disputed on the web ."
"52","Highlighting disputed claims on the web We describe Dispute Finder , a browser extension that alerts a user when information they read online is disputed by a source that they might trust . Dispute Finder examines the text on the page that the user is browsing and highlights any phrases that resemble known disputed claims . If a user clicks on a highlighted phrase then Dispute Finder shows them a list of articles that support other points of view . Dispute Finder builds a database of known disputed claims by crawling web sites that already maintain lists of disputed claims , and by allowing users to enter claims that they believe are disputed . Dispute Finder identifies snippets that make known disputed claims by running a simple textual entailment algorithm inside the browser extension , referring to a cached local copy of the claim database . In this paper , we explain the design of Dispute Finder , and the trade-offs between the various design decisions that we explored ."
"53","Towards robust trust establishment in web-based social networks with socialtrust We propose the SocialTrust framework for tamper-resilient trust establishment in online social networks . Two of the salient features of SocialTrust are its dynamic revision of trust by ( i ) distinguishing relationship quality from trust ; and ( ii ) incorporating a personalized feedback mechanism for adapting as the social network evolves ."
"54","Addressing the testing challenge with a web-based e-assessment system that tutors as it assesses Secondary teachers across the country are being asked to use formative assessment data to inform their classroom instruction . At the same time , critics of No Child Left Behind are calling the bill `` No Child Left Untestedö emphasizing the negative side of assessment , in that every hour spent assessing students is an hour lost from instruction . Or does it have to be ? What if we better integrated assessment into the classroom , and we allowed students to learn during the test ? Maybe we could even provide tutoring on the steps of solving problems . Our hypothesis is that we can achieve more accurate assessment by not only using data on whether students get test items right or wrong , but by also using data on the effort required for students to learn how to solve a test item . We provide evidence for this hypothesis using data collected with our E-ASSISTment system by more than 600 students over the course of the 2004-2005 school year . We also show that we can track student knowledge over time using modern longitudinal data analysis techniques . In a separate paper ( 9 ) , we report on the ASSISTment system 's architecture and scalability , while this paper is focused on how we can reliably assess student learning ."
"55","Collective privacy management in social networks Social Networking is one of the major technological phenomena of the Web 2.0 , with hundreds of millions of people participating . Social networks enable a form of self expression for users , and help them to socialize and share content with other users . In spite of the fact that content sharing represents one of the prominent features of existing Social Network sites , Social Networks yet do not support any mechanism for collaborative management of privacy settings for shared content . In this paper , we model the problem of collaborative enforcement of privacy policies on shared data by using game theory . In particular , we propose a solution that offers automated ways to share images based on an extended notion of content ownership . Building upon the Clarke-Tax mechanism , we describe a simple mechanism that promotes truthfulness , and that rewards users who promote co-ownership . We integrate our design with inference techniques that free the users from the burden of manually selecting privacy preferences for each picture . To the best of our knowledge this is the first time such a protection mechanism for Social Networking has been proposed . In the paper , we also show a proof-of-concept application , which we implemented in the context of Facebook , one of today 's most popular social networks . We show that supporting these type of solutions is not also feasible , but can be implemented through a minimal increase in overhead to end-users ."
"56","Parallel crawling for online social networks Given a huge online social network , how do we retrieve information from it through crawling ? Even better , how do we improve the crawling performance by using parallel crawlers that work independently ? In this paper , we present the framework of parallel crawlers for online social networks , utilizing a centralized queue . To show how this works in practice , we describe our implementation of the crawlers for an online auction website . The crawlers work independently , therefore the failing of one crawler does not affect the others at all . The framework ensures that no redundant crawling would occur . Using the crawlers that we built , we visited a total of approximately 11 million auction users , about 66,000 of which were completely crawled ."
"57","Privacy wizards for social networking sites Privacy is an enormous problem in online social networking sites . While sites such as Facebook allow users fine-grained control over who can see their profiles , it is difficult for average users to specify this kind of detailed policy . In this paper , we propose a template for the design of a social networking privacy wizard . The intuition for the design comes from the observation that real users conceive their privacy preferences ( which friends should be able to see which information ) based on an implicit set of rules . Thus , with a limited amount of user input , it is usually possible to build a machine learning model that concisely describes a particular user 's preferences , and then use this model to configure the user 's privacy settings automatically . As an instance of this general framework , we have built a wizard based on an active learning paradigm called uncertainty sampling . The wizard iteratively asks the user to assign privacy `` labels '' to selected ( `` informative '' ) friends , and it uses this input to construct a classifier , which can in turn be used to automatically assign privileges to the rest of the user 's ( unlabeled ) friends . To evaluate our approach , we collected detailed privacy preference data from 45 real Facebook users . Our study revealed two important things . First , real users tend to conceive their privacy preferences in terms of communities , which can easily be extracted from a social network graph using existing techniques . Second , our active learning wizard , using communities as features , is able to recommend high-accuracy privacy settings using less user input than existing policy-specification tools ."
"58","Detection and analysis of drive-by-download attacks and malicious JavaScript code JavaScript is a browser scripting language that allows developers to create sophisticated client-side interfaces for web applications . However , JavaScript code is also used to carry out attacks against the user 's browser and its extensions . These attacks usually result in the download of additional malware that takes complete control of the victim 's platform , and are , therefore , called `` drive-by downloads . '' Unfortunately , the dynamic nature of the JavaScript language and its tight integration with the browser make it difficult to detect and block malicious JavaScript code . This paper presents a novel approach to the detection and analysis of malicious JavaScript code . Our approach combines anomaly detection with emulation to automatically identify malicious JavaScript code and to support its analysis . We developed a system that uses a number of features and machine-learning techniques to establish the characteristics of normal JavaScript code . Then , during detection , the system is able to identify anomalous JavaScript code by emulating its behavior and comparing it to the established profiles . In addition to identifying malicious code , the system is able to support the analysis of obfuscated code and to generate detection signatures for signature-based systems . The system has been made publicly available and has been used by thousands of analysts ."
"59","Probabilistic question recommendation for question answering communities User-Interactive Question Answering ( QA ) communities such as Yahoo ! Answers are growing in popularity . However , as these QA sites always have thousands of new questions posted daily , it is difficult for users to find the questions that are of interest to them . Consequently , this may delay the answering of the new questions . This gives rise to question recommendation techniques that help users locate interesting questions . In this paper , we adopt the Probabilistic Latent Semantic Analysis ( PLSA ) model for question recommendation and propose a novel metric to evaluate the performance of our approach . The experimental results show our recommendation approach is effective ."
"60","Tag-oriented document summarization Social annotations on a Web document are highly generalized description of topics contained in that page . Their tagged frequency indicates the user attentions with various degrees . This makes annotations a good resource for summarizing multiple topics in a Web page . In this paper , we present a tag-oriented Web document summarization approach by using both document content and the tags annotated on that document . To improve summarization performance , a new tag ranking algorithm named EigenTag is proposed in this paper to reduce noise in tags . Meanwhile , association mining technique is employed to expand tag set to tackle the sparsity problem . Experimental results show our tag-oriented summarization has a significant improvement over those not using tags ."
"61","News article extraction with template-independent wrapper We consider the problem of template-independent news extraction . The state-of-the-art news extraction method is based on template-level wrapper induction , which has two serious limitations . 1 ) It can not correctly extract pages belonging to an unseen template until the wrapper for that template has been generated . 2 ) It is costly to maintain up-to-date wrappers for hundreds of websites , because any change of a template may lead to the invalidation of the corresponding wrapper . In this paper we formalize news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site . Novel features dedicated to news titles and bodies are developed respectively . Correlations between the news title and the news body are exploited . Our template-independent wrapper can extract news pages from different sites regardless of templates . In experiments , a wrapper is learned from 40 pages from a single news site . It achieved 98.1 % accuracy over 3,973 news pages from 12 news sites ."
"62","Malicious interface design : exploiting the user In an ideal world , interface design is the art and science of helping users accomplish tasks in a timely , efficient , and pleasurable manner . This paper studies the inverse situation , the vast emergence of deliberately constructed malicious interfaces that violate design best practices in order to accomplish goals counter to those of the user . This has become a commonplace occurrence both on and off the desktop , particularly on the web . A primary objective of this paper is to formally define this problem , including construction of a taxonomy of malicious interface techniques and a preliminary analysis of their impact on users . Findings are presented that gauge the self-reported tolerance and expectation levels of users with regard to malicious interfaces as well as the effectiveness and ease of use of existing countermeasures . A second objective of this paper is to increase awareness , dialogue , and research in a domain that we consider largely unexplored but critical to future usability of the WWW . Our results were accomplished through significant compilation of malicious interface techniques based on review of thousands of web sites and by conducting three surveys . Ultimately , this paper concludes that malicious interfaces are a ubiquitous problem that demands intervention by the security and human computer interaction communities in order to reduce the negative impact on the global user population ."
"63","Large scale integration of senses for the semantic web Nowadays , the increasing amount of semantic data available on the Web leads to a new stage in the potential of Semantic Web applications . However , it also introduces new issues due to the heterogeneity of the available semantic resources . One of the most remarkable is redundancy , that is , the excess of different semantic descriptions , coming from different sources , to describe the same intended meaning . In this paper , we propose a technique to perform a large scale integration of senses ( expressed as ontology terms ) , in order to cluster the most similar ones , when indexing large amounts of online semantic information . It can dramatically reduce the redundancy problem on the current Semantic Web . In order to make this objective feasible , we have studied the adaptability and scalability of our previous work on sense integration , to be translated to the much larger scenario of the Semantic Web . Our evaluation shows a good behavior of these techniques when used in large scale experiments , then making feasible the proposed approach ."
"64","Context-aware citation recommendation When you write papers , how many times do you want to make some citations at a place but you are not sure which papers to cite ? Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations ? In this paper , we present our initiative of building a context-aware citation recommendation system . High quality citation recommendation is challenging : not only should the citations recommended be relevant to the paper under composition , but also should match the local contexts of the places citations are made . Moreover , it is far from trivial to model how the topic of the whole paper and the contexts of the citation places should affect the selection and ranking of citations . To tackle the problem , we develop a context-aware approach . The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document . Our approach can recommend citations for a context effectively . Moreover , it can recommend a set of citations for a paper with high quality . We implement a prototype system in CiteSeerX . An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates the effectiveness and the scalability of our approach ."
"65","DEW : DNS-enhanced web for faster content delivery With a key component of latency on the Web being connection set up between clients and Web servers , several ways to avoid connections have been explored . While the work in recent years on Content Distribution Networks ( CDNs ) have moved some content ` closer ' to users at the cost of increasing DNS traffic , they have not fully exploited the available unused potential of existing protocols . We explore ways by which a variety of Web responses can be piggybacked on DNS messages . While we evaluated our idea in the Web context , the approach is generic and not restricted to Web responses . We propose an architecture for HTTP piggybacking in DNS messages and carry out a detailed performance analysis based on a trace-driven simulation study . Our architecture requires minimal extensions to existing protocols , utilizing only the allowed optional fields for these extensions . It is fully compatible and can coexist with the current Web ."
"66","Shout out : integrating news and reader comments A useful approach for enabling computers to automatically create new content is utilizing the text , media , and information already present on the World Wide Web . The newly created content is known as `` machine-generated content '' . For example , a machine-generated content system may create a multimedia news show with two animated anchors presenting a news story ; one anchor reads the news story with text taken from an existing news article , and the other anchor regularly interrupts with his or her own opinion about the story . In this paper , we present such a system , and describe in detail its strategy for autonomously extracting and selecting the opinions given by the second anchor ."
"67","Implementing the media fragments URI specification In this paper , we describe two examples of implementations of the Media Fragments URI specification which is currently being developed by the W3C Media Fragments Working Group . The group 's mission is to create standard addressing schemes for media fragments on the Web using Uniform Resource Identifiers ( URIs ) . We describe two scenarios to illustrate the implementations . More specifically , we show how User Agents ( UA ) will either be able to resolve media fragment URIs without help from the server , or will make use of a media fragments-aware server . Finally , we present some ongoing discussions and issues regarding the implementation of the Media Fragments specification ."
"68","Facetedpedia : dynamic generation of query-dependent faceted interfaces for wikipedia This paper proposes Facetedpedia , a faceted retrieval system for information discovery and exploration in Wikipedia . Given the set of Wikipedia articles resulting from a keyword query , Facetedpedia generates a faceted interface for navigating the result articles . Compared with other faceted retrieval systems , Facetedpedia is fully automatic and dynamic in both facet generation and hierarchy construction , and the facets are based on the rich semantic information from Wikipedia . The essence of our approach is to build upon the collaborative vocabulary in Wikipedia , more specifically the intensive internal structures ( hyperlinks ) and folksonomy ( category system ) . Given the sheer size and complexity of this corpus , the space of possible choices of faceted interfaces is prohibitively large . We propose metrics for ranking individual facet hierarchies by user 's navigational cost , and metrics for ranking interfaces ( each with k facets ) by both their average pairwise similarities and average navigational costs . We thus develop faceted interface discovery algorithms that optimize the ranking metrics . Our experimental evaluation and user study verify the effectiveness of the system ."
"69","Privacy in dynamic social networks Anonymization of social networks before they are published or shared has become an important research question . Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network . However , social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis . We study the problem of repeatedly publishing social network data as the network evolves , while preserving privacy of users . Publishing multiple instances of the same network independently has privacy risks , since stitching the information together may allow an adversary to identify users in the networks . We propose methods to anonymize a dynamic network such that the privacy of users is preserved when new nodes and edges are added to the published network . These methods make use of link prediction algorithms to model the evolution of the social network . Using this predicted graph to perform group-based anonymization , the loss in privacy caused by new edges can be reduced . We evaluate the privacy loss on publishing multiple social network instances using our methods ."
"70","Clustering query refinements by user intent We address the problem of clustering the refinements of a user search query . The clusters computed by our proposed algorithm can be used to improve the selection and placement of the query suggestions proposed by a search engine , and can also serve to summarize the different aspects of information relevant to the original user query . Our algorithm clusters refinements based on their likely underlying user intents by combining document click and session co-occurrence information . At its core , our algorithm operates by performing multiple random walks on a Markov graph that approximates user search behavior . A user study performed on top search engine queries shows that our clusters are rated better than corresponding clusters computed using approaches that use only document click or only sessions co-occurrence information ."
"71","P2Cast : peer-to-peer patching scheme for VoD service Providing video on demand ( VoD ) service over the Internet in a scalable way is a challenging problem . In this paper , we propose P2Cast - an architecture that uses a peer-to-peer approach to cooperatively stream video using patching techniques , while only relying on unicast connections among peers . We address the following two key technical issues in P2Cast : ( 1 ) constructing an application overlay appropriate for streaming ; and ( 2 ) providing continuous stream playback ( without glitches ) in the face of disruption from an early departing client . Our simulation experiments show that P2Cast can serve many more clients than traditional client-server unicast service , and that it generally out-performs multicast-based patching if clients can cache more than of a stream 's initial portion . We handle disruptions by delaying the start of playback and applying the shifted forwarding technique . A threshold on the length of time during which arriving clients are served in a single session in P2Cast serves as a knob to adjust the balance between the scalability and the clients ' viewing quality in P2Cast ."
"72","Sig . ma : live views on the web of data We demonstrate Sig . ma , both a service and an end user application to access the Web of Data as an integrated information space . Sig . ma uses an holistic approach in which large scale semantic web indexing , logic reasoning , data aggregation heuristics , ad hoc ontology consolidation , external services and responsive user interaction all play together to create rich entity descriptions . These consolidated entity descriptions then form the base for embeddable data mashups , machine oriented services as well as data browsing services . Finally , we discuss Sig . ma 's peculiar characteristics and report on lessions learned and ideas it inspires ."
"73","Factorizing personalized Markov chains for next-basket recommendation Recommender systems are an important component of many websites . Two of the most popular approaches are based on matrix factorization ( MF ) and Markov chains ( MC ) . MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences . On the other hand , MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user . In this paper , we present a method bringing both approaches together . Our method is based on personalized transition graphs over underlying Markov chains . That means for each user an own transition matrix is learned - thus in total the method uses a transition cube . As the observations for estimating the transitions are usually very limited , our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition . We show that our factorized personalized MC ( FPMC ) model subsumes both a common Markov chain and the normal matrix factorization model . For learning the model parameters , we introduce an adaption of the Bayesian Personalized Ranking ( BPR ) framework for sequential basket data . Empirically , we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization ."
"74","Exploring web scale language models for search query processing It has been widely observed that search queries are composed in a very different style from that of the body or the title of a document . Many techniques explicitly accounting for this language style discrepancy have shown promising results for information retrieval , yet a large scale analysis on the extent of the language differences has been lacking . In this paper , we present an extensive study on this issue by examining the language model properties of search queries and the three text streams associated with each web document : the body , the title , and the anchor text . Our information theoretical analysis shows that queries seem to be composed in a way most similar to how authors summarize documents in anchor texts or titles , offering a quantitative explanation to the observations in past work . We apply these web scale n-gram language models to three search query processing ( SQP ) tasks : query spelling correction , query bracketing and long query segmentation . By controlling the size and the order of different language models , we find that the perplexity metric to be a good accuracy indicator for these query processing tasks . We show that using smoothed language models yields significant accuracy gains for query bracketing for instance , compared to using web counts as in the literature . We also demonstrate that applying web-scale language models can have marked accuracy advantage over smaller ones ."
"75","Regular expressions considered harmful in client-side XSS filters Cross-site scripting flaws have now surpassed buffer overflows as the world 's most common publicly-reported security vulnerability . In recent years , browser vendors and researchers have tried to develop client-side filters to mitigate these attacks . We analyze the best existing filters and find them to be either unacceptably slow or easily circumvented . Worse , some of these filters could introduce vulnerabilities into sites that were previously bug-free . We propose a new filter design that achieves both high performance and high precision by blocking scripts after HTML parsing but before execution . Compared to previous approaches , our approach is faster , protects against more vulnerabilities , and is harder for attackers to abuse . We have contributed an implementation of our filter design to the WebKit open source rendering engine , and the filter is now enabled by default in the Google Chrome browser ."
"76","Scalable techniques for document identifier assignment in inverted indexes Web search engines depend on the full-text inverted index data structure . Because the query processing performance is so dependent on the size of the inverted index , a plethora of research has focused on fast end effective techniques for compressing this structure . Recently , several authors have proposed techniques for improving index compression by optimizing the assignment of document identifiers to the documents in the collection , leading to significant reduction in overall index size . In this paper , we propose improved techniques for document identifier assignment . Previous work includes simple and fast heuristics such as sorting by URL , as well as more involved approaches based on the Traveling Salesman Problem or on graph partitioning . These techniques achieve good compression but do not scale to larger document collections . We propose a new framework based on performing a Traveling Salesman computation on a reduced sparse graph obtained through Locality Sensitive Hashing . This technique achieves improved compression while scaling to tens of millions of documents . Based on this framework , we describe a number of new algorithms , and perform a detailed evaluation on three large data sets showing improvements in index size ."
"77","Detecting the origin of text segments efficiently In the origin detection problem an algorithm is given a set S of documents , ordered by creation time , and a query document D. It needs to output for every consecutive sequence of k alphanumeric terms in D the earliest document in $ S$ in which the sequence appeared ( if such a document exists ) . Algorithms for the origin detection problem can , for example , be used to detect the `` origin '' of text segments in D and thus to detect novel content in D. They can also find the document from which the author of D has copied the most ( or show that D is mostly original . ) We concentrate on solutions that use only a fixed amount of memory . We propose novel algorithms for this problem and evaluate them together with a large number of previously published algorithms . Our results show that ( 1 ) detecting the origin of text segments efficiently can be done with very high accuracy even when the space used is less than 1 % of the size of the documents in $ S$ , ( 2 ) the precision degrades smoothly with the amount of available space , ( 3 ) various estimation techniques can be used to increase the performance of the algorithms ."
"78","Wikipedia vandalism detection Wikipedia is an online encyclopedia that anyone can access and edit . It has become one of the most important sources of knowledge online and many third party projects rely on it for a wide-range of purposes . The open model of Wikipedia allows pranksters , lobbyists and spammers to attack the integrity of the encyclopedia and this endangers it as a public resource . This is known in the community as vandalism . A plethora of methods have been developed within the Wikipedia and the scientific community to tackle this problem . We have participated in this effort and developed one of the leading approaches . Our research aims to create a fully-working antivandalism system and get it working in the real world ."
"79","The recurrence dynamics of social tagging How often do tags recur ? How hard is predicting tag recurrence ? What tags are likely to recur ? We try to answer these questions by analyzing the RSDC08 dataset , in both individual and collective settings . Our findings provide useful insights for the development of tag suggestion techniques etc. ."
"80","Data summaries for on-demand queries over linked data Typical approaches for querying structured Web Data collect ( crawl ) and pre-process ( index ) large amounts of data in a central data repository before allowing for query answering . However , this time-consuming pre-processing phase however leverages the benefits of Linked Data -- where structured data is accessible live and up-to-date at distributed Web resources that may change constantly -- only to a limited degree , as query results can never be current . An ideal query answering system for Linked Data should return current answers in a reasonable amount of time , even on corpora as large as the Web . Query processors evaluating queries directly on the live sources require knowledge of the contents of data sources . In this paper , we develop and evaluate an approximate index structure summarising graph-structured content of sources adhering to Linked Data principles , provide an algorithm for answering conjunctive queries over Linked Data on theWeb exploiting the source summary , and evaluate the system using synthetically generated queries . The experimental results show that our lightweight index structure enables complete and up-to-date query results over Linked Data , while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost ."
"81","Groupme ! This paper presents the GroupMe ! system , a resource sharing system with advanced tagging functionality . GroupMe ! provides a novel user interface , which enables users to organize and arrange arbitrary Web resources into groups . The content of such groups can be overlooked and inspected immediately as resources are visualized in a multimedia-based fashion . In this paper , we furthermore introduce new folksonomy-based ranking strategies that exploit the group structure shipped with GroupMe ! folksonomies . Experiments show that those strategies significantly improve the performance of such ranking algorithms ."
"82","Exploiting content redundancy for web information extraction We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites . We start by populating a seed database with records extracted from a few initial sites . We then identify values within the pages of each new site that match attribute values contained in the seed set of records . To filter out noisy attribute value matches , we exploit the fact that attribute values occur at fixed positions within template-based sites . We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages . Finally , we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach ."
"83","Incentivizing high-quality user-generated content We model the economics of incentivizing high-quality user generated content ( UGC ) , motivated by settings such as online review forums , question-answer sites , and comments on news articles and blogs . We provide a game-theoretic model within which to study the problem of incentivizing high quality UGC , in which contributors are strategic and motivated by exposure . Our model has the feature that both the quality of contributions as well as the extent of participation is determined endogenously in a free-entry Nash equilibrium . The model predicts , as observed in practice , that if exposure is independent of quality , there will be a flood of low quality contributions in equilibrium . An ideal mechanism in this context would elicit both high quality and high participation in equilibrium , with near-optimal quality as the available attention diverges , and should be easily implementable in practice . We consider a very simple elimination mechanism , which subjects each contribution to rating by some number A of viewers , and eliminates any contributions that are not uniformly rated positively . We construct and analyze free-entry Nash equilibria for this mechanism , and show that A can be chosen to achieve quality that tends to optimal , along with diverging participation , as the number of viewers diverges ."
"84","Constructing folksonomies by integrating structured metadata Aggregating many personal hierarchies into a common taxonomy , also known as a folksonomy , presents several challenges due to its sparseness , ambiguity , noise , and inconsistency . We describe an approach to folksonomy learning based on relational clustering that addresses these challenges by exploiting structured metadata contained in personal hierarchies . Our approach clusters similar hierarchies using their structure and tag statistics , then incrementally weaves them into a deeper , bushier tree . We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr . We evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy created by the Open Directory Project . Our empirical results suggest that the proposed approach improves upon the state-of-the-art folksonomy learning method ."
"85","Constructing folksonomies from user-specified relations on flickr Automatic folksonomy construction from tags has attracted much attention recently . However , inferring hierarchical relations between concepts from tags has a drawback in that it is difficult to distinguish between more popular and more general concepts . Instead of tags we propose to use user-specified relations for learning folksonomy . We explore two statistical frameworks for aggregating many shallow individual hierarchies , expressed through the collection\/set relations on the social photosharing site Flickr , into a common deeper folksonomy that reflects how a community organizes knowledge . Our approach addresses a number of challenges that arise while aggregating information from diverse users , namely noisy vocabulary , and variations in the granularity level of the concepts expressed . Our second contribution is a method for automatically evaluating learned folksonomy by comparing it to a reference taxonomy , e.g. , the Web directory created by the Open Directory Project . Our empirical results suggest that user-specified relations are a good source of evidence for learning folksonomies ."
"86","Data quality in web archiving Web archives preserve the history of Web sites and have high long-term value for media and business analysts . Such archives are maintained by periodically re-crawling entire Web sites of interest . From an archivist 's point of view , the ideal case to ensure highest possible data quality of the archive would be to `` freeze '' the complete contents of an entire Web site during the time span of crawling and capturing the site . Of course , this is practically infeasible . To comply with the politeness specification of a Web site , the crawler needs to pause between subsequent http requests in order to avoid unduly high load on the site 's http server . As a consequence , capturing a large Web site may span hours or even days , which increases the risk that contents collected so far are incoherent with the parts that are still to be crawled . This paper introduces a model for identifying coherent sections of an archive and , thus , measuring the data quality in Web archiving . Additionally , we present a crawling strategy that aims to ensure archive coherence by minimizing the diffusion of Web site captures . Preliminary experiments demonstrate the usefulness of the model and the effectiveness of the strategy ."
"87","Web page rank prediction with markov models In this paper we propose a method for predicting the ranking position of a Web page . Assuming a set of successive past top-k rankings , we study the evolution of Web pages in terms of ranking trend sequences used for Markov Models training , which are in turn used to predict future rankings . The predictions are highly accurate for all experimental setups and similarity measures ."
"88","Modeling relationship strength in online social networks Previous work analyzing social networks has mainly focused on binary friendship relations . However , in online social networks the low cost of link formation can lead to networks with heterogeneous relationship strengths ( e.g. , acquaintances and best friends mixed together ) . In this case , the binary friendship indicator provides only a coarse representation of relationship information . In this work , we develop an unsupervised model to estimate relationship strength from interaction activity ( e.g. , communication , tagging ) and user similarity . More specifically , we formulate a link-based latent variable model , along with a coordinate ascent optimization procedure for the inference . We evaluate our approach on real-world data from Facebook and LinkedIn , showing that the estimated link weights result in higher autocorrelation and lead to improved classification accuracy ."
"89","Entity relation discovery from web tables and links The World-Wide Web consists not only of a huge number of unstructured texts , but also a vast amount of valuable structured data . Web tables ( 2 ) are a typical type of structured information that are pervasive on the web , and Web-scale methods that automatically extract web tables have been studied extensively ( 1 ) . Many powerful systems ( e.g. OCTOPUS ( 4 ) , Mesa ( 3 ) ) use extracted web tables as a fundamental component . In the database vernacular , a table is defined as a set of tuples which have the same attributes . Similarly , a web table is defined as a set of rows ( corresponding to database tuples ) which have the same column headers ( corresponding to database attributes ) . Therefore , to extract a web table is to extract a relation on the web . In databases , tables often contain foreign keys which refer to other tables . Therefore , it follows that hyperlinks inside a web table sometimes function as foreign keys to other relations whose tuples are contained in the hyperlink 's target pages . In this paper , we explore this idea by asking : can we discover new attributes for web tables by exploring hyperlinks inside web tables ? This poster proposes a solution that takes a web table as input . Frequent patterns are generated as new candidate relations by following hyperlinks in the web table . The confidence of candidates are evaluated , and trustworthy candidates are selected to become new attributes for the table . Finally , we show the usefulness of our method by performing experiments on a variety of web domains ."
"90","Extracting data records from the web using tag path clustering Fully automatic methods that extract lists of objects from the Web have been studied extensively . Record extraction , the first step of this object extraction process , identifies a set of Web page segments , each of which represents an individual object ( e.g. , a product ) . State-of-the-art methods suffice for simple search , but they often fail to handle more complicated or noisy Web page structures due to a key limitation -- their greedy manner of identifying a list of records through pairwise comparison ( i.e. , similarity match ) of consecutive segments . This paper introduces a new method for record extraction that captures a list of objects in a more robust way based on a holistic analysis of a Web page . The method focuses on how a distinct tag path appears repeatedly in the DOM tree of the Web document . Instead of comparing a pair of individual segments , it compares a pair of tag path occurrence patterns ( called visual signals ) to estimate how likely these two tag paths represent the same list of objects . The paper introduces a similarity measure that captures how closely the visual signals appear and interleave . Clustering of tag paths is then performed based on this similarity measure , and sets of tag paths that form the structure of data records are extracted . Experiments show that this method achieves higher accuracy than previous methods ."
"91","LINKREC : a unified framework for link recommendation with user attributes and graph structure With the phenomenal success of networking sites ( e.g. , Facebook , Twitter and LinkedIn ) , social networks have drawn substantial attention . On online social networking sites , link recommendation is a critical task that not only helps improve user experience but also plays an essential role in network growth . In this paper we propose several link recommendation criteria , based on both user attributes and graph structure . To discover the candidates that satisfy these criteria , link relevance is estimated using a random walk algorithm on an augmented social graph with both attribute and structure information . The global and local influence of the attributes is leveraged in the framework as well . Besides link recommendation , our framework can also rank attributes in a social network . Experiments on DBLP and IMDB data sets demonstrate that our method outperforms state-of-the-art methods based on network structure and node attribute information for link recommendation ."
"92","Cross-domain sentiment classification via spectral feature alignment Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of users publishing sentiment data ( e.g. , reviews , blogs ) . Although traditional classification algorithms can be used to train sentiment classifiers from manually labeled text data , the labeling work can be time-consuming and expensive . Meanwhile , users often use some different words when they express sentiment in different domains . If we directly apply a classifier trained in one domain to other domains , the performance will be very low due to the differences between these domains . In this work , we develop a general solution to sentiment classification when we do not have any labels in a target domain but have some labeled data in a different domain , regarded as source domain . In this cross-domain sentiment classification setting , to bridge the gap between the domains , we propose a spectral feature alignment ( SFA ) algorithm to align domain-specific words from different domains into unified clusters , with the help of domain-independent words as a bridge . In this way , the clusters can be used to reduce the gap between domain-specific words of the two domains , which can be used to train sentiment classifiers in the target domain accurately . Compared to previous approaches , SFA can discover a robust representation for cross-domain data by fully exploiting the relationship between the domain-specific and domain-independent words via simultaneously co-clustering them in a common latent space . We perform extensive experiments on two real world datasets , and demonstrate that SFA significantly outperforms previous approaches to cross-domain sentiment classification ."
"93","Enabling entity-based aggregators for web 2.0 data Selecting and presenting content culled from multiple heterogeneous and physically distributed sources is a challenging task . The exponential growth of the web data in modern times has brought new requirements to such integration systems . Data is not any more produced by content providers alone , but also from regular users through the highly popular Web 2.0 social and semantic web applications . The plethora of the available web content increased its demand by regular users who could not any more wait the development of advanced integration tools . They wanted to be able to build in a short time their own specialized integration applications . Aggregators came to the risk of these users . They allowed them not only to combine distributed content , but also to process it in ways that generate new services available for further consumption . To cope with the heterogeneous data , the Linked Data initiative aims at the creation and exploitation of correspondences across data values . In this work , although we share the Linked Data community vision , we advocate that for the modern web , linking at the data value level is not enough . Aggregators should base their integration tasks on the concept of an entity , i.e. , identifying whether different pieces of information correspond to the same real world entity , such as an event or a person . We describe our theory , system , and experimental results that illustrate the approach 's effectiveness ."
"94","Behavioral profiles for advanced email features We examine the behavioral patterns of email usage in a large-scale enterprise over a three-month period . In particular , we focus on two main questions : ( Q1 ) what do replies depend on ? and ( Q2 ) what is the gain of augmenting contacts through the friends of friends from the email social graph ? For Q1 , we identify and evaluate the significance of several factors that affect the reply probability and the email response time . We find that all factors of our considered set are significant , provide their relative ordering , and identify the recipient list size , and the intensity of email communication between the correspondents as the dominant factors . We highlight various novel threshold behaviors and provide support for existing hypotheses such as that of the least-effort reply . For Q2 , we find that the number of new contacts extracted from the friends-of-friends relationships amounts to a large number , but which is still a limited portion of the total enterprise size . We believe that our results provide significant insights towards informed design of advanced email features , including those of social-networking type ."
"95","Antourage : mining distance-constrained trips from flickr We study how to automatically extract tourist trips from large volumes of geo-tagged photographs . Working with more than 8 million of these photographs that are publicly available via photo - sharing communities such as Flickr and Panoramio , our goal is to satisfy the needs of a tourist who specifies a starting location ( typically a hotel ) together with a bounded travel distance and demands a tour that visits the popular sites along the way . Our system , named ANTOURAGE , solves this intractable problem using a novel adaptation of the max-min ant system ( MMAS ) meta-heuristic . Experiments using GPS metadata crawled from Flickr show that ANTOURAGE can generate high-quality tours ."
"96","Inferring query intent from reformulations and clicks Many researchers have noted that web search queries are often ambiguous or unclear . We present an approach for identifying the popular meanings of queries using web search logs and user click behavior . We show our approach to produce more complete and user-centric intents than expert judges by evaluating on TREC queries . This approach was also used by the TREC 2009 Web Track judges to obtain more representative topic descriptions from real queries ."
"97","Web-assisted annotation , semantic indexing and search of television and radio news The Rich News system , that can automatically annotate radio and television news with the aid of resources retrieved from the World Wide Web , is described . Automatic speech recognition gives a temporally precise but conceptually inaccurate annotation model . Information extraction from related web news sites gives the opposite : conceptual accuracy but no temporal data . Our approach combines the two for temporally accurate conceptual semantic annotation of broadcast news . First low quality transcripts of the broadcasts are produced using speech recognition , and these are then automatically divided into sections corresponding to individual news stories . A key phrases extraction component finds key phrases for each story and uses these to search for web pages reporting the same event . The text and meta-data of the web pages is then used to create index documents for the stories in the original broadcasts , which are semantically annotated using the KIM knowledge management platform . A web interface then allows conceptual search and browsing of news stories , and playing of the parts of the media files corresponding to each news story . The use of material from the World Wide Web allows much higher quality textual descriptions and semantic annotations to be produced than would have been possible using the ASR transcript directly . The semantic annotations can form a part of the Semantic Web , and an evaluation shows that the system operates with high precision , and with a moderate level of recall ."
"98","Ad-hoc object retrieval in the web of data Semantic Search refers to a loose set of concepts , challenges and techniques having to do with harnessing the information of the growing Web of Data ( WoD ) for Web search . Here we propose a formal model of one specific semantic search task : ad-hoc object retrieval . We show that this task provides a solid framework to study some of the semantic search problems currently tackled by commercial Web search engines . We connect this task to the traditional ad-hoc document retrieval and discuss appropriate evaluation metrics . Finally , we carry out a realistic evaluation of this task in the context of a Web search application ."
"99","On business activity modeling using grammars Web based applications offer a mainstream channel for businesses to manage their activities . We model such business activity in a grammar-based framework . The Backus Naur form notation is used to represent the syntax of a regular grammar corresponding to Web log patterns of interest . Then , a deterministic finite state machine is used to parse Web logs against the grammar . Detected tasks are associated with metadata such as time taken to perform the activity , and aggregated along relevant corporate dimensions ."
"100","CS AKTive space : representing computer science in the semantic web We present a Semantic Web application that we callCS AKTive Space . The application exploits a wide range of semantically heterogeneousand distributed content relating to Computer Science research in theUK . This content is gathered on a continuous basis using a variety of methods including harvesting and scraping as well as adopting a range models for content acquisition . The content currently comprises aroundten million RDF triples and we have developed storage , retrieval andmaintenance methods to support its management . The content is mediated through an ontology constructed for the application domainand incorporates components from other published ontologies . CS AKTive Spacesupports the exploration of patterns and implications inherent in the content and exploits a variety of visualisations and multi dimensional representations . Knowledge services supported in the applicationinclude investigating communities of practice : who is working , researching or publishing with whom . This work illustrates a number ofsubstantial challenges for the Semantic Web . These include problems of referential integrity , tractable inference and interaction support . Wereview our approaches to these issues and discuss relevant related work ."
"101","Fast and parallel webpage layout The web browser is a CPU-intensive program . Especially on mobile devices , webpages load too slowly , expending significant time in processing a document 's appearance . Due to power constraints , most hardware-driven speedups will come in the form of parallel architectures . This is also true of mobile devices such as phones and e-books . In this paper , we introduce new algorithms for CSS selector matching , layout solving , and font rendering , which represent key components for a fast layout engine . Evaluation on popular sites shows speedups as high as 80x . We also formulate the layout problem with attribute grammars , enabling us to not only parallelize our algorithm but prove that it computes in O ( log ) time and without reflow ."
"102","DSNotify : handling broken links in the web of data The Web of Data has emerged as a way of exposing structured linked data on the Web . It builds on the central building blocks of the Web ( URIs , HTTP ) and benefits from its simplicity and wide-spread adoption . It does , however , also inherit the unresolved issues such as the broken link problem . Broken links constitute a major challenge for actors consuming Linked Data as they require them to deal with reduced accessibility of data . We believe that the broken link problem is a major threat to the whole Web of Data idea and that both Linked Data consumers and providers will require solutions that deal with this problem . Since no general solutions for fixing such links in the Web of Data have emerged , we make three contributions into this direction : first , we provide a concise definition of the broken link problem and a comprehensive analysis of existing approaches . Second , we present DSNotify , a generic framework able to assist human and machine actors in fixing broken links . It uses heuristic feature comparison and employs a time-interval-based blocking technique for the underlying instance matching problem . Third , we derived benchmark datasets from knowledge bases such as DBpedia and evaluated the effectiveness of our approach with respect to the broken link problem . Our results show the feasibility of a time-interval-based blocking approach for systems that aim at detecting and fixing broken links in the Web of Data ."
"103","Volunteer computing : a model of the factors determining contribution to community-based scientific research Volunteer computing is a powerful way to harness distributed resources to perform large-scale tasks , similarly to other types of community-based initiatives . Volunteer computing is based on two pillars : the first is computational - allocating and managing large computing tasks ; the second is participative - making large numbers of individuals volunteer their computer resources to a project . While the computational aspects of volunteer computing received much research attention , the participative aspect remains largely unexplored . In this study we aim to address this gap : by drawing on social psychology and online communities research , we develop and test a three-dimensional model of the factors determining volunteer computing users ' contribution . We investigate one of the largest volunteer computing projects - SETI@home - by linking survey data about contributors ' motivations to their activity logs . Our findings highlight the differences between volunteer computing and other forms of community-based projects , and reveal the intricate relationship between individual motivations , social affiliation , tenure in the project , and resource contribution . Implications for research and practice are discussed ."
"104","Alhambra : a system for creating , enforcing , and testing browser security policies Alhambra is a browser-based system designed to enforce and test web browser security policies . At the core of Alhambra is a policy-enhanced browser supporting fine-grain security policies that restrict web page contents and execution . Alhambra requires no server-side modifications or additions to the web application . Policies can restrict the construction of the document as well as the execution of JavaScript using access control rules and a taint-tracking engine . Using the Alhambra browser , we present two security policies that we have built using our architecture , both designed to prevent cross-site scripting . The first policy uses a taint-tracking engine to prevent cross-site scripting attacks that exploit bugs in the client-side of the web applications . The second one uses browsing history to create policies that restrict the contents of documents and prevent the inclusion of malicious content . Using Alhambra we analyze the impact of policies on the compatibility of web pages . To test compatibility , Alhambra supports revisiting user-generated browsing sessions and comparing multiple security policies in parallel to quickly and automatically evaluate security policies . To compare security policies for identical pages we have also developed useful comparison metrics that quantify differences between identical pages executed with different security policies . Not only do we show that our policies are effective with minimal compatibility cost , we also demonstrate that Alhambra can enforce strong security policies and provide quantitative evaluation of the differences introduced by security policies ."
"105","Distributing private data in challenged network environments Developing countries face significant challenges in network access , making even simple network tasks unpleasant . Many standard techniques - caching and predictive prefetching - help somewhat , but provide little or no assistance for personal data that is needed only by a single user . Sulula addresses this problem by leveraging the near-ubiquity of cellular phones able to send and receive simple SMS messages . Rather than visit a kiosk and fetch data on demand - a tiresome process at best - users request a future visit . If capacity exists , the kiosk can schedule secure retrieval of that user 's data , saving time and more efficiently utilizing the kiosk 's limited connectivity . When the user arrives at a provisioned kiosk , she need only obtain the session key on-demand , and thereafter has instant access . In addition , Sulula allows users to schedule data uploads . Experimental results show significant gains for the end user , saving tens of minutes of time for a typical email\/news reading session . We also describe a small , ongoing deployment in-country for proof-of-concept , lessons learned from that experience , and provide a discussion on pricing and marketplace issues that remain to be addressed to make the system viable for developing-world access ."
"106","Mining advertiser-specific user behavior using adfactors Consider an online ad campaign run by an advertiser . The ad serving companies that handle such campaigns record users ' behavior that leads to impressions of campaign ads , as well as users ' responses to such impressions . This is summarized and reported to the advertisers to help them evaluate the performance of their campaigns and make better budget allocation decisions . The most popular reporting statistics are the click-through rate and the conversion rate . While these are indicative of the effectiveness of an ad campaign , the advertisers often seek to understand more sophisticated long-term effects of their ads on the brand awareness and the user behavior that leads to the conversion , thus creating a need for the reporting measures that can capture both the duration and the frequency of the pathways to user conversions . In this paper , we propose an alternative data mining framework for analyzing user-level advertising data . In the aggregation step , we compress individual user histories into a graph structure , called the adgraph , representing local correlations between ad events . For the reporting step , we introduce several scoring rules , called the adfactors ( AF ) , that can capture global role of ads and ad paths in the adgraph , in particular , the structural correlation between an ad impression and the user conversion . We present scalable local algorithms for computing the adfactors ; all algorithms were implemented using the MapReduce programming model and the Pregel framework . Using an anonymous user-level dataset of sponsored search campaigns for eight different advertisers , we evaluate our framework with different adgraphs and adfactors in terms of their statistical fit to the data , and show its value for mining the long-term behavioral patterns in the advertising data ."
"107","Exploiting social context for review quality prediction Online reviews in which users publish detailed commentary about their experiences and opinions with products , services , or events are extremely valuable to users who rely on them to make informed decisions . However , reviews vary greatly in quality and are constantly increasing in number , therefore , automatic assessment of review helpfulness is of growing importance . Previous work has addressed the problem by treating a review as a stand-alone document , extracting features from the review text , and learning a function based on these features for predicting the review quality . In this work , we exploit contextual information about authors ' identities and social networks for improving review quality prediction . We propose a generic framework for incorporating social context information by adding regularization constraints to the text-based predictor . Our approach can effectively use the social context information available for large quantities of unlabeled reviews . It also has the advantage that the resulting predictor is usable even when social context is unavailable . We validate our framework within a real commerce portal and experimentally demonstrate that using social context information can help improve the accuracy of review quality prediction especially when the available training data is sparse ."
"108","Measurement and analysis of an online content voting network : a case study of Digg In online content voting networks , aggregate user activities ( e.g. , submitting and rating content ) make high-quality content thrive through the unprecedented scale , high dynamics and divergent quality of user generated content ( UGC ) . To better understand the nature and impact of online content voting networks , we have analyzed Digg , a popular online social news aggregator and rating website . Based on a large amount of data collected , we provide an in-depth study of Digg . We study structural properties of Digg social network , revealing some strikingly distinct properties such as low link symmetry and the power-law distribution of node outdegree with truncated tails . We explore impact of the social network on user digging activities , and investigate the issues of content promotion , content filtering , vote spam and content censorship , which are inherent to content rating networks . We also provide insight into design of content promotion algorithms and recommendation-assisted content discovery . Overall , we believe that the results presented in this paper are crucial in understanding online content rating networks ."
"109","Beyond position bias : examining result attractiveness as a source of presentation bias in clickthrough data Leveraging clickthrough data has become a popular approach for evaluating and optimizing information retrieval systems . Although data is plentiful , one must take care when interpreting clicks , since user behavior can be affected by various sources of presentation bias . While the issue of position bias in clickthrough data has been the topic of much study , other presentation bias effects have received comparatively little attention . For instance , since users must decide whether to click on a result based on its summary ( e.g. , the title , URL and abstract ) , one might expect clicks to favor `` more attractive '' results . In this paper , we examine result summary attractiveness as a potential source of presentation bias . This study distinguishes itself from prior work by aiming to detect systematic biases in click behavior due to attractive summaries inflating perceived relevance . Our experiments conducted on the Google web search engine show substantial evidence of presentation bias in clicks towards results with more attractive titles ."
"110","Automated object persistence for JavaScript Traditionally web applications have required an internet connection in order to work with data . Browsers have lacked any mechanisms to allow web applications to operate offline with a set of data to provide constant access to applications . Recently , through browser plug-ins such as Google Gears , browsers have gained the ability to persist data for offline use . However , until now it 's been difficult for a web developer using these plug-ins to manage persisting data both locally for offline use and in the internet cloud due to : synchronization requirements , managing throughput and latency to the cloud , and making it work within the confines of a standards-compliant web browser . Historically in non-browser environments , programming language environments have offered automated object persistence to shield the developer from these complexities . In our research we have created a framework which introduces automated persistence of data objects for JavaScript utilizing the internet . Unlike traditional object persistence solutions , ours relies only on existing or forthcoming internet standards and does not rely upon specific runtime mechanisms such as OS or interpreter\/compiler support . A new design was required in order to be suitable to the internet 's unique characteristics of varying connection quality and a browser 's specific restrictions . We validate our approach using benchmarks which show that our framework can handle thousands of data objects automatically , reducing the amount of work needed by developers to support offline Web applications ."
"111","Diversifying web search results Result diversity is a topic of great importance as more facets of queries are discovered and users expect to find their desired facets in the first page of the results . However , the underlying questions of how ` diversity ' interplays with ` quality ' and when preference should be given to one or both are not well-understood . In this work , we model the problem as expectation maximization and study the challenges of estimating the model parameters and reaching an equilibrium . One model parameter , for example , is correlations between pages which we estimate using textual contents of pages and click data ( when available ) . We conduct experiments on diversifying randomly selected queries from a query log and the queries chosen from the disambiguation topics of Wikipedia . Our algorithm improves upon Google in terms of the diversity of random queries , retrieving 14 % to 38 % more aspects of queries in top 5 , while maintaining a precision very close to Google . On a more selective set of queries that are expected to benefit from diversification , our algorithm improves upon Google in terms of precision and diversity of the results , and significantly outperforms another baseline system for result diversification ."
"112","Statistical models of music-listening sessions in social media User experience in social media involves rich interactions with the media content and other participants in the community . In order to support such communities , it is important to understand the factors that drive the users ' engagement . In this paper we show how to define statistical models of different complexity to describe patterns of song listening in an online music community . First , we adapt the LDA model to capture music taste from listening activities across users and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste . Second , we define a graphical model that takes into account listening sessions and captures the listening moods of users in the community . Our session model leads to groups of songs and groups of listeners with similar behavior across listening sessions and enables faster inference when compared to the LDA model . Our experiments with the data from an online media site demonstrate that the session model is better in terms of the perplexity compared to two other models : the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent groupings of songs ."
"113","Money , glory and cheap talk : analyzing strategic behavior of contestants in simultaneous crowdsourcing contests on TopCoder.com Crowdsourcing is a new Web phenomenon , in which a firm takes a function once performed in-house and outsources it to a crowd , usually in the form of an open contest . Designing efficient crowdsourcing mechanisms is not possible without deep understanding of incentives and strategic choices of all participants . This paper presents an empirical analysis of determinants of individual performance in multiple simultaneous crowdsourcing contests using a unique dataset for the world 's largest competitive software development portal : TopCoder.com . Special attention is given to studying the effects of the reputation system currently used by TopCoder.com on behavior of contestants . We find that individual specific traits together with the project payment and the number of project requirements are significant predictors of the final project quality . Furthermore , we find significant evidence of strategic behavior of contestants . High rated contestants face tougher competition from their opponents in the competition phase of the contest . In order to soften the competition , they move first in the registration phase of the contest , signing up early for particular projects . Although registration in TopCoder contests is non-binding , it deters entry of opponents in the same contest ; our lower bound estimate shows that this strategy generates significant surplus gain to high rated contestants . We conjecture that the reputation + cheap talk mechanism employed by TopCoder has a positive effect on allocative efficiency of simultaneous all-pay contests and should be considered for adoption in other crowdsourcing platforms ."
"114","Live web search experiments for the rest of us There are significant barriers to academic research into user Web search preferences . Academic researchers are unable to manipulate the results shown by a major search engine to users and would have no access to the interaction data collected by the engine . Our initial approach to overcoming this was to ask participants to submit queries to an experimental search engine rather than their usual search tool . Over several different experiments we found that initial user buy-in was high but that people quickly drifted back to their old habits and stopped contributing data . Here , we report our investigation of possible reasons why this occurs . An alternative approach is exemplified by the Lemur browser toolbar , which allows local collection of user interaction data from search engine sessions , but does not allow result pages to be modified . We will demonstrate a new Firefox toolbar that we have developed to support experiments in which search results may be arbitrarily manipulated . Using our toolbar , academics can set up the experiments they want to conduct , while collecting ( subject to human experimentation guidelines ) queries , clicks and dwell times as well as optional explicit judgments ."
"115","On the high density of leadership nuclei in endorsement social networks In this paper we study the community structure of endorsement networks , i.e. , social networks in which a directed edge u → v is asserting an action of support from user u to user v. Examples include scenarios in which a user u is favoring a photo , liking a post , or following the microblog of user v. Starting from the hypothesis that the footprint of a community in an endorsement network is a bipartite directed clique from a set of followers to a set of leaders , we apply frequent itemset mining techniques to discover such bicliques . Our analysis of real networks discovers that an interesting phenomenon is taking place : the leaders of a community are endorsing each other forming a very dense nucleus ."
"116","Semantic virtual environments Today 's Virtual Environment ( VE ) systems share a number of issues with the HTML-based World Wide Web . Their content is usually designed for presentation to humans , and thus is not suitable for machine access . This is complicated by the large number of different data models and network protocols in use . Accordingly , it is difficult to develop VE software , such as agents , services , and tools . In this paper we adopt the Semantic Web idea to the field of virtual environments . Using the Resource Description Framework ( RDF ) we establish a machine-understandable abstraction of existing VE systems -- the Semantic Virtual Environments ( SVE ) . On this basis it is possible to develop system-independent software , which could even operate across VE system boundaries ."
"117","Designing an architecture for delivering mobile information services to the rural developing world Implementing successful rural computing applications requires addressing a number of significant challenges . Recent advances in mobile phone computing capabilities make this device a likely candidate to address the client hardware constraints . Long battery life , wireless connectivity , solid-state memory , low price and immediate utility all make it better suited to rural conditions than a PC . However , current mobile software platforms are not as appropriate . Web-based mobile applications are hard to use , do not take advantage of the mobile phone 's media capabilities and require an online connection . Custom mobile applications are difficult to develop and distribute . To address these limitations we present CAM - a new framework for developing and deploying mobile computing applications in the rural developing world . CAM applications are accessed by capturing barcodes using the mobile phone camera , or entering numeric strings with the keypad . Supporting minimal navigation , direct linkage to paper practices and offline multi-media interaction , CAM is uniquely adapted to rural device , user and infrastructure constraints . To illustrate the breadth of the framework , we list a number of CAM-based applications that we have implemented or are planning . These include processing microfinance loans , facilitating rural supply chains , documenting grassroots innovation and accessing electronic medical histories ."
"118","Not so creepy crawler : easy crawler generation with standard xml queries Web crawlers are increasingly used for focused tasks such as the extraction of data from Wikipedia or the analysis of social networks like last . fm . In these cases , pages are far more uniformly structured than in the general Web and thus crawlers can use the structure of Web pages for more precise data extraction and more expressive analysis . In this demonstration , we present a focused , structure-based crawler generator , the `` Not so Creepy Crawler '' ( nc2 ) . What sets nc2 apart , is that all analysis and decision tasks of the crawling process are delegated to an ( arbitrary ) XML query engine such as XQuery or Xcerpt . Customizing crawlers just means writing ( declarative ) XML queries that can access the currently crawled document as well as the metadata of the crawl process . We identify four types of queries that together sufice to realize a wide variety of focused crawlers . We demonstrate nc2 with two applications : The first extracts data about cities from Wikipedia with a customizable set of attributes for selecting and reporting these cities . It illustrates the power of nc2 where data extraction from Wiki-style , fairly homogeneous knowledge sites is required . In contrast , the second use case demonstrates how easy nc2 makes even complex analysis tasks on social networking sites , here exemplified by last . fm ."
"119","SafeVchat : detecting obscene content and misbehaving users in online video chat services Online video chat services such as Chatroulette , Omegle , and vChatter that randomly match pairs of users in video chat sessions are fast becoming very popular , with over a million users per month in the case of Chatroulette . A key problem encountered in such systems is the presence of flashers and obscene content . This problem is especially acute given the presence of underage minors in such systems . This paper presents SafeVchat , a novel solution to the problem of flasher detection that employs an array of image detection algorithms . A key contribution of the paper concerns how the results of the individual detectors are fused together into an overall decision classifying the user as misbehaving or not , based on Dempster-Shafer Theory . The paper introduces a novel , motion-based skin detection method that achieves significantly higher recall and better precision . The proposed methods have been evaluated over real-world data and image traces obtained from Chatroulette.com ."
"120","Pragmatic evaluation of folksonomies Recently , a number of algorithms have been proposed to obtain hierarchical structures - so-called folksonomies - from social tagging data . Work on these algorithms is in part driven by a belief that folksonomies are useful for tasks such as : ( a ) Navigating social tagging systems and ( b ) Acquiring semantic relationships between tags . While the promises and pitfalls of the latter have been studied to some extent , we know very little about the extent to which folksonomies are pragmatically useful for navigating social tagging systems . This paper sets out to address this gap by presenting and applying a pragmatic framework for evaluating folksonomies . We model exploratory navigation of a tagging system as decentralized search on a network of tags . Evaluation is based on the fact that the performance of a decentralized search algorithm depends on the quality of the background knowledge used . The key idea of our approach is to use hierarchical structures learned by folksonomy algorithm as background knowledge for decentralized search . Utilizing decentralized search on tag networks in combination with different folksonomies as hierarchical background knowledge allows us to evaluate navigational tasks in social tagging systems . Our experiments with four state-of-the-art folksonomy algorithms on five different social tagging datasets reveal that existing folksonomy algorithms exhibit significant , previously undiscovered , differences with regard to their utility for navigation . Our results are relevant for engineers aiming to improve navigability of social tagging systems and for scientists aiming to evaluate different folksonomy algorithms from a pragmatic perspective ."
"121","visKQWL , a visual renderer for a semantic web query language KiWi is a semantic Wiki that combines the Wiki philosophy of collaborative content creation with the methods of the Semantic Web in order to enable effective knowledge management . Querying a Wiki must be simple enough for beginning users , yet powerful enough to accommodate experienced users . To this end , the keyword-based KiWi query language ( KWQL ) supports queries ranging from simple lists of keywords to expressive rules for selecting and reshaping Wiki ( meta - ) data . In this demo , we showcase visKWQL , a visual interface for the KWQL language aimed at supporting users in the query construction process . visKWQL and its editor are described , and their functionality is illustrated using example queries . visKWQL 's editor provides guidance throughout the query construction process through hints , warnings and highlighting of syntactic errors . The editor enables round-tripping between the twin languages KWQL and visKWQL , meaning that users can switch freely between the textual and visual form when constructing or editing a query . It is implemented using HTML , JavaScript , and CSS , and can thus be used in ( almost ) any web browser without any additional software ."
"122","LCA-based selection for XML document collections In this paper , we address the problem of database selection for XML document collections , that is , given a set of collections and a user query , how to rank the collections based on their goodness to the query . Goodness is determined by the relevance of the documents in the collection to the query . We consider keyword queries and support Lowest Common Ancestor ( LCA ) semantics for defining query results , where the relevance of each document to a query is determined by properties of the LCA of those nodes in the XML document that contain the query keywords . To avoid evaluating queries against each document in a collection , we propose maintaining in a preprocessing phase , information about the LCAs of all pairs of keywords in a document and use it to approximate the properties of the LCA-based results of a query . To improve storage and processing efficiency , we use appropriate summaries of the LCA information based on Bloom filters . We address both a boolean and a weighted version of the database selection problem . Our experimental results show that our approach incurs low errors in the estimation of the goodness of a collection and provides rankings that are very close to the actual ones ."
"123","A scalable machine-learning approach for semi-structured named entity recognition Named entity recognition studies the problem of locating and classifying parts of free text into a set of predefined categories . Although extensive research has focused on the detection of person , location and organization entities , there are many other entities of interest , including phone numbers , dates , times and currencies ( to name a few examples ) . We refer to these types of entities as `` semi-structured named entities '' , since they usually follow certain syntactic formats according to some conventions , although their structure is typically not well-defined . Regular expression solutions require significant amount of manual effort and supervised machine learning approaches rely on large sets of labeled training data . Therefore , these approaches do not scale when we need to support many semi-structured entity types in many languages and regions . In this paper , we study this problem and propose a novel three-level bootstrapping framework for the detection of semi-structured entities . We describe the proposed techniques for phone , date and time entities , and perform extensive evaluations on English , German , Polish , Swedish and Turkish documents . Despite the minimal input from the user , our approach can achieve 95 % precision and 84 % recall for phone entities , and 94 % precision and 81 % recall for date and time entities , on average . We also discuss implementation details and report run time performance results , which show significant improvements over regular expression based solutions ."
"124","Rated aspect summarization of short comments Web 2.0 technologies have enabled more and more people to freely comment on different kinds of entities ( e.g. sellers , products , services ) . The large scale of information poses the need and challenge of automatic summarization . In many cases , each of the user-generated short comments comes with an overall rating . In this paper , we study the problem of generating a `` rated aspect summary '' of short comments , which is a decomposed view of the overall ratings for the major aspects so that a user could gain different perspectives towards the target entity . We formally define the problem and decompose the solution into three steps . We demonstrate the effectiveness of our methods by using eBay sellers ' feedback comments . We also quantitatively evaluate each step of our methods and study how well human agree on such a summarization task . The proposed methods are quite general and can be used to generate rated aspect summary automatically given any collection of short comments each associated with an overall rating ."
"125","Differences in the mechanics of information diffusion across topics : idioms , political hashtags , and complex contagion on twitter There is a widespread intuitive sense that different kinds of information spread differently on-line , but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment . Here we study this issue on Twitter , analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users . We find significant variation in the ways that widely-used hashtags on different topics spread . Our results show that this variation is not attributable simply to differences in `` stickiness , '' the probability of adoption based on one or more exposures , but also to a quantity that could be viewed as a kind of `` persistence '' - the relative extent to which repeated exposures to a hashtag continue to have significant marginal effects . We find that hashtags on politically controversial topics are particularly persistent , with repeated exposures continuing to have unusually large marginal effects on adoption ; this provides , to our knowledge , the first large-scale validation of the `` complex contagion '' principle from sociology , which posits that repeated exposures to an idea are particularly crucial when the idea is in some way controversial or contentious . Among other findings , we discover that hashtags representing the natural analogs of Twitter idioms and neologisms are particularly non-persistent , with the effect of multiple exposures decaying rapidly relative to the first exposure . We also study the subgraph structure of the initial adopters for different widely-adopted hashtags , again finding structural differences across topics . We develop simulation-based and generative models to analyze how the adoption dynamics interact with the network structure of the early adopters on which a hashtag spreads ."
"126","Using context - and content-based trust policies on the semantic web The current discussion about a future Semantic Web trust architecture is focused on reputational trust mechanisms based on explicit trust ratings . What is often overlooked is the fact that , besides of ratings , huge parts of the application-specific data published on the Semantic Web are also trust relevant and therefore can be used for flexible , fine-grained trust evaluations . In this poster we propose the usage of context - and content-based trust mechanisms and outline a trust architecture which allows the formulation of subjective and task-specific trust policies as a combination of reputation - , context - and content-based trust mechanisms ."
"127","Network bucket testing Bucket testing , also known as A\/B testing , is a practice that is widely used by on-line sites with large audiences : in a simple version of the methodology , one evaluates a new feature on the site by exposing it to a very small fraction of the total user population and measuring its effect on this exposed group . For traditional uses of this technique , uniform independent sampling of the population is often enough to produce an exposed group that can serve as a statistical proxy for the full population . In on-line social network applications , however , one often wishes to perform a more complex test : evaluating a new social feature that will only produce an effect if a user and some number of his or her friends are exposed to it . In this case , independent uniform draws from the population will be unlikely to produce groups that contains users together with their friends , and so the construction of the sample must take the network structure into account . This leads quickly to challenging combinatorial problems , since there is an inherent tension between producing enough correlation to select users and their friends , but also enough uniformity and independence that the selected group is a reasonable sample of the full population . Here we develop an algorithmic framework for bucket testing in a network that addresses these challenges . First we describe a novel walk-based sampling method for producing samples of nodes that are internally well-connected but also approximately uniform over the population . Then we show how a collection of multiple independent subgraphs constructed this way can yield reasonable samples for testing . We demonstrate the effectiveness of our algorithms through computational experiments on large portions of the Facebook network ."
"128","Collaborative location and activity recommendations with GPS history data With the increasing popularity of location-based services , such as tour guide and location-based social network , we now have accumulated many location data on the Web . In this paper , we show that , by using the location data based on GPS and users ' comments at various locations , we can discover interesting locations and possible activities that can be performed there for recommendations . Our research is highlighted in the following location-related queries in our daily life : 1 ) if we want to do something such as sightseeing or food-hunting in a large city such as Beijing , where should we go ? 2 ) If we have already visited some places such as the Bird 's Nest building in Beijing 's Olympic park , what else can we do there ? By using our system , for the first question , we can recommend her to visit a list of interesting locations such as Tiananmen Square , Bird 's Nest , etc. . For the second question , if the user visits Bird 's Nest , we can recommend her to not only do sightseeing but also to experience its outdoor exercise facilities or try some nice food nearby . To achieve this goal , we first model the users ' location and activity histories that we take as input . We then mine knowledge , such as the location features and activity-activity correlations from the geographical databases and the Web , to gather additional inputs . Finally , we apply a collective matrix factorization method to mine interesting locations and activities , and use them to recommend to the users where they can visit if they want to perform some specific activities and what they can do if they visit some specific places . We empirically evaluated our system using a large GPS dataset collected by 162 users over a period of 2.5 years in the real-world . We extensively evaluated our system and showed that our system can outperform several state-of-the-art baselines ."
"129","SourceRank : relevance and trust assessment for deep web sources based on inter-source agreement One immediate challenge in searching the deep web databases is source selection - i.e. selecting the most relevant web databases for answering a given query . The existing database selection methods ( both text and relational ) assess the source quality based on the query-similarity-based relevance assessment . When applied to the deep web these methods have two deficiencies . First is that the methods are agnostic to the correctness ( trustworthiness ) of the sources . Secondly , the query based relevance does not consider the importance of the results . These two considerations are essential for the open collections like the deep web . Since a number of sources provide answers to any query , we conjuncture that the agreements between these answers are likely to be helpful in assessing the importance and the trustworthiness of the sources . We compute the agreement between the sources as the agreement of the answers returned . While computing the agreement , we also measure and compensate for possible collusion between the sources . This adjusted agreement is modeled as a graph with sources at the vertices . On this agreement graph , a quality score of a source that we call SourceRank , is calculated as the stationary visit probability of a random walk . We evaluate SourceRank in multiple domains , including sources in Google Base , with sizes up to 675 sources . We demonstrate that the SourceRank tracks source corruption . Further , our relevance evaluations show that SourceRank improves precision by 22-60 % over the Google Base and the other baseline methods . SourceRank has been implemented in a system called Factal ."
"130","Dynamic and graphical web page breakpoints Breakpoints are perhaps the quintessential feature of a de-bugger : they allow a developer to stop time and study the program state . Breakpoints are typically specified by selecting a line of source code . For large , complex , web pages with multiple developers , the relevant source line for a given user interface problem may not be known to the developer . In this paper we describe the implementation of breakpoints in dynamically created source , and on error messages , network events , DOMmutation , DOMobject property changes , and CSS style rule updates . Adding these domain-specific breakpoints to a general-purpose debugger for Javascript allows the developer to initiate the debugging process via Web page abstractions rather than lower level source code views . The breakpoints are implemented in the open source Fire-bug project , version 1.5 , for the Firefox Web browser ."
"131","HyperANF : approximating the neighborhood function of very large graphs on a budget The neighborhood function NG ( t ) of a graph G gives , for each t ∈ N , the number of pairs of nodes x , y such that y is reachable from x in less that t hops . The neighborhood function provides a wealth of information about the graph ( 10 ) ( e.g. , it easily allows one to compute its diameter ) , but it is very expensive to compute it exactly . Recently , the ANF algorithm ( 10 ) ( approximate neighborhood function ) has been proposed with the purpose of approximating NG ( t ) on large graphs . We describe a breakthrough improvement over ANF in terms of speed and scalability . Our algorithm , called HyperANF , uses the new HyperLogLog counters ( 5 ) and combines them efficiently through broadword programming ( 8 ) ; our implementation uses talk decomposition to exploit multi-core parallelism . With HyperANF , for the first time we can compute in a few hours the neighborhood function of graphs with billions of nodes with a small error and good confidence using a standard workstation . Then , we turn to the study of the distribution of the distances between reachable nodes ( that can be efficiently approximated by means of HyperANF ) , and discover the surprising fact that its index of dispersion provides a clear-cut characterisation of proper social networks vs. web graphs . We thus propose the spid ( Shortest-Paths Index of Dispersion ) of a graph as a new , informative statistics that is able to discriminate between the above two types of graphs . We believe this is the first proposal of a significant new non-local structural index for complex networks whose computation is highly scalable ."
"132","Prophiler : a fast filter for the large-scale detection of malicious web pages Malicious web pages that host drive-by-download exploits have become a popular means for compromising hosts on the Internet and , subsequently , for creating large-scale botnets . In a drive-by-download exploit , an attacker embeds a malicious script ( typically written in JavaScript ) into a web page . When a victim visits this page , the script is executed and attempts to compromise the browser or one of its plugins . To detect drive-by-download exploits , researchers have developed a number of systems that analyze web pages for the presence of malicious code . Most of these systems use dynamic analysis . That is , they run the scripts associated with a web page either directly in a real browser ( running in a virtualized environment ) or in an emulated browser , and they monitor the scripts ' executions for malicious activity . While the tools are quite precise , the analysis process is costly , often requiring in the order of tens of seconds for a single page . Therefore , performing this analysis on a large set of web pages containing hundreds of millions of samples can be prohibitive . One approach to reduce the resources required for performing large-scale analysis of malicious web pages is to develop a fast and reliable filter that can quickly discard pages that are benign , forwarding to the costly analysis tools only the pages that are likely to contain malicious code . In this paper , we describe the design and implementation of such a filter . Our filter , called Prophiler , uses static analysis techniques to quickly examine a web page for malicious content . This analysis takes into account features derived from the HTML contents of a page , from the associated JavaScript code , and from the corresponding URL . We automatically derive detection models that use these features using machine-learning techniques applied to labeled datasets . To demonstrate the effectiveness and efficiency of Prophiler , we crawled and collected millions of pages , which we analyzed for malicious behavior . Our results show that our filter is able to reduce the load on a more costly dynamic analysis tools by more than 85 % , with a negligible amount of missed malicious pages ."
"133","How much is your personal recommendation worth ? Suppose you buy a new laptop and , simply because you like it so much , you recommend it to friends , encouraging them to purchase it as well . What would be an adequate price for the vendor of the laptop to pay for your recommendation ? Personal recommendations like this are of considerable commercial interest , but unlike in sponsored search auctions there can be no truthful prices . Despite this `` lack of truthfulness '' the vendor of the product might still decide to pay you for recommendation e.g. because she wants to ( i ) provide you with an additional incentive to actually recommend her or to ( ii ) increase your satisfaction and\/or brand loyalty . This leads us to investigate a pricing scheme based on the Shapley value ( 5 ) that satisfies certain `` axioms of fairness '' . We find that it is vulnerable to manipulations and show how to overcome these difficulties using the anonymity-proof Shapley value of ( 4 ) ."
"134","Exploiting web search engines to search structured databases Web search engines often federate many user queries to relevant structured databases . For example , a product related query might be federated to a product database containing their descriptions and specifications . The relevant structured data items are then returned to the user along with web search results . However , each structured database is searched in isolation . Hence , the search often produces empty or incomplete results as the database may not contain the required information to answer the query . In this paper , we propose a novel integrated search architecture . We establish and exploit the relationships between web search results and the items in structured databases to identify the relevant structured data items for a much wider range of queries . Our architecture leverages existing search engine components to implement this functionality at very low overhead . We demonstrate the quality and efficiency of our techniques through an extensive experimental study ."
"135","How semantics make better wikis Wikis are popular collaborative hypertext authoring environments , but they neither support structured access nor information reuse . Adding semantic annotations helps to address these limitations . We present an architecture for Semantic Wikis and discuss design decisions including structured access , views , and annotation language . We present our prototype SemperWiki that implements this architecture ."
"136","Efficient resource allocation and power saving in multi-tiered systems In this paper , we present Fastrack , a parameter-free algorithm for dynamic resource provisioning that uses simple statistics to promptly distill information about changes in workload burstiness . This information , coupled with the application 's end-to-end response times and system bottleneck characteristics , guide resource allocation that shows to be very effective under a broad variety of burstiness profiles and bottleneck scenarios ."
"137","Automatic extraction of clickable structured web contents for name entity queries Today the major web search engines answer queries by showing ten result snippets , which need to be inspected by users for identifying relevant results . In this paper we investigate how to extract structured information from the web , in order to directly answer queries by showing the contents being searched for . We treat users ' search trails ( i.e. , post-search browsing behaviors ) as implicit labels on the relevance between web contents and user queries . Based on such labels we use information extraction approach to build wrappers and extract structured information . An important observation is that many web sites contain pages for name entities of certain categories ( e.g. , AOL Music contains a page for each musician ) , and these pages have the same format . This makes it possible to build wrappers from a small amount of implicit labels , and use them to extract structured information from many web pages for different name entities . We propose STRUCLICK , a fully automated system for extracting structured information for queries containing name entities of certain categories . It can identify important web sites from web search logs , build wrappers from users ' search trails , filter out bad wrappers built from random user clicks , and combine structured information from different web sites for each query . Comparing with existing approaches on information extraction , STRUCLICK can assign semantics to extracted data without any human labeling or supervision . We perform comprehensive experiments , which show STRUCLICK achieves high accuracy and good scalability ."
"138","Finding hierarchy in directed online social networks Social hierarchy and stratification among humans is a well studied concept in sociology . The popularity of online social networks presents an opportunity to study social hierarchy for different types of networks and at different scales . We adopt the premise that people form connections in a social network based on their perceived social hierarchy ; as a result , the edge directions in directed social networks can be leveraged to infer hierarchy . In this paper , we define a measure of hierarchy in a directed online social network , and present an efficient algorithm to compute this measure . We validate our measure using ground truth including Wikipedia notability score . We use this measure to study hierarchy in several directed online social networks including Twitter , Delicious , YouTube , Flickr , LiveJournal , and curated lists of several categories of people based on different occupations , and different organizations . Our experiments on different online social networks show how hierarchy emerges as we increase the size of the network . This is in contrast to random graphs , where the hierarchy decreases as the network size increases . Further , we show that the degree of stratification in a network increases very slowly as we increase the size of the graph ."
"139","Ranking community answers via analogical reasoning Due to the lexical gap between questions and answers , automatically detecting right answers becomes very challenging for community question-answering sites . In this paper , we propose an analogical reasoning-based method . It treats questions and answers as relational data and ranks an answer by measuring the analogy of its link to a query with the links embedded in previous relevant knowledge ; the answer that links in the most analogous way to the new question is assumed to be the best answer . We based our experiments on 29.8 million Yahoo ! Answer question-answer threads and showed the effectiveness of the approach ."
"140","Equip tourists with knowledge mined from travelogues With the prosperity of tourism and Web 2.0 technologies , more and more people have willingness to share their travel experiences on the Web ( e.g. , weblogs , forums , or Web 2.0 communities ) . These so-called travelogues contain rich information , particularly including location-representative knowledge such as attractions ( e.g. , Golden Gate Bridge ) , styles ( e.g. , beach , history ) , and activities ( e.g. , diving , surfing ) . The location-representative information in travelogues can greatly facilitate other tourists ' trip planning , if it can be correctly extracted and summarized . However , since most travelogues are unstructured and contain much noise , it is difficult for common users to utilize such knowledge effectively . In this paper , to mine location-representative knowledge from a large collection of travelogues , we propose a probabilistic topic model , named as Location-Topic model . This model has the advantages of ( 1 ) differentiability between two kinds of topics , i.e. , local topics which characterize locations and global topics which represent other common themes shared by various locations , and ( 2 ) representation of locations in the local topic space to encode both location-representative knowledge and similarities between locations . Some novel applications are developed based on the proposed model , including ( 1 ) destination recommendation for on flexible queries , ( 2 ) characteristic summarization for a given destination with representative tags and snippets , and ( 3 ) identification of informative parts of a travelogue and enriching such highlights with related images . Based on a large collection of travelogues , the proposed framework is evaluated using both objective and subjective evaluation methods and shows promising results ."
"141","Optimal rare query suggestion with implicit user feedback Query suggestion has been an effective approach to help users narrow down to the information they need . However , most of existing studies focused on only popular\/head queries . Since rare queries possess much less information ( e.g. , clicks ) than popular queries in the query logs , it is much more difficult to efficiently suggest relevant queries to a rare query . In this paper , we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs . Our model resembles the principle of pseudo-relevance feedback which assumes that top-returned results by search engines are relevant . However , we argue that the clicked URLs and skipped URLs contain different levels of information and thus should be treated differently . Hence , our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation . Our model specifically optimizes two parameters : ( 1 ) the restarting ( jumping ) rate of random walk , and ( 2 ) the combination ratio of click and skip information . Unlike the Rocchio algorithm , our learning process does not involve the content of the URLs but simply leverages the click and skip counts in the query-URL bipartite graphs . Consequently , our model is capable of scaling up to the need of commercial search engines . Experimental results on one-month query logs from a large commercial search engine with over 40 million rare queries demonstrate the superiority of our framework , with statistical significance , over the traditional random walk models and pseudo-relevance feedback models ."
"142","Growing parallel paths for entity-page discovery In this paper , we use the structural and relational information on the Web to find entity-pages . Specifically , given a Web site and an entity-page ( e.g. , department and faculty member homepage ) we seek to find all of the entity-pages of the same type ( e.g. , all faculty members in the department ) . To do this , we propose a web structure mining method which grows parallel paths through the web graph and DOM trees . We show that by utilizing these parallel paths we can efficiently discover all entity-pages of the same type . Finally , we demonstrate the accuracy of our method with a case study on various domains ."
"143","Learning to re-rank : query-dependent image re-ranking using click data Our objective is to improve the performance of keyword based image search engines by re-ranking their original results . To this end , we address three limitations of existing search engines in this paper . First , there is no straight-forward , fully automated way of going from textual queries to visual features . Image search engines therefore primarily rely on static and textual features for ranking . Visual features are mainly used for secondary tasks such as finding similar images . Second , image rankers are trained on query-image pairs labeled with relevance judgments determined by human experts . Such labels are well known to be noisy due to various factors including ambiguous queries , unknown user intent and subjectivity in human judgments . This leads to learning a sub-optimal ranker . Finally , a static ranker is typically built to handle disparate user queries . The ranker is therefore unable to adapt its parameters to suit the query at hand which again leads to sub-optimal results . We demonstrate that all of these problems can be mitigated by employing a re-ranking algorithm that leverages aggregate user click data . We hypothesize that images clicked in response to a query are mostly relevant to the query . We therefore re-rank the original search results so as to promote images that are likely to be clicked to the top of the ranked list . Our re-ranking algorithm employs Gaussian Process regression to predict the normalized click count for each image , and combines it with the original ranking score . Our approach is shown to significantly boost the performance of the Bing image search engine on a wide range of tail queries ."
"144","we . b : the web of short urls Short URLs have become ubiquitous . Especially popular within social networking services , short URLs have seen a significant increase in their usage over the past years , mostly due to Twitter 's restriction of message length to 140 characters . In this paper , we provide a first characterization on the usage of short URLs . Specifically , our goal is to examine the content short URLs point to , how they are published , their popularity and activity over time , as well as their potential impact on the performance of the web . Our study is based on traces of short URLs as seen from two different perspectives : i ) collected through a large-scale crawl of URL shortening services , and ii ) collected by crawling Twitter messages . The former provides a general characterization on the usage of short URLs , while the latter provides a more focused view on how certain communities use shortening services . Our analysis highlights that domain and website popularity , as seen from short URLs , significantly differs from the distributions provided by well publicised services such as Alexa . The set of most popular websites pointed to by short URLs appears stable over time , despite the fact that short URLs have a limited high popularity lifetime . Surprisingly short URLs are not ephemeral , as a significant fraction , roughly 50 % , appears active for more than three months . Overall , our study emphasizes the fact that short URLs reflect an `` alternative '' web and , hence , provide an additional view on web usage and content consumption complementing traditional measurement sources . Furthermore , our study reveals the need for alternative shortening architectures that will eliminate the non-negligible performance penalty imposed by today 's shortening services ."
"145","HyLiEn : a hybrid approach to general list extraction on the web We consider the problem of automatically extracting general lists from the web . Existing approaches are mostly dependent upon either the underlying HTML markup or the visual structure of the Web page . We present HyLiEn an unsupervised , Hybrid approach for automatic List discovery and Extraction on the Web . It employs general assumptions about the visual rendering of lists , and the structural representation of items contained in them . We show that our method significantly outperforms existing methods ."
"146","Inverted index compression via online document routing Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web . To satisfy this requirement , the Web is frequently crawled . Due to the sheer size of their indexes , search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning , such that each server indexes only several million pages . To ensure documents from the same host ( e.g. , www.nytimes.com ) are distributed uniformly over the servers , for load balancing purposes , random routing of documents to servers is common . To expedite the time documents become searchable after being crawled , documents may be simply appended to the existing index partitions . However , indexing by merely appending documents , results in larger index sizes since document reordering for index compactness is no longer performed . This , in turn , degrades search query processing performance which depends heavily on index sizes . A possible way to balance quick document indexing with efficient query processing , is to deploy online document routing strategies that are designed to reduce index sizes . This work considers the effects of several online document routing strategies on the aggregated partitioned index size . We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions ( i.e. , host distribution ) . We suggest and evaluate several online routing strategies with regard to their compression , host distribution , and complexity . In particular , we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme . In addition , our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics . Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages ."
"147","Analyzing and accelerating web access in a school in peri-urban India While computers and Internet access have growing penetration amongst schools in the developing world , intermittent connectivity and limited bandwidth often prevent them from being fully utilized by students and teachers . In this paper , we make two contributions to help address this problem . First , we characterize six weeks of HTTP traffic from a primary school outside of Bangalore , India , illuminating opportunities and constraints for improving performance in such settings . Second , we deploy an aggressive caching and prefetching engine and show that it accelerates a user 's overall browsing experience ( apart from video content ) by 2.8 x. Our accelerator leverages innovative techniques that have been proposed , but not evaluated in detail , including the effectiveness of serving stale pages , cached page highlighting , and client-side prefetching . Unlike proxy-based techniques , our system is bundled as an open-source Firefox plugin and runs directly on client machines . This allows easy installation and configuration by end users , which is especially important in developing regions where a lack of permissions or technical expertise often prevents modification of internal network settings ."
"148","Estimating sizes of social networks via biased sampling Online social networks have become very popular in recent years and their number of users is already measured in many hundreds of millions . For various commercial and sociological purposes , an independent estimate of their sizes is important . In this work , algorithms for estimating the number of users in such networks are considered . The proposed schemes are also applicable for estimating the sizes of networks ' sub-populations . The suggested algorithms interact with the social networks via their public APIs only , and rely on no other external information . Due to obvious traffic and privacy concerns , the number of such interactions is severely limited . We therefore focus on minimizing the number of API interactions needed for producing good size estimates . We adopt the abstraction of social networks as undirected graphs and use random node sampling . By counting the number of collisions or non-unique nodes in the sample , we produce a size estimate . Then , we show analytically that the estimate error vanishes with high probability for smaller number of samples than those required by prior-art algorithms . Moreover , although our algorithms are provably correct for any graph , they excel when applied to social network-like graphs . The proposed algorithms were evaluated on synthetic as well real social networks such as Facebook , IMDB , and DBLP . Our experiments corroborated the theoretical results , and demonstrated the effectiveness of the algorithms ."
"149","Visualization of Geo-annotated pictures in mobile phones In this work , a novel mobile browser for geo-referenced pictures is introduced and described . We use the term browser to denote a system aimed at browsing pictures selected from a large set like Internet photo sharing services . The criteria to filter a subset of pictures to browse are three : the user 's actual position , the user 's actual heading , and the user 's preferences . In this work we only focus on the first two criteria leaving the integration of user 's preferences for future developments ."
"150","Highly scalable web applications with zero-copy data transfer The performance of server-side applications is becoming increasingly important as more applications exploit the Web application model . Extensive work has been done to improve the performance of individual software components such as Web servers and programming language runtimes . This paper describes a novel approach to boost Web application performance by improving inter-process communication between a programming language runtime and Web server runtime . The approach reduces redundant processing for memory copying and the context switch overhead between user space and kernel space by exploiting the zero-copy data transfer methodology , such as the sendfile system call . In order to transparently utilize this optimization feature with existing Web applications , we propose enhancements of the PHP runtime , FastCGI protocol , and Web server . Our proposed approach achieves a 126 % performance improvement with micro-benchmarks and a 44 % performance improvement for a standard Web benchmark , SPECweb2005 ."
"151","Evaluating a new approach to strong web cache consistency with snapshots of collected content The problem of Web cache consistency continues to be an important one . Current Web caches use heuristic-based policies for determining the freshness of cached objects , often forcing content providers to unnecessarily mark their content as uncacheable simply to retain control over it . Server-driven invalidation has been proposed as a mechanism for providing strong cache consistency for Web objects , but it requires servers to maintain per-client state even for infrequently changing objects . We propose an alternative approach to strong cache consistency , called MONARCH , which does not require servers to maintain per-client state . In this work we focus on a new approach for evaluation of MONARCH in comparison with current practice and other cache consistency policies . This approach uses snapshots of content collected from real Web sites as input to a simulator . Results of the evaluation show MONARCH generates little more request traffic than an optimal cache coherency policy ."
"152","Smartback : supporting users in back navigation This paper presents the design and user evaluation of SmartBack , a feature that complements the standard Back button by enabling users to jump directly to key pages in their navigation session , making common navigation activities more efficient . Defining key pages was informed by the findings of a user study that involved detailed monitoring of Web usage and analysis of Web browsing in terms of navigation trails . The pages accessible through SmartBack are determined automatically based on the structure of the user 's navigation trails or page association with specific user 's activities , such as search or browsing bookmarked sites . We discuss implementation decisions and present results of a usability study in which we deployed the SmartBack prototype and monitored usage for a month in both corporate and home settings . The results show that the feature brings qualitative improvement to the browsing experience of individuals who use it ."
"153","Mining collective local knowledge from Google MyMaps The emerging popularity of location-aware devices and location-based services has generated a growing archive of digital traces of people 's activities and opinions in physical space . In this study , we leverage geo-referenced user-generated content from Google MyMaps to discover collective local knowledge and understand the differing perceptions of urban space . Working with the large collection of publicly available , annotation-rich MyMaps data , we propose a highly parallelizable approach in order to merge identical places , discover landmarks , and recommend places . Additionally , we conduct interviews with New York City residents\/visitors to validate the quantitative findings ."
"154","Find me if you can : improving geographical prediction with social and spatial proximity Geography and social relationships are inextricably intertwined ; the people we interact with on a daily basis almost always live near us . As people spend more time online , data regarding these two dimensions -- geography and social relationships -- are becoming increasingly precise , allowing us to build reliable models to describe their interaction . These models have important implications in the design of location-based services , security intrusion detection , and social media supporting local communities . Using user-supplied address data and the network of associations between members of the Facebook social network , we can directly observe and measure the relationship between geography and friendship . Using these measurements , we introduce an algorithm that predicts the location of an individual from a sparse set of located users with performance that exceeds IP-based geolocation . This algorithm is efficient and scalable , and could be run on a network containing hundreds of millions of users ."
"155","AdHeat : an influence-based diffusion model for propagating hints to match ads In this paper , we present AdHeat , a social ad model considering user influence in addition to relevance for matching ads . Traditionally , ad placement employs the relevance model . Such a model matches ads with Web page content , user interests , or both . We have observed , however , on social networks that the relevance model suffers from two shortcomings . First , influential users ( users who contribute opinions ) seldom click ads that are highly relevant to their expertise . Second , because influential users ' contents and activities are attractive to other users , hint words summarizing their expertise and activities may be widely preferred . Therefore , we propose AdHeat , which diffuses hint words of influential users to others and then matches ads for each user with aggregated hints . We performed experiments on a large online Q&A community with half a million users . The experimental results show that AdHeat outperforms the relevance model on CTR ( click through rate ) by significant margins ."
"156","Efficiently querying rdf data in triple stores Efficiently querying RDF data is being an important factor in applying Semantic Web technologies to real-world applications . In this context , many efforts have been made to store and query RDF data in relational database using particular schemas . In this paper , we propose a new scheme to store , index , and query RDF data in triple stores . Graph feature of RDF data is taken into considerations which might help reduce the join costs on the vertical database structure . We would partition RDF triples into overlapped groups , store them in a triple table with one more column of group identity , and build up a signature tree to index them . Based on this infrastructure , a complex RDF query is decomposed into multiple pieces of sub-queries which could be easily filtered into some RDF groups using signature tree index , and finally is evaluated with a composed and optimized SQL with specific constraints . We compare the performance of our method with prior art on typical queries over a large scaled LUBM and UOBM benchmark data ( more than 10 million triples ) . For some extreme cases , they can promote 3 to 4 orders of magnitude ."
"157","Meteor-s web service annotation framework The World Wide Web is emerging not only as an infrastructure for data , but also for a broader variety of resources that are increasingly being made available as Web services . Relevant current standards like UDDI , WSDL , and SOAP are in their fledgling years and form the basis of making Web services a workable and broadly adopted technology . However , realizing the fuller scope of the promise of Web services and associated service oriented architecture will requite further technological advances in the areas of service interoperation , service discovery , service composition , and process orchestration . Semantics , especially as supported by the use of ontologies , and related Semantic Web technologies , are likely to provide better qualitative and scalable solutions to these requirements . Just as semantic annotation of data in the Semantic Web is the first critical step to better search , integration and analytics over heterogeneous data , semantic annotation of Web services is an equally critical first step to achieving the above promise . Our approach is to work with existing Web services technologies and combine them with ideas from the Semantic Web to create a better framework for Web service discovery and composition . In this paper we present MWSAF ( METEOR-S Web Service Annotation Framework ) , a framework for semi-automatically marking up Web service descriptions with ontologies . We have developed algorithms to match and annotate WSDL files with relevant ontologies . We use domain ontologies to categorize Web services into domains . An empirical study of our approach is presented to help evaluate its performance ."
"158","Yago : a core of semantic knowledge We present YAGO , a light-weight and extensible ontology with high coverage and quality . YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts . This includes the Is-A hierarchy as well as non-taxonomic relations between entities ( such as HASONEPRIZE ) . The facts have been automatically extracted from Wikipedia and unified with WordNet , using a carefully designed combination of rule-based and heuristic methods described in this paper . The resulting knowledge base is a major step beyond WordNet : in quality by adding knowledge about individuals like persons , organizations , products , etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude . Our empirical evaluation of fact correctness shows an accuracy of about 95 % . YAGO is based on a logically clean model , which is decidable , extensible , and compatible with RDFS . Finally , we show how YAGO can be further extended by state-of-the-art information extraction techniques ."
"159","YAGO2 : exploring and querying world knowledge in time , space , context , and many languages We present YAGO2 , an extension of the YAGO knowledge base with focus on temporal and spatial knowledge . It is automatically built from Wikipedia , GeoNames , and WordNet , and contains nearly 10 million entities and events , as well as 80 million facts representing general world knowledge . An enhanced data representation introduces time and location as first-class citizens . The wealth of spatio-temporal information in YAGO can be explored either graphically or through a special time - and space-aware query language ."
"160","Compressing and searching XML data via two zips XML is fast becoming the standard format to store , exchange and publish over the web , and is getting embedded in applications . Two challenges in handling XML are its size ( the XML representation of a document is significantly larger than its native state ) and the complexity of its search ( XML search involves path and content searches on labeled tree structures ) . We address the basic problems of compression , navigation and searching of XML documents . In particular , we adopt recently proposed theoretical algorithms ( 11 ) for succinct tree representations to design and implement a compressed index for XML , called XBZIPiNDEX , in which the XML document is maintained in a highly compressed format , and both navigation and searching can be done uncompressing only a tiny fraction of the data . This solution relies on compressing and indexing two arrays derived from the XML data . With detailed experiments we compare this with other compressed XML indexing and searching engines to show that XBZIPiNDEX has compression ratio up to 35 % better than the ones achievable by those other tools , and its time performance on some path and content search operations is order of magnitudes faster : few milliseconds over hundreds of MBs of XML files versus tens of seconds , on standard XML data sources ."
"161","Fluid annotations through open hypermedia : using and extending emerging web standards The Fluid Documents project has developed various research prototypes that show that powerful annotation techniques based on animated typographical changes can help readers utilize annotations more effectively . Our recently-developed Fluid Open Hypermedia prototype supports the authoring and browsing of fluid annotations on third-party Web pages . This prototype is an extension of the Arakne Environment , an open hypermedia application that can augment Web pages with externally stored hypermedia structures . This paper describes how various Web standards , including DOM , CSS , XLink , XPointer , and RDF , can be used and extended to support fluid annotations ."
"162","Structured objects in owl : representation and reasoning Applications of semantic technologies often require the representation of and reasoning with structured objects - that is , objects composed of parts connected in complex ways . Although OWL is a general and powerful language , its class descriptions and axioms can not be used to describe arbitrarily connected structures . An OWL representation of structured objects can thus be underconstrained , which reduces the inferences that can be drawn and causes performance problems in reasoning . To address these problems , we extend OWL with description graphs , which allow for the description of structured objects in a simple and precise way . To represent conditional aspects of the domain , we also allow for SWRL-like rules over description graphs . Based on an observation about the nature of structured objects , we ensure decidability of our formalism . We also present a hypertableau-based decision procedure , which we implemented in the HermiT reasoner . To evaluate its performance , we have extracted description graphs from the GALEN and FMA ontologies , classified them successfully , and even detected a modeling error in GALEN ."
"163","Bridging the gap between OWL and relational databases Schema statements in OWL are interpreted quite differently from analogous statements in relational databases . If these statements are meant to be interpreted as integrity constraints ( ICs ) , OWL 's interpretation may seem confusing and\/or inappropriate . Therefore , we propose an extension of OWL with ICs that captures the intuition behind ICs in relational databases . We discuss the algorithms for checking IC satisfaction for different types of knowledge bases , and show that , if the constraints are satisfied , we can disregard them while answering a broad range of positive queries ."
"164","RDF triples in XML RDF\/XML does not layer RDF on top of XML ina useful way . We use a simple direct representation of the RDF abstract syntax in XML . We add the ability to name graphs , noting that in practice this is already widely used . We use XSLT as a general syntactic extensibility mechanism to provide human friendly macros for our syntax . This provides a simple serialization solving a persistent problem in the Semantic Web ."
"165","Choosing reputable servents in a P2P network Peer-to-peer information sharing environments are increasingly gaining acceptance on the Internet as they provide an infrastructure in which the desired information can be located and downloaded while preserving the anonymity of both requestors and providers . As recent experience with P2P environments such as Gnutella shows , anonymity opens the door to possible misuses and abuses by resource providers exploiting the network as a way to spread tampered with resources , including malicious programs , such as Trojan Horses and viruses . In this paper we propose an approach to P2P security where servents can keep track , and share with others , information about the reputation of their peers . Reputation sharing is based on a distributed polling algorithm by which resource requestors can assess the reliability of perspective providers before initiating the download . The approach nicely complements the existing P2P protocols and has a limited impact on current implementations . Furthermore , it keeps the current level of anonymity of requestors and providers , as well as that of the parties sharing their view on others ' reputations ."
"166","Named graphs , provenance and trust The Semantic Web consists of many RDF graphs nameable by URIs . This paper extends the syntax and semantics of RDF to cover such Named Graphs . This enables RDF statements that describe graphs , which is beneficial in many Semantic Web application areas . As a case study , we explore the application area of Semantic Web publishing : Named Graphs allow publishers to communicate assertional intent , and to sign their graphs ; information consumers can evaluate specific graphs using task-specific trust policies , and act on information from those Named Graphs that they accept . Graphs are trusted depending on : their content ; information about the graph ; and the task the user is performing . The extension of RDF to Named Graphs provides a formally defined framework to be a foundation for the Semantic Web trust layer ."
"167","Cooperative leases : scalable consistency maintenance in content distribution networks In this paper , we argue that cache consistency mechanisms designed for stand-alone proxies do not scale to the large number of proxies in a content distribution network and are not flexible enough to allow consistency guarantees to be tailored to object needs . To meet the twin challenges of scalability and flexibility , we introduce the notion of cooperative consistency along with a mechanism , called cooperative leases , to achieve it . By supporting & Dgr ; - consistency semantics and by using a single lease for multiple proxies , cooperative leases allows the notion of leases to be applied in a flexible , scalable manner to CDNs . Further , the approach employs application-level multicast to propagate server notifications to proxies in a scalable manner . We implement our approach in the Apache web server and the Squid proxy cache and demonstrate its efficacy using a detailed experimental evaluation . Our results show a factor of 2.5 reduction in server message overhead and a 20 % reduction in server state space overhead when compared to original leases albeit at an increased inter-proxy communication overhead ."
"168","Extracting query modifications from nonlinear SVMs When searching the WWW , users often desire results restricted to a particular document category . Ideally , a user would be able to filter results with a text classifier to minimize false positive results ; however , current search engines allow only simple query modifications . To automate the process of generating effective query modifications , we introduce a sensitivity analysis-based method for extracting rules from nonlinear support vector machines . The proposed method allows the user to specify a desired precision while attempting to maximize the recall . Our method performs several levels of dimensionality reduction and is vastly faster than searching the combination feature space ; moreover , it is very effective on real-world data ."
"169","Influence and passivity in social media The ever-increasing amount of information flowing through Social Media forces the members of these networks to compete for attention and influence by relying on other people to spread their message . A large study of information propagation within Twitter reveals that the majority of users act as passive information consumers and do not forward the content to the network . Therefore , in order for individuals to become influential they must not only obtain attention and thus be popular , but also overcome user passivity . We propose an algorithm that determines the influence and passivity of users based on their information forwarding activity . An evaluation performed with a 2.5 million user dataset shows that our influence measure is a good predictor of URL clicks , outperforming several other measures that do not explicitly take user passivity into account . We demonstrate that high popularity does not necessarily imply high influence and vice-versa ."
"170","Recommendations for the long tail by term-query graph We define a new approach to the query recommendation problem . In particular , our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries . In other words we are targeting queries in the long tail . The model is based on a graph having two sets of nodes : Term nodes , and Query nodes . The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes , moves along Query nodes , and restarts ( with a given probability ) only from the same initial subset of Term nodes . Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself . Given a query , we extract its terms and we set the restart subset to this term set . Therefore , we do not require a query to have been previously observed for the recommending model to be able to generate suggestions ."
"171","An xpath-based discourse analysis module for spoken dialogue systems This paper describes an XPath-based discourse analysis module for Spoken Dialogue Systems that allows the dialogue author to easily manipulate and query both the user input 's semantic representation and the dialogue context using a simple and compact formalism . We show that , in managing the human-machine interaction , the discourse context and the dialogue history are effectively represented as Document Object Model ( DOM ) structures . DOM defines interfaces that dialogue scripts can use to dynamically access and update the content , the structure and the style of the documents . In general , this approach applies also to richer multimedia and multimodal interactions where the interpretation of the user input depends on a combination of input modalitie ."
"172","Anonymizing user profiles for personalized web search We study the problem of anonymizing user profiles so that user privacy is sufficiently protected while the anonymized profiles are still effective in enabling personalized web search . We propose a Bayes-optimal privacy notion to bound the prior and posterior probability of associating a user with an individual term in the anonymized user profile set . We also propose a novel bundling technique that clusters user profiles into groups by taking into account the semantic relationships between the terms while satisfying the privacy constraint . We evaluate our approach through a set of preliminary experiments using real data demonstrating its feasibility and effectiveness ."
"173","Context-sensitive query auto-completion Query auto completion is known to provide poor predictions of the user 's query when her input prefix is very short ( e.g. , one or two characters ) . In this paper we show that context , such as the user 's recent queries , can be used to improve the prediction quality considerably even for such short prefixes . We propose a context-sensitive query auto completion algorithm , NearestCompletion , which outputs the completions of the user 's input that are most similar to the context queries . To measure similarity , we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity . The mapping from queries to vectors is done through a new query expansion technique that we introduce , which expands a query by traversing the query recommendation tree rooted at the query . In order to evaluate our approach , we performed extensive experimentation over the public AOL query log . We demonstrate that when the recent user 's queries are relevant to the current query she is typing , then after typing a single character , NearestCompletion 's MRR is 48 % higher relative to the MRR of the standard MostPopularCompletion algorithm on average . When the context is irrelevant , however , NearestCompletion 's MRR is essentially zero . To mitigate this problem , we propose HybridCompletion , which is a hybrid of NearestCompletion with MostPopularCompletion . HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion , achieving a total improvement of 31.5 % in MRR relative to MostPopularCompletion on average ."
"174","Unsupervised query segmentation using only query logs We introduce an unsupervised query segmentation scheme that uses query logs as the only resource and can effectively capture the structural units in queries . We believe that Web search queries have a unique syntactic structure which is distinct from that of English or a bag-of-words model . The segments discovered by our scheme help understand this underlying grammatical structure . We apply a statistical model based on Hoeffding 's Inequality to mine significant word n-grams from queries and subsequently use them for segmenting the queries . Evaluation against manually segmented queries shows that this technique can detect rare units that are missed by our Pointwise Mutual Information ( PMI ) baseline ."
"175","The web of topics : discovering the topology of topic evolution in a corpus In this paper we study how to discover the evolution of topics over time in a time-stamped document collection . Our approach is uniquely designed to capture the rich topology of topic evolution inherent in the corpus . Instead of characterizing the evolving topics at fixed time points , we conceptually define a topic as a quantized unit of evolutionary change in content and discover topics with the time of their appearance in the corpus . Discovered topics are then connected to form a topic evolution graph using a measure derived from the underlying document network . Our approach allows inhomogeneous distribution of topics over time and does not impose any topological restriction in topic evolution graphs . We evaluate our algorithm on the ACM corpus . The topic evolution graphs obtained from the ACM corpus provide an effective and concrete summary of the corpus with remarkably rich topology that are congruent to our background knowledge . In a finer resolution , the graphs reveal concrete information about the corpus that were previously unknown to us , suggesting the utility of our approach as a navigational tool for the corpus ."
"176","Ranking related entities for web search queries Entity ranking is a recent paradigm that refers to retrieving and ranking related objects and entities from different structured sources in various scenarios . Entities typically have associated categories and relationships with other entities . In this work , we present an extensive analysis of Web-scale entity ranking , based on machine learned ranking models using an ensemble of pairwise preference models . Our proposed system for entity ranking uses structured knowledge bases , entity relationship graphs and user data to derive useful features to facilitate semantic search with entities directly within the learning to rank framework . The experimental results are validated on a large-scale graph containing millions of entities and hundreds of millions of entity relationships . We show that our proposed ranking solution clearly improves a simple user behavior based ranking model ."
"177","Efficient diversification of search results using query logs We study the problem of diversifying search results by exploiting the knowledge mined from query logs . Our proposal exploits the presence of different `` specializations '' of queries in query logs to detect the submission of ambiguous\/faceted queries , and manage them by diversifying the search results returned in order to cover the different possible interpretations of the query . We present an original formulation of the results diversification problem in terms of an objective function to be maximized that admits the finding of an optimal solution in linear time ."
"178","Layered label propagation : a multiresolution coordinate-free ordering for compressing social networks We continue the line of research on graph compression started with WebGraph , but we move our focus to the compression of social networks in a proper sense ( e.g. , LiveJournal ) : the approaches that have been used for a long time to compress web graphs rely on a specific ordering of the nodes ( lexicographical URL ordering ) whose extension to general social networks is not trivial . In this paper , we propose a solution that mixes clusterings and orders , and devise a new algorithm , called Layered Label Propagation , that builds on previous work on scalable clustering and can be used to reorder very large graphs ( billions of nodes ) . Our implementation uses task decomposition to perform aggressively on multi-core architecture , making it possible to reorder graphs of more than 600 millions nodes in a few hours . Experiments performed on a wide array of web graphs and social networks show that combining the order produced by the proposed algorithm with the WebGraph compression framework provides a major increase in compression with respect to all currently known techniques , both on web graphs and on social networks . These improvements make it possible to analyze in main memory significantly larger graphs ."
"179","Leveraging auxiliary text terms for automatic image annotation This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms . First , the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks . Second , automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts , which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms . The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks . Finally , a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list . Our experiments on a large-scale database of web pages have provided very positive results ."
"180","Accelerating instant question search with database techniques Distributed question answering services , like Yahoo Answer and Aardvark , are known to be useful for end users and have also opened up numerous topics ranging in many research fields . In this paper , we propose a user-support tool for composing questions in such services . Our system incrementally recommends similar questions while users are typing their question in a sentence , which gives the users opportunities to know that there are similar questions that have already been solved . A question database is semantically analyzed and searched in the semantic space by boosting the performance of similarity searches with database techniques such as server\/client caching and LSH ( Locality Sensitive Hashing ) . The more text the user enters , the more similar the recommendations will become to the ultimately desired question . This unconscious editing-as-a-sequence-of-searches approach helps users to form their question incrementally through interactive supplementary information . Not only askers nor repliers , but also service providers have advantages such as that the knowledge of the service will be autonomously refined by avoiding for novice users to repeat questions which have been already solved ."
"181","EP-SPARQL : a unified language for event processing and stream reasoning Streams of events appear increasingly today in various Web applications such as blogs , feeds , sensor data streams , geospatial information , on-line financial data , etc. . Event Processing ( EP ) is concerned with timely detection of compound events within streams of simple events . State-of-the-art EP provides on-the-fly analysis of event streams , but can not combine streams with background knowledge and can not perform reasoning tasks . On the other hand , semantic tools can effectively handle background knowledge and perform reasoning thereon , but can not deal with rapidly changing data provided by event streams . To bridge the gap , we propose Event Processing SPARQL ( EP-SPARQL ) as a new language for complex events and Stream Reasoning . We provide syntax and formal semantics of the language and devise an effective execution model for the proposed formalism . The execution model is grounded on logic programming , and features effective event processing and inferencing capabilities over temporal and static knowledge . We provide an open-source prototype implementation and present a set of tests to show the usefulness and effectiveness of our approach ."
"182","Dynamics of bidding in a P2P lending service : effects of herding and predicting loan success Online peer-to-peer ( P2P ) lending services are a new type of social platform that enables individuals borrow and lend money directly from one to another . In this paper , we study the dynamics of bidding behavior in a P2P loan auction website , Prosper.com . We investigate the change of various attributes of loan requesting listings over time , such as the interest rate and the number of bids . We observe that there is herding behavior during bidding , and for most of the listings , the numbers of bids they receive reach spikes at very similar time points . We explain these phenomena by showing that there are economic and social factors that lenders take into account when deciding to bid on a listing . We also observe that the profits the lenders make are tied with their bidding preferences . Finally , we build a model based on the temporal progression of the bidding , that reliably predicts the success of a loan request listing , as well as whether a loan will be paid back or not ."
"183","Standing on the shoulders of ants : stigmergy in the web Stigmergy is a biological term used when discussing insect or swarm behavior , and describes a model supporting environmental communication separately from artefacts or agents . This phenomenon is demonstrated in the behavior of ants and their food gathering process when following pheromone trails , or similarly termites and their termite mound building process . What is interesting with this mechanism is that highly organized societies are achieved without an apparent management structure . Stigmergic behavior is implicit in the Web where the volume of users provides a self-organizing and self-contextualization of content in sites which facilitate collaboration . However , the majority of content is generated by a minority of the Web participants . A significant contribution from this research would be to create a model of Web stigmergy , identifying virtual pheromones and their importance in the collaborative process . This paper explores how exploiting stigmergy has the potential of providing a valuable mechanism for identifying and analyzing online user behavior recording actionable knowledge otherwise lost in the existing web interaction dynamics . Ultimately this might assist our building better collaborative Web sites ."
"184","CoSi : context-sensitive keyword query interpretation on RDF databases The demo will present CoSi , a system that enables context-sensitive interpretation of keyword queries on RDF databases . The techniques for representing , managing and exploiting query history are central to achieving this objective . The demonstration will show the effectiveness of our approach for capturing a user 's querying context from their query history . Further , it will show how context is utilized to influence the interpretation of a new query . The demonstration is based on DBPedia , the RDF representation of Wikipedia ."
"185","Measurement and analysis of cyberlocker services Cyberlocker Services ( CLS ) such as RapidShare and Megaupload have recently become popular . The decline of Peer-to-Peer ( P2P ) file sharing has prompted various services including CLS to replace it . We propose a comprehensive multi-level characterization of the CLS ecosystem . We answer three research questions : ( a ) what is a suitable measurement infrastructure for gathering CLS workloads ; ( b ) what are the characteristics of the CLS ecosystem ; and ( c ) what are the implications of CLS on Web 2.0 ( and the Internet ) . To the best of our knowledge , this work is the first to characterize the CLS ecosystem . The work will highlight the content , usage , performance , infrastructure , quality of service , and evolution characteristics of CLS ."
"186","EntityTagger : automatically tagging entities with descriptive phrases We consider the problem of entity tagging : given one or more named entities from a specific domain , the goal is to automatically associate descriptive phrases , referred to as etags ( entity tags ) , to each entity . Consider a product catalog containing product names and possibly short descriptions . For a product in the catalog , say Ricoh G600 Digital Camera , we want to associate etags such as `` water resistant '' , `` rugged '' and `` outdoor '' to it , even though its name or description does not mention those phrases . Entity tagging can enable more effective search over entities . We propose to leverage signals in web documents to perform such tagging . We develop techniques to perform such tagging in a domain independent manner while ensuring high precision and high recall ."
"187","A self organizing document map algorithm for large scale hyperlinked data inspired by neuronal migration Web document clustering is one of the research topics that is being pursued continuously due to the large variety of applications . Since Web documents usually have variety and diversity in terms of domains , content and quality , one of the technical difficulties is to find a reasonable number and size of clusters . In this research , we pay attention to SOMs ( Self Organizing Maps ) because of their capability of visualized clustering that helps users to investigate characteristics of data in detail . The SOM is widely known as a `` scalable '' algorithm because of its capability to handle large numbers of records . However , it is effective only when the vectors are small and dense . Although several research efforts on making the SOM scalable have been conducted , technical issues on scalability and performance for sparse high-dimensional data such as hyperlinked documents still remain . In this paper , we introduce MIGSOM , an SOM algorithm inspired by a recent discovery on neuronal migration . The two major advantages of MIGSOM are its scalability for sparse high-dimensional data and its clustering visualization functionality . In this paper , we describe the algorithm and implementation , and show the practicality of the algorithm by applying MIGSOM to a huge scale real data set : Wikipedia 's hyperlink data ."
"188","Enhancing web search with entity intent Web entities , such as documents and hyperlinks , are created for different purposes , or intents . Existing intent-based retrieval methods largely focus on information seekers ' intent expressed by queries , ignoring the other side of the problem : web content creators ' intent . We argue that understanding why the content was created is also important . In this work , we propose to classify such intents into two broad categories : `` navigational '' and `` informational '' . Then we incorporate such intents into traditional retrieval models , and show their effect on ranking performance ."
"189","SCAD : collective discovery of attribute values Search engines today offer a rich user experience , no longer restricted to `` ten blue links '' . For example , the query `` Canon EOS Digital Camera '' returns a photo of the digital camera , and a list of suitable merchants and prices . Similar results are offered in other domains like food , entertainment , travel , etc. . All these experiences are fueled by the availability of structured data about the entities of interest . To obtain this structured data , it is necessary to solve the following problem : given a category of entities with its schema , and a set of Web pages that mention and describe entities belonging to the category , build a structured representation for the entity under the given schema . Specifically , collect structured numerical or discrete attributes of the entities . Most previous approaches regarded this as an information extraction problem on individual documents , and made no special use of numerical attributes . In contrast , we present an end-to-end framework which leverages signals not only from the Web page context , but also from a collective analysis of all the pages corresponding to an entity , and from constraints related to the actual values within the domain . Our current implementation uses a general and flexible Integer Linear Program ( ILP ) to integrate all these signals into holistic decisions over all attributes . There is one ILP per entity and it is small enough to be solved in under 38 milliseconds in our experiments . We apply the new framework to a setting of significant practical importance : catalog expansion for Commerce search engines , using data from Bing Shopping . Finally , we present experiments that validate the effectiveness of the framework and its superiority to local extraction ."
"190","Open and decentralized access across location-based services Users now interact with multiple Location-Based Services ( LBS ) through a myriad set of location-aware devices and interfaces . However , current LBS tend to be centralized silos with ad-hoc APIs , which limits potential for information sharing and reuse . Further , LBS subscriptions and user experiences are not easily portable across devices . We propose a general architecture for providing open and decentralized access to LBS , based on Tiled Feeds - a RESTful protocol for access and interactions with LBS using feeds , and Feed Subscription Management ( FSM ) - a generalized feed-based service management protocol . We describe two client designs , and demonstrate how they enable standardized access to LBS services , promote information sharing and mashup creation , and offer service management across various types of location-enabled devices ."
"191","Game theoretic models for social network analysis The existing methods and techniques for social network analysis are inadequate to capture both the behavior ( such as rationality and intelligence ) of individuals and the strategic interactions that occur among these individuals . Game theory is a natural tool to overcome this inadequacy since it provides rigorous mathematical models of strategic interaction among autonomous , intelligent , and rational agents . Motivated by the above observation , this tutorial provides the conceptual underpinnings of the use of game theoretic models in social network analysis . In the first part of the tutorial , we provide rigorous foundations of relevant concepts in game theory and social network analysis . In the second part of the tutorial , we present a comprehensive study of four contemporary and pertinent problems in social networks : social network formation , determining in influential individuals for viral marketing , query incentive networks , and community detection ."
"192","Comparative study of clustering techniques for short text documents We compare various document clustering techniques including K-means , SVD-based method and a graph-based approach and their performance on short text data collected from Twitter . We define a measure for evaluating the cluster error with these techniques . Observations show that graph-based approach using affinity propagation performs best in clustering short text data with minimal cluster error ."
"193","Pay as you browse : microcomputations as micropayments in web-based services Currently , several online businesses deem that advertising revenues alone are not sufficient to generate profits and are therefore set to charge for online content . In this paper , we explore a complement to the current advertisement model ; more specifically , we propose a micropayment model for non-specialized commodity web-services based on microcomputations . In our model , a user that wishes to access online content offered by a website does not need to register or pay to access the website ; instead , he will accept to run microcomputations on behalf of the website in exchange for access to the content . These microcomputations can , for example , support ongoing computing projects that have clear social benefits ( e.g. , projects relating to HIV , dengue , cancer , etc. ) or can contribute towards commercial computing projects . We argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing frameworks ( e.g. , the BOINC platform ) . We implement a preliminary prototype of a system based on our model through which we evaluate its performance and usability . Finally , we analyze the security and privacy of our proposal and we show that it ensures payment for the content while preserving the privacy of users ."
"194","Towards liquid service oriented architectures The advent of Cloud computing platforms , and the growing pervasiveness of Multicore processor architectures have revealed the inadequateness of traditional programming models based on sequential computations , opening up many challenges for research on parallel programming models for building distributed , service-oriented systems . More in detail , the dynamic nature of Cloud computing and its virtualized infrastructure pose new challenges in term of application design , deployment and dynamic reconfiguration . An application developed to be delivered as a service in the Cloud has to deal with poorly understood issues such as elasticity , infinite scalability and portability across heterogeneous virtualized environments . In this position paper we define the problem of providing a novel parallel programming model for building application services that can be transparently deployed on multicore and cloud execution environments . To this end , we introduce and motivate a research plan for the definition of a novel programming framework for Web service-based applications . Our vision called `` Liquid Architecture '' is based on a programming model inspired by core ideas tied to the REST architectural style coupled with a self-configuring runtime that allows transparent deployment of Web services on a broad range of heterogeneous platforms , from multicores to clouds ."
"195","Geographical topic discovery and comparison This paper studies the problem of discovering and comparing geographical topics from GPS-associated documents . GPS-associated documents become popular with the pervasiveness of location-acquisition technologies . For example , in Flickr , the geo-tagged photos are associated with tags and GPS locations . In Twitter , the locations of the tweets can be identified by the GPS locations from smart phones . Many interesting concepts , including cultures , scenes , and product sales , correspond to specialized geographical distributions . In this paper , we are interested in two questions : ( 1 ) how to discover different topics of interests that are coherent in geographical regions ? ( 2 ) how to compare several topics across different geographical locations ? To answer these questions , this paper proposes and compares three ways of modeling geographical topics : location-driven model , text-driven model , and a novel joint model called LGTA ( Latent Geographical Topic Analysis ) that combines location and text . To make a fair comparison , we collect several representative datasets from Flickr website including Landscape , Activity , Manhattan , National park , Festival , Car , and Food . The results show that the first two methods work in some datasets but fail in others . LGTA works well in all these datasets at not only finding regions of interests but also providing effective comparisons of the topics across different locations . The results confirm our hypothesis that the geographical distributions can help modeling topics , while topics provide important cues to group different geographical regions ."
"196","Factal : integrating deep web based on trust and relevance We demonstrate `` Factal '' -- a system for integrating deep web sources . Factal is based on the recently introduced source selection method `` SourceRank '' ; which is a measure of trust and relevance based on the agreement between the sources . SourceRank selects popular and trustworthy sources from autonomous and open collections like the deep web . This trust and popularity awareness distinguishes Factal from the existing systems like Google Product Search . Factal selects and searches active online databases on multiple domains . The demonstration scenarios include improved trustworthiness , relevance of results , and comparison shopping . We believe that by incorporating effective source selection based on the SourceRank , Factal demonstrates a significant step towards a deep-web-scale integration system ."
"197","Automatic construction of a context-aware sentiment lexicon : an optimization approach The explosion of Web opinion data has made essential the need for automatic tools to analyze and understand people 's sentiments toward different topics . In most sentiment analysis applications , the sentiment lexicon plays a central role . However , it is well known that there is no universally optimal sentiment lexicon since the polarity of words is sensitive to the topic domain . Even worse , in the same domain the same word may indicate different polarities with respect to different aspects . For example , in a laptop review , `` large '' is negative for the battery aspect while being positive for the screen aspect . In this paper , we focus on the problem of learning a sentiment lexicon that is not only domain specific but also dependent on the aspect in context given an unlabeled opinionated text collection . We propose a novel optimization framework that provides a unified and principled way to combine different sources of information for learning such a context-dependent sentiment lexicon . Experiments on two data sets ( hotel reviews and customer feedback surveys on printers ) show that our approach can not only identify new sentiment words specific to the given domain but also determine the different polarities of a word depending on the aspect in context . In further quantitative evaluation , our method is proved to be effective in constructing a high quality lexicon by comparing with a human annotated gold standard . In addition , using the learned context-dependent sentiment lexicon improved the accuracy in an aspect-level sentiment classification task ."
"198","A word at a time : computing word relatedness using temporal semantic analysis Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search , clustering , and disambiguation . Previous approaches to computing semantic relatedness mostly used static language resources , while essentially ignoring their temporal aspects . We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time . Consider , for instance , a newspaper archive spanning many years . Two words such as `` war '' and `` peace '' might rarely co-occur in the same articles , yet their patterns of use over time might be similar . In this paper , we propose a new semantic relatedness model , Temporal Semantic Analysis ( TSA ) , which captures this temporal information . The previous state of the art method , Explicit Semantic Analysis ( ESA ) , represented word semantics as a vector of concepts . TSA uses a more refined representation , where each concept is no longer scalar , but is instead represented as time series over a corpus of temporally-ordered documents . To the best of our knowledge , this is the first attempt to incorporate temporal evidence into models of semantic relatedness . Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks ."
"199","We know who you followed last summer : inferring social link creation times in twitter Understanding a network 's temporal evolution appears to require multiple observations of the graph over time . These often expensive repeated crawls are only able to answer questions about what happened from observation to observation , and not what happened before or between network snapshots . Contrary to this picture , we propose a method for Twitter 's social network that takes a single static snapshot of network edges and user account creation times to accurately infer when these edges were formed . This method can be exact in theory , and we demonstrate empirically for a large subset of Twitter relationships that it is accurate to within a few hours in practice . We study users who have a very large number of edges or who are recommended by Twitter . We examine the graph formed by these nearly 1,800 Twitter celebrities and their 862 million edges in detail , showing that a single static snapshot can give novel insights about Twitter 's evolution . We conclude from this analysis that real-world events and changes to Twitter 's interface for recommending users strongly influence network growth ."
"200","Characterizing search intent diversity into click models Modeling a user 's click-through behavior in click logs is a challenging task due to the well-known position bias problem . Recent advances in click models have adopted the examination hypothesis which distinguishes document relevance from position bias . In this paper , we revisit the examination hypothesis and observe that user clicks can not be completely explained by relevance and position bias . Specifically , users with different search intents may submit the same query to the search engine but expect different search results . Thus , there might be a bias between user search intent and the query formulated by the user , which can lead to the diversity in user clicks . This bias has not been considered in previous works such as UBM , DBN and CCM . In this paper , we propose a new intent hypothesis as a complement to the examination hypothesis . This hypothesis is used to characterize the bias between the user search intent and the query in each search session . This hypothesis is very general and can be applied to most of the existing click models to improve their capacities in learning unbiased relevance . Experimental results demonstrate that after adopting the intent hypothesis , click models can better interpret user clicks and achieve a significant NDCG improvement ."
"201","Sentence-level contextual opinion retrieval Existing opinion retrieval techniques do not provide context-dependent relevant results . Most of the approaches used by state-of-the-art techniques are based on frequency of query terms , such that all documents containing query terms are retrieved , regardless of contextual relevance to the intent of the human seeking the opinion . However , in a particular opinionated document , words could occur in different contexts , yet meet the frequency attached to a certain opinion threshold , thus explicitly creating a bias in overall opinion retrieved . In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism . Model evaluation performed between our contextual model , BM25 , and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval ."
"202","Statically locating web application bugs caused by asynchronous calls Ajax becomes more and more important for web applications that care about client side user experience . It allows sending requests asynchronously , without blocking clients from continuing execution . Callback functions are only executed upon receiving the responses . While such mechanism makes browsing a smooth experience , it may cause severe problems in the presence of unexpected network latency , due to the non-determinism of asynchronism . In this paper , we demonstrate the possible problems caused by the asynchronism and propose a static program analysis to automatically detect such bugs in web applications . As client side Ajax code is often wrapped in server-side scripts , we also develop a technique that extracts client-side JavaScript code from server-side scripts . We evaluate our technique on a number of real-world web applications . Our results show that it can effectively identify real bugs . We also discuss possible ways to avoid such bugs ."
"203","Embedding MindMap as a service for user-driven composition of web applications The World Wide Web is evolving towards a very large distributed platform allowing ubiquitous access to a wide range of Web applications with minimal delay and no installation required . Such Web applications range from having users undertake simple tasks , such as filling a form , to more complex tasks including collaborative work , project management , and more generally , creating , consulting , annotating , and sharing Web content . However , users are lacking a simple but yet powerful mechanism to compose Web applications , similarly to what desktop environments allowed for decades using the file explorer paradigm and the desktop metaphor . Attempts have been made to adapt the desktop metaphor to the Web environment giving birth to Webtops ( Web desktops ) . It essentially consisted of embedding a desktop environment in a Web browser and provide access to various Web applications within the same User Interface . However , those attempts did not take into consideration to the radical differences between Web and desktop environments and applications . In this work , we introduce a new approach for Web application composition based on the mindmap metaphor . It allows browsing artifacts ( Web resources ) and enabling user-driven composition of their associated Web applications . Essentially , a mindmap is a graph of widgets representing artifacts created or used by Web applications and allow to list and launch all possible Web applications associated to each artifact . A tool has been developed to experiment the new metaphor and is provided as a service to be embedded in Web applications via a Web browser 's plug-in . We demonstrate in this paper three case studies regarding the DBLP Web site , Wikipedia and Google Picasa Web applications ."
"204","Parallel boosted regression trees for web search ranking Gradient Boosted Regression Trees ( GBRT ) are the current state-of-the-art learning paradigm for machine learned web-search ranking - a domain notorious for very large data sets . In this paper , we propose a novel method for parallelizing the training of GBRT . Our technique parallelizes the construction of the individual regression trees and operates using the master-worker paradigm as follows . The data are partitioned among the workers . At each iteration , the worker summarizes its data-partition using histograms . The master processor uses these to build one layer of a regression tree , and then sends this layer to the workers , allowing the workers to build histograms for the next layer . Our algorithm carefully orchestrates overlap between communication and computation to achieve good performance . Since this approach is based on data partitioning , and requires a small amount of communication , it generalizes to distributed and shared memory machines , as well as clouds . We present experimental results on both shared memory machines and clusters for two large scale web search ranking data sets . We demonstrate that the loss in accuracy induced due to the histogram approximation in the regression tree creation can be compensated for through slightly deeper trees . As a result , we see no significant loss in accuracy on the Yahoo data sets and a very small reduction in accuracy for the Microsoft LETOR data . In addition , on shared memory machines , we obtain almost perfect linear speed-up with up to about 48 cores on the large data sets . On distributed memory machines , we get a speedup of 25 with 32 processors . Due to data partitioning our approach can scale to even larger data sets , on which one can reasonably expect even higher speedups ."
"205","The freshman handbook : a hint for the server placement of social networks There has been a recent unprecedented increase in the use of Online Social Networks ( OSNs ) to expand our social life , exchange information and share common interests . Many popular OSNs today attract hundreds of millions of users who share tremendous amount of data on it such as Facebook , Twitter , and Buzz . Given the huge business opportunities OSNs may bring , more and more new social applications has emerged on the Internet . For these newcomers in the social network business , one of the first key decisions to make is to where to deploy the computational resources to best accommodate future client requests . In this work , we aim at providing useful suggests to the new born social network providers ( freshman ) on the intelligent server placement , by exploring available public information from existing social network communities . In this work , we first propose three scalable server placement strategies for OSNs . Our solution can scalably select server locations among all the possible locations , at the same time reducing the cost for inter-user data sharing ."
"206","Detecting group review spam It is well-known that many online reviews are not written by genuine users of products , but by spammers who write fake reviews to promote or demote some target products . Although some existing works have been done to detect fake reviews and individual spammers , to our knowledge , no work has been done on detecting spammer groups . This paper focuses on this task and proposes an effective technique to detect such groups ."
"207","Scalable integration and processing of linked data The goal of this tutorial is to introduce , motivate and detail techniques for integrating heterogeneous structured data from across the Web . Inspired by the growth in Linked Data publishing , our tutorial aims at educating Web researchers and practitioners about this new publishing paradigm . The tutorial will show how Linked Data enables uniform access , parsing and interpretation of data , and how this novel wealth of structured data can potentially be exploited for creating new applications or enhancing existing ones . As such , the tutorial will focus on Linked Data publishing and related Semantic Web technologies , introducing scalable techniques for crawling , indexing and automatically integrating structured heterogeneous Web data through reasoning ."
"208","Mobile search pattern evolution : the trend and the impact of voice queries In this paper we study the characteristics of search queries submitted from mobile devices using Yahoo ! Search for Mobile during a 2 months period in early of 2010 , and compare the results with a similar study conducted in late 2007 . The major findings include 1 ) mobile search queries have become much more diverse , and 2 ) user interest and information needs have been substantially changed at least in some areas of search topics , including adult and local intent queries . In addition we investigate the impact of voice query search interface offered by Yahoo ! 's mobile search service . We examine how unstructured spoken queries differ from conventional search queries ."
"209","Hierarchical organization of unstructured consumer reviews In this paper , we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews . Based on the derived hierarchy , we generate a hierarchical organization of the consumer reviews based on various aspects of the product , and aggregate consumer opinions on the aspects . With such hierarchical organization , people can easily grasp the overview of consumer reviews and opinions on various aspects , as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy . We conduct evaluation on two product review data sets : Liu et al. 's data set containing 314 reviews for five products ( 2 ) , and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products . The experimental results demonstrate the effectiveness of our approach ."
"210","Finding influential mediators in social networks Given a social network , who are the key players controlling the bottlenecks of influence propagation if some persons would like to activate specific individuals ? In this paper , we tackle the problem of selecting a set of k mediator nodes as the influential gateways whose existence determines the activation probabilities of targeted nodes from some given seed nodes . We formally define the k-Mediators problem . To have an effective and efficient solution , we propose a three-step greedy method by considering the probabilistic influence and the structural connectivity on the pathways from sources to targets . To the best of our knowledge , this is the first work to consider the k-Mediators problem in networks . Experiments on the DBLP co-authorship graph show the effectiveness and efficiency of the proposed method ."
"211","Domain-independent entity extraction from web search query logs Query logs of a Web search engine have been increasingly used as a vital source for data mining . This paper presents a study on large-scale domain-independent entity extraction from search query logs . We present a completely unsupervised method to extract entities by applying pattern-based heuristics and statistical measures . We compare against existing techniques that use Web documents as well as search logs , and show that we improve over the state of the art . We also provide an in-depth qualitative analysis outlining differences and commonalities between these methods ."
"212","Track globally , deliver locally : improving content delivery networks by tracking geographic social cascades Providers such as YouTube offer easy access to multimedia content to millions , generating high bandwidth and storage demand on the Content Delivery Networks they rely upon . More and more , the diffusion of this content happens on online social networks such as Facebook and Twitter , where social cascades can be observed when users increasingly repost links they have received from others . In this paper we describe how geographic information extracted from social cascades can be exploited to improve caching of multimedia files in a Content Delivery Network . We take advantage of the fact that social cascades can propagate in a geographically limited area to discern whether an item is spreading locally or globally . This informs cache replacement policies , which utilize this information to ensure that content relevant to a cascade is kept close to the users who may be interested in it . We validate our approach by using a novel dataset which combines social interaction data with geographic information : we track social cascades of YouTube links over Twitter and build a proof-of-concept geographic model of a realistic distributed Content Delivery Network . Our performance evaluation shows that we are able to improve cache hits with respect to cache policies without geographic and social information ."
"213","Filtering microblogging messages for social tv Social TV was named one of the ten most important emerging technologies in 2010 by the MIT Technology Review . Manufacturers of set-top boxes and televisions have recently started to integrate access to social networks into their products . Some of these systems allow users to read microblogging messages related to the TV program they are currently watching . However , such systems suffer from low precision and recall when they use the title of the show as keywords when retrieving messages , without any additional filtering . We propose a bootstrapping approach to collecting microblogging messages related to a given TV program . We start with a small set of annotated data , in which , for a given show and a candidate message , we annotate the pair to be relevant or irrelevant . From this annotated data set , we train an initial classifier . The features are designed to capture the association between the TV program and the message . Using our initial classifier and a large dataset of unlabeled messages we derive broader features for a second classifier to further improve precision ."
"214","Learning to rank with multiple objective functions We investigate the problem of learning to rank with document retrieval from the perspective of learning for multiple objective functions . We present solutions to two open problems in learning to rank : first , we show how multiple measures can be combined into a single graded measure that can be learned . This solves the problem of learning from a ` scorecard ' of measures by making such scorecards comparable , and we show results where a standard web relevance measure ( NDCG ) is used for the top-tier measure , and a relevance measure derived from click data is used for the second-tier measure ; the second-tier measure is shown to significantly improve while leaving the top-tier measure largely unchanged . Second , we note that the learning-to-rank problem can itself be viewed as changing as the ranking model learns : for example , early in learning , adjusting the rank of all documents can be advantageous , but later during training , it becomes more desirable to concentrate on correcting the top few documents for each query . We show how an analysis of these problems leads to an improved , iteration-dependent cost function that interpolates between a cost function that is more appropriate for early learning , with one that is more appropriate for late-stage learning . The approach results in a significant improvement in accuracy with the same size models . We investigate these ideas using LambdaMART , a state-of-the-art ranking algorithm ."
"215","Information credibility on twitter We analyze the information credibility of news propagated through Twitter , a popular microblogging service . Previous research has shown that most of the messages posted on Twitter are truthful , but the service is also used to spread misinformation and false rumors , often unintentionally . On this paper we focus on automatic methods for assessing the credibility of a given set of tweets . Specifically , we analyze microblog postings related to `` trending '' topics , and classify them as credible or not credible , based on features extracted from them . We use features from users ' posting and re-posting ( `` re-tweeting '' ) behavior , from the text of the posts , and from citations to external sources . We evaluate our methods using a significant number of human assessments about the credibility of items on a recent sample of Twitter postings . Our results shows that there are measurable differences in the way messages propagate , that can be used to classify them automatically as credible or not credible , with precision and recall in the range of 70 % to 80 % ."
"216","Towards a theory model for product search With the growing pervasiveness of the Internet , online search for products and services is constantly increasing . Most product search engines are based on adaptations of theoretical models devised for information retrieval . However , the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects . We propose a theory model for product search based on expected utility theory from economics . Specifically , we propose a ranking technique in which we rank highest the products that generate the highest surplus , after the purchase . In a sense , the top ranked products are the `` best value for money '' for a specific user . Our approach builds on research on `` demand estimation '' from economics and presents a solid theoretical foundation on which further research can build on . We build algorithms that take into account consumer demographics , heterogeneity of consumer preferences , and also account for the varying price of the products . We show how to achieve this without knowing the demographics or purchasing histories of individual consumers but by using aggregate demand data . We evaluate our work , by applying the techniques on hotel search . Our extensive user studies , using more than 15,000 user-provided ranking comparisons , demonstrate an overwhelming preference for the rankings generated by our techniques , compared to a large number of existing strong state-of-the-art baselines ."
"217","On the informativeness of cascade and intent-aware effectiveness measures The Maximum Entropy Method provides one technique for validating search engine effectiveness measures . Under this method , the value of an effectiveness measure is used as a constraint to estimate the most likely distribution of relevant documents under a maximum entropy assumption . This inferred distribution may then be compared to the actual distribution to quantify the `` informativeness '' of the measure . The inferred distribution may also be used to estimate values for other effectiveness measures . Previous work focused on traditional effectiveness measures , such as average precision . In this paper , we extend the Maximum Entropy Method to the newer cascade and intent-aware effectiveness measures by considering the dependency of the documents ranked in a results list . These measures are intended to reflect the novelty and diversity of search results in addition to the traditional relevance . Our results indicate that intent-aware measures based on the cascade model are informative in terms of both inferring actual distribution and predicting the values of other retrieval measures ."
"218","Online spelling correction for query completion In this paper , we study the problem of online spelling correction for query completions . Misspelling is a common phenomenon among search engines queries . In order to help users effectively express their information needs , mechanisms for automatically correcting misspelled queries are required . Online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered . As latency is crucial to the utility of the suggestions , such an algorithm needs to be not only accurate , but also efficient . To tackle this problem , we propose and study a generative model for input queries , based on a noisy channel transformation of the intended queries . Utilizing spelling correction pairs , we train a Markov n-gram transformation model that captures user spelling behavior in an unsupervised fashion . To find the top spell-corrected completion suggestions in real-time , we adapt the A \* search algorithm with various pruning heuristics to dynamically expand the search space efficiently . Evaluation of the proposed methods demonstrates a substantial increase in the effectiveness of online spelling correction over existing techniques ."
"219","Helix : online enterprise data analytics The size , heterogeneity and dynamicity of data within an enterprise makes indexing , integration and analysis of the data increasingly difficult tasks . On the other hand , there has been a massive increase in the amount of high-quality open data available on the Web that could provide invaluable insights to data analysts and business intelligence specialists within the enterprise . The goal of Helix project is to provide users within the enterprise with a platform that allows them to perform online analysis of almost any type and amount of internal data using the power of external knowledge bases available on the Web . Such a platform requires a novel , data-format agnostic indexing mechanism , and light-weight data linking techniques that could link semantically related records across internal and external data sources of various characteristics . We present the initial architecture of our system and discuss several research challenges involved in building such a system ."
"220","Truthy : mapping the spread of astroturf in microblog streams Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information . In particular , microblogs have become crucial grounds on which public relations , marketing , and political battles are fought . We demonstrate a web service that tracks political memes in Twitter and helps detect astroturfing , smear campaigns , and other misinformation in the context of U.S. political elections . We also present some cases of abusive behaviors uncovered by our service . Our web service is based on an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining , visualizing , mapping , classifying , and modeling massive streams of public microblogging events ."
"221","Measuring a commercial content delivery network Content delivery networks ( CDNs ) have become a crucial part of the modern Web infrastructure . This paper studies the performance of the leading content delivery provider - Akamai . It measures the performance of the current Akamai platform and considers a key architectural question faced by both CDN designers and their prospective customers : whether the co-location approach to CDN platforms adopted by Akamai , which tries to deploy servers in numerous Internet locations , brings inherent performance benefits over a more consolidated data center approach pursued by other influential CDNs such as Limelight . We believe the methodology we developed for this study will be useful for other researchers in the CDN arena ."
"222","Citizen sensor data mining , social media analytics and development centric web applications With the rapid rise in the popularity of social media ( 500M + Facebook users , 100M + twitter users ) , and near ubiquitous mobile access ( 4 + billion actively-used mobile phones ) , the sharing of observations and opinions has become common-place ( nearly 100M tweets a day , 1.8 trillion SMSs in US last year ) . This has given us an unprecedented access to the pulse of a populace and the ability to perform analytics on social data to support a variety of socially intelligent applications -- be it towards targeted online content delivery , crisis management , organizing revolutions or promoting social development in underdeveloped and developing countries . This tutorial will address challenges and techniques for building applications that support a broad variety of users and types of social media . This tutorial will focus on social intelligence applications for social development , and cover the following research efforts in sufficient depth : 1 ) understanding and analysis of informal text , esp . microblogs ( e.g. , issues of cultural entity extraction and role of semantic\/background knowledge enhanced techniques ) , and 2 ) building social media analytics platforms . Technical insights will be coupled with identification of computational techniques and real-world examples ."
"223","Traffic characterization and internet usage in rural Africa While Internet connectivity has reached a significant part of the world 's population , those living in rural areas of the developing world are still largely disconnected . Recent efforts have provided Internet connectivity to a growing number of remote locations , yet Internet traffic demands cause many of these networks to fail to deliver basic quality of service needed for simple applications . For an in-depth investigation of the problem , we gather and analyze network traces from a rural wireless network in Macha , Zambia . We supplement our analysis with on-site interviews from Macha , Zambia and Dwesa , South Africa , another rural community that hosts a local wireless network . The results reveal that Internet traffic in rural Africa differs significantly from the developed world . We observe dominance of web-based traffic , as opposed to peer-to-peer traffic common in urban areas . Application-wise , online social networks are the most popular , while the majority of bandwidth is consumed by large operating system updates . Our analysis also uncovers numerous network anomalies , such as significant malware traffic . Finally , we find a strong feedback loop between network performance and user behavior . Based on our findings , we conclude with a discussion of new directions in network design that take into account both technical and social factors ."
"224","Learning to model relatedness for news recommendation With the explosive growth of online news readership , recommending interesting news articles to users has become extremely important . While existing Web services such as Yahoo ! and Digg attract users ' initial clicks by leveraging various kinds of signals , how to engage such users algorithmically after their initial visit is largely under-explored . In this paper , we study the problem of post-click news recommendation . Given that a user has perused a current news article , our idea is to automatically identify `` related '' news articles which the user would like to read afterwards . Specifically , we propose to characterize relatedness between news articles across four aspects : relevance , novelty , connection clarity , and transition smoothness . Motivated by this understanding , we define a set of features to capture each of these aspects and put forward a learning approach to model relatedness . In order to quantitatively evaluate our proposed measures and learn a unified relatedness function , we construct a large test collection based on a four-month commercial news corpus with editorial judgments . The experimental results show that the proposed heuristics can indeed capture relatedness , and that the learned unified relatedness function works quite effectively ."
"225","Model characterization curves for federated search using click-logs : predicting user engagement metrics for the span of feasible operating points Modern day federated search engines aggregate heterogeneous types of results from multiple vertical search engines and compose a single search engine result page ( SERP ) . The search engine aggregates the results and produces one ranked list , constraining the vertical results to specific slots on the SERP . The usual way to compare two ranking algorithms is to first fix their operating points ( internal thresholds ) , and then run an online experiment that lasts multiple weeks . Online user engagement metrics are then compared to decide which algorithm is better . However , this method does not characterize and compare the behavior over the entire span of operating points . Furthermore , this time-consuming approach is not practical if we have to conduct the experiment over numerous operating points . In this paper we propose a method of characterizing the performance of models that allows us to predict answers to `` what if '' questions about online user engagement using click-logs over the entire span of feasible operating points . We audition verticals at various slots on the SERP and generate click-logs . This log is then used to create operating curves between variables of interest ( for example between result quality and click-through ) . The operating point for the system then can be chosen to achieve a specific trade-off between the variables . We apply this methodology to predict i ) the online performance of two different models , ii ) the impact of changing internal quality thresholds on clickthrough , iii ) the behavior of introducing a new feature , iv ) which machine learning loss function will give better online engagement , v ) the impact of sampling distribution of head and tail queries in the training process . The results are reported on a well-known federated search engine . We validate the predictions with online experiments ."
"226","Towards semantic knowledge propagation from text corpus to web images In this paper , we study the problem of transfer learning from text to images in the context of network data in which link based bridges are available to transfer the knowledge between the different domains . The problem of classification of image data is often much more challenging than text data because of the following two reasons : ( a ) Labeled text data is very widely available for classification purposes . On the other hand , this is often not the case for image data , in which a lot of images are available from many sources , but many of them are often not labeled . ( b ) The image features are not directly related to semantic concepts inherent in class labels . On the other hand , since text data tends to have natural semantic interpretability ( because of their human origins ) , they are often more directly related to class labels . Therefore , the relationships between the images and text features also provide additional hints for the classification process in terms of the image feature transformations which provide the most effective results . The semantic challenges of image features are glaringly evident , when we attempt to recognize complex abstract concepts , and the visual features often fail to discriminate such concepts . However , the copious availability of bridging relationships between text and images in the context of web and social network data can be used in order to design for effective classifiers for image data . One of our goals in this paper is to develop a mathematical model for the functional relationships between text and image features , so as indirectly transfer semantic knowledge through feature transformations . This feature transformation is accomplished by mapping instances from different domains into a common space of unspecific topics . This is used as a bridge to semantically connect the two heterogeneous spaces . This is also helpful for the cases where little image data is available for the classification process . We evaluate our knowledge transfer techniques on an image classification task with labeled text corpora and show the effectiveness with respect to competing algorithms ."
"227","VoiSTV : voice-enabled social TV Until recently , the TV viewing experience has not been a very social activity compared to activities on the World Wide Web . In this work , we will present a Voice-enabled Social TV system ( VoiSTV ) which allows users to interact , follow and monitor the online social media messages related to a TV show while watching it . Users can create , send , and reply to messages using spoken language . VoiSTV also provides metadata information about TV shows such as trends , hot topics , popularity as well as aggregated sentiment of show-related messages , all of which are valuable for TV program search and recommendation ."
"228","A self-training approach for resolving object coreference on the semantic web An object on the Semantic Web is likely to be denoted with multiple URIs by different parties . Object coreference resolution is to identify `` equivalent '' URIs that denote the same object . Driven by the Linking Open Data ( LOD ) initiative , millions of URIs have been explicitly linked with owl : sameAs statements , but potentially coreferent ones are still considerable . Existing approaches address the problem mainly from two directions : one is based upon equivalence inference mandated by OWL semantics , which finds semantically coreferent URIs but probably omits many potential ones ; the other is via similarity computation between property-value pairs , which is not always accurate enough . In this paper , we propose a self-training approach for object coreference resolution on the Semantic Web , which leverages the two classes of approaches to bridge the gap between semantically coreferent URIs and potential candidates . For an object URI , we firstly establish a kernel that consists of semantically coreferent URIs based on owl : sameAs , ( inverse ) functional properties and ( max - ) cardinalities , and then extend such kernel iteratively in terms of discriminative property-value pairs in the descriptions of URIs . In particular , the discriminability is learned with a statistical measurement , which not only exploits key characteristics for representing an object , but also takes into account the matchability between properties from pragmatics . In addition , frequent property combinations are mined to improve the accuracy of the resolution . We implement a scalable system and demonstrate that our approach achieves good precision and recall for resolving object coreference , on both benchmark and large-scale datasets ."
"229","Improving recommendation for long-tail queries via templates The ability to aggregate huge volumes of queries over a large population of users allows search engines to build precise models for a variety of query-assistance features such as query recommendation , correction , etc. . Yet , no matter how much data is aggregated , the long-tail distribution implies that a large fraction of queries are rare . As a result , most query assistance services perform poorly or are not even triggered on long-tail queries . We propose a method to extend the reach of query assistance techniques ( and in particular query recommendation ) to long-tail queries by reasoning about rules between query templates rather than individual query transitions , as currently done in query-flow graph models . As a simple example , if we recognize that ` Montezuma ' is a city in the rare query `` Montezuma surf '' and if the rule ` city surf → beach has been observed , we are able to offer `` Montezuma beach '' as a recommendation , even if the two queries were never observed in a same session . We conducted experiments to validate our hypothesis , first via traditional small-scale editorial assessments but more interestingly via a novel automated large scale evaluation methodology . Our experiments show that general coverage can be relatively increased by 24 % using templates without penalizing quality . Furthermore , for 36 % of the 95M queries in our query flow graph , which have no out edges and thus could not be served recommendations , we can now offer at least one recommendation in 98 % of the cases ."
"230","Automatically generating labels based on unified click model Ground truth labels are one of the most important parts in many test collections for information retrieval . Each label , depicting the relevance between a query-document pair , is usually judged by a human , and this process is time-consuming and labor-intensive . Automatically Generating labels from click-through data has attracted increasing attention . In this paper , we propose a Unified Click Model to predict the multi-level labels , which aims at comprehensively considering the advantages of the Position Models and Cascade Models . Experiments show that the proposed click model outperforms the existing click models in predicting the multi-level labels , and could replace the labels judged by humans for test collections ."
"231","A non-syntactic approach for text sentiment classification with stopwords The present approach uses stopwords and the gaps that occur between successive stopwords - formed by contentwords - as features for sentiment classification ."
"232","DIDO : a disease-determinants ontology from web sources This paper introduces DIDO , a system providing convenient access to knowledge about factors involved in human diseases , automatically extracted from textual Web sources . The knowledge base is bootstrapped by integrating entities from hand-crafted sources like MeSH and OMIM . As these are short on relationships between dierent types of biomedical entities , DIDO employs flexible and robust pattern learning and constraint-based reasoning methods to automatically extract new relational facts from textual sources . These facts can then be iteratively added to the knowledge base . The result is a semantic graph of typed entities and relations between diseases , their symptoms , and their factors , with emphasis on environmental factors but covering also molecular determinants . We demonstrate the value of DIDO for knowledge discovery about causal factors and properties of complex diseases , including factor-disease chains ."
"233","A framework for evaluating network measures for functional importance Many metrics such as degree , closeness , and PageRank have been introduced to determine the relative importance of a node within a network . The desired function of a network , however , is domain-specific . For example , the robustness can be crucial for a communication network , while efficiency is more preferred for fast spreading of advertisements in viral marketing . The information provided by some widely used measures are often conflicting under such varying demands . In this paper , we present a novel framework for evaluating network metrics regarding typical functional requirements . We also propose an analysis of five well established measures to compare their performance of ranking nodes on functional importance in a real-life network ."
"234","Milgram-routing in social networks We demonstrate how a recent model of social networks ( `` Affiliation Networks '' , ( 21 ) ) offers powerful cues in local routing within social networks , a theme made famous by sociologist Milgram 's `` six degrees of separation '' experiments . This model posits the existence of an `` interest space '' that underlies a social network ; we prove that in networks produced by this model , not only do short paths exist among all pairs of nodes but natural local routing algorithms can discover them effectively . Specifically , we show that local routing can discover paths of length O ( log2 n ) to targets chosen uniformly at random , and paths of length O ( 1 ) to targets chosen with probability proportional to their degrees . Experiments on the co-authorship graph derived from DBLP data confirm our theoretical results , and shed light into the power of one step of lookahead in routing algorithms for social networks ."
"235","Low-infrastructure methods to improve internet access for mobile users in emerging regions As information technology supports more aspects of modern life , digital access has become an important tool for developing regions to lift themselves from poverty . Though broadband internet connectivity will not be universally available in the short-term , widely-employed mobile devices coupled with novel delay-tolerant networking do allow limited forms of connectivity . This paper explores the design space for internet access systems operating with constrained connectivity . Our starting point is C-LINK , a collaborative caching system that enhances the performance of interactive web access over DTN and cellular connectivity . We discuss our experiences and results from deploying C-LINK in Nicaragua , before moving on to a broader design study of other issues that further influence operation . We consider the impact of ( i ) storing web content collaboratively cached across all user nodes , ( ii ) hybrid transport layers exploiting the best attributes of limited cellular and DTN-style connectivity . We also explore the behavior of future systems under a range of usage and mobility scenarios . Even under adverse conditions , our techniques can improve average service latency for page requests by a factor of 2X . Our results point to the considerable power of leveraging user mobility and collaboration in providing very-low-infrastructure internet access to developing regions ."
"236","Buy-it-now or take-a-chance : a simple sequential screening mechanism We present a simple auction mechanism which extends the second-price auction with reserve and is truthful in expectation . This mechanism is particularly effective in private value environments where the distribution of valuations are irregular . Bidders can `` buy-it-now '' , or alternatively `` take-a-chance '' where the top d bidders are equally likely to win . The randomized take-a-chance allocation incentivizes high valuation bidders to buy-it-now . We show that for a large class of valuations , this mechanism achieves similar allocations and revenues as Myerson 's optimal mechanism , and outperforms the second-price auction with reserve . In addition , we present an evaluation of bid data from Microsoft 's AdECN platform . We find the valuations are irregular , and counterfactual experiments suggest our BIN-TAC mechanism would improve revenue by 11 % relative to an optimal second-price mechanism with reserve ."
"237","Second international workshop on RESTful design ( WS-REST 2011 ) Over the past few years , the discussion between the two major architectural styles for designing and implementing Web services , the RPC-oriented approach and the resource-oriented approach , has been mainly held outside of traditional research communities . Mailing lists , forums and developer communities have seen long and fascinating debates around the assumptions , strengths , and weaknesses of these two approaches . The Second International Workshop on RESTful Design ( WS-REST 2011 ) has the goal of getting more researchers involved in the debate by providing a forum where discussions around the resource-oriented style of Web services design take place . Representational State Transfer ( REST ) is an architectural style and as such can be applied in different ways , can be extended by additional constraints , or can be specialized with more specific interaction patterns . WS-REST is the premier forum for discussing research ideas , novel applications and results centered around REST at the World Wide Web conference , which provides a great setting to host this second edition of the workshop dedicated to research on the architectural style underlying the Web ."
"238","An adaptive ontology-based approach to identify correlation between publications In this paper , we propose an adaptive ontology-based approach for related paper identification , to meet most researchers ' practical needs . By searching ontology , we can return a diverse set of papers that are explicitly and implicitly related to an input paper . Moreover , our approach does not rely on known ontology . Instead , we build and update ontology for a collection with any domain of interest . Being independent from known ontology , our approach is much more adaptive for different domains ."
"239","Extracting events and event descriptions from Twitter This paper describes methods for automatically detecting events involving known entities from Twitter and understanding both the events as well as the audience reaction to them . We show that NLP techniques can be used to extract events , their main actors and the audience reactions with encouraging results ."
"240","Analysis and tracking of emotions in english and bengali texts : a computational approach The present discussion highlights the aspects of an ongoing doctoral thesis grounded on the analysis and tracking of emotions from English and Bengali texts . Development of lexical resources and corpora meets the preliminary urgencies . The research spectrum aims to identify the evaluative emotional expressions at word , phrase , sentence , and document level granularities along with their associated holders and topics . Tracking of emotions based on topic or event was carried out by employing sense based affect scoring techniques . The labeled emotion corpora are being prepared from unlabeled examples to cope with the scarcity of emotional resources , especially for the resource constraint language like Bengali . Different unsupervised , supervised and semi-supervised strategies , adopted for coloring each outline of the research spectrum produce satisfactory outcomes"
"241","Performance enhancement of scheduling algorithms in clusters and grids using improved dynamic load balancing techniques This paper describes the research work done for during PhD study . Cluster computing , grid computing and cloud computing are distributed computing environments ( DCEs ) widely accepted for the next generation Web based commercial and scientific applications . These applications work around the globally distributed data of petabyte scale that can only be processed by the aggregating the capability of globally distributed resources . The resource management and process scheduling in large scale distributed computing environment are a challenging task . In this research work we have devised new scheduling algorithms and resource management strategies specially designed for the cluster and grid cloud and peer-to-peer computing . The research work finally presented the distributed computing solutions to one scientific and one commercial application viz . e-Learning and data mining ."
"242","A study on the impact of product images on user clicks for online shopping In this paper we study the importance of image based features on the click-through rate ( CTR ) in the context of a large scale product search engine . Typically product search engines use text based features in their ranking function . We present a novel idea of using image based features , common in the photography literature , in addition to text based features . We used a stochastic gradient boosting based regression model to learn relationships between features and CTR . Our results indicate statistically significant correlations between the image features and CTR . We also see improvements to NDCG and mean standard regression ."
"243","Predicting popular messages in Twitter Social network services have become a viable source of information for users . In Twitter , information deemed important by the community propagates through retweets . Studying the characteristics of such popular messages is important for a number of tasks , such as breaking news detection , personalized message recommendation , viral marketing and others . This paper investigates the problem of predicting the popularity of messages as measured by the number of future retweets and sheds some light on what kinds of factors influence information propagation in Twitter . We formulate the task into a classification problem and study two of its variants by investigating a wide spectrum of features based on the content of the messages , temporal information , metadata of messages and users , as well as structural properties of the users ' social graph on a large scale dataset . We show that our method can successfully predict messages which will attract thousands of retweets with good performance ."
"244","ARROW : GenerAting SignatuRes to Detect DRive-By DOWnloads A drive-by download attack occurs when a user visits a webpage which attempts to automatically download malware without the user 's consent . Attackers sometimes use a malware distribution network ( MDN ) to manage a large number of malicious webpages , exploits , and malware executables . In this paper , we provide a new method to determine these MDNs from the secondary URLs and redirect chains recorded by a high-interaction client honeypot . In addition , we propose a novel drive-by download detection method . Instead of depending on the malicious content used by previous methods , our algorithm first identifies and then leverages the URLs of the MDN 's central servers , where a central server is a common server shared by a large percentage of the drive-by download attacks in the same MDN . A set of regular expression-based signatures are then generated based on the URLs of each central server . This method allows additional malicious webpages to be identified which launched but failed to execute a successful drive-by download attack . The new drive-by detection system named ARROW has been implemented , and we provide a large-scale evaluation on the output of a production drive-by detection system . The experimental results demonstrate the effectiveness of our method , where the detection coverage has been boosted by 96 % with an extremely low false positive rate ."
"245","Efficient k-nearest neighbor graph construction for generic similarity measures K-Nearest Neighbor Graph ( K-NNG ) construction is an important operation with many web related applications , including collaborative filtering , similarity search , and many others in data mining and machine learning . Existing methods for K-NNG construction either do not scale , or are specific to certain similarity measures . We present NN-Descent , a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures . Our method is based on local search , has minimal space overhead and does not rely on any shared global index . Hence , it is especially suitable for large-scale applications where data structures need to be distributed over the network . We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90 % recall with each point comparing only to several percent of the whole dataset on average ."
"246","Towards identifying arguments in Wikipedia pages Wikipedia is one of the most widely used repositories of human knowledge today , contributed mostly by a few hundred thousand regular editors . In this open environment , inevitably , differences of opinion arise among editors of the same article . Especially for polemical topics such as religion and politics , difference of opinions among editors may lead to intense `` edit wars '' in which editors compete to have their opinions and points of view accepted . While such disputes can compromise the reliability of the article ( or at least portions of it ) , they are recorded in the edit history of the articles . We posit that exposing such disputes to the reader , and pointing to the portions of the text where they manifest most prominently can be beneficial in helping concerned readers in understanding such topics . In this paper , we discuss our initial efforts towards the problem of automatic evaluation of extracting controversial points in Wikipedia pages ."
"247","Efficiently evaluating graph constraints in content-based publish\/subscribe We introduce the problem of evaluating graph constraints in content-based publish\/subscribe ( pub\/sub ) systems . This problem formulation extends traditional content-based pub\/sub systems in the following manner : publishers and subscribers are connected via a ( logical ) directed graph G with node and edge constraints , which limits the set of valid paths between them . Such graph constraints can be used to model a Web advertising exchange ( where there may be restrictions on how advertising networks can connect advertisers and publishers ) and content delivery problems in social networks ( where there may be restrictions on how information can be shared via the social graph ) . In this context , we develop efficient algorithms for evaluating graph constraints over arbitrary directed graphs G. We also present experimental results that demonstrate the effectiveness and scalability of the proposed algorithms using a realistic dataset from Yahoo ! 's Web advertising exchange ."
"248","Web-scale entity-relation search architecture Enabling entity search and ranking at Web-scale is fraught with many challenges : annotating the corpus with entities and types , query language design , index design , query processing logic , and answer consolidation . We describe a Web-scale entity search engine we are building to handle over a billion Web pages , over 200,000 types , over 1,500,000 entities , and hundreds of entity annotations per page . We describe the design of compressed , token span oriented indices for entity and type annotations . Our prototype demonstrates the practicality of Web-scale entity-relation search ."
"249","Identifying primary content from web pages and its application to web search ranking Web pages are usually highly structured documents . In some documents , content with different functionality is laid out in blocks , some merely supporting the main discourse . In other documents , there may be several blocks of unrelated main content . Indexing a web page as if it were a linear document can cause problems because of the diverse nature of its content . If the retrieval function treats all blocks of the web page equally without attention to structure , it may lead to irrelevant query matches . In this paper , we describe how content quality of different blocks of a web page can be utilized to improve a retrieval function . Our method is based on segmenting a web page into semantically coherent blocks and learning a predictor of segment content quality . We also describe how to use segment content quality estimates as weights in the BM25F formulation . Experimental results show our method improves relevance of retrieved results by as much as 4.5 % compared to BM25F that treats the body of a web page as a single section , and by a larger margin of over 9 % for difficult queries ."
"250","Einstein : physicist or vegetarian ? summarizing semantic type graphs for knowledge discovery The Web and , in particular , knowledge-sharing communities such as Wikipedia contain a huge amount of information encompassing disparate and diverse fields . Knowledge bases such as DBpedia or Yago represent the data in a concise and more structured way bearing the potential of bringing database tools to Web Search . The wealth of data , however , poses the challenge of how to retrieve important and valuable information , which is often intertwined with trivial and less important details . This calls for an efficient and automatic summarization method . In this demonstration proposal , we consider the novel problem of summarizing the information related to a given entity , like a person or an organization . To this end , we utilize the rich type graph that knowledge bases provide for each entity , and define the problem of selecting the best cost-restricted subset of types as summary with good coverage of salient properties . We propose a demonstration of our system which allows the user to specify the entity to summarize , an upper bound on the cost of the resulting summary , as well as to browse the knowledge base in a more simple and intuitive manner ."
"251","Rewriting queries on SPARQL views The problem of answering SPARQL queries over virtual SPARQL views is commonly encountered in a number of settings , including while enforcing security policies to access RDF data , or when integrating RDF data from disparate sources . We approach this problem by rewriting SPARQL queries over the views to equivalent queries over the underlying RDF data , thus avoiding the costs entailed by view materialization and maintenance . We show that SPARQL query rewriting combines the most challenging aspects of rewriting for the relational and XML cases : like the relational case , SPARQL query rewriting requires synthesizing multiple views ; like the XML case , the size of the rewritten query is exponential to the size of the query and the views . In this paper , we present the first native query rewriting algorithm for SPARQL . For an input SPARQL query over a set of virtual SPARQL views , the rewritten query resembles a union of conjunctive queries and can be of exponential size . We propose optimizations over the basic rewriting algorithm to ( i ) minimize each conjunctive query in the union ; ( ii ) eliminate conjunctive queries with empty results from evaluation ; and ( iii ) efficiently prune out big portions of the search space of empty rewritings . The experiments , performed on two RDF stores , show that our algorithms are scalable and independent of the underlying RDF stores . Furthermore , our optimizations have order of magnitude improvements over the basic rewriting algorithm in both the rewriting size and evaluation time ."
"252","Autopedia : automatic domain-independent Wikipedia article generation This paper proposes a general framework , named Autopedia , to generate high-quality wikipedia articles for given concepts in any domains , by automatically selecting the best wikipedia template consisting the sub-topics to organize the article for the input concept . Experimental results on 4,526 concepts validate the effectiveness of Autopedia , and the wikipedia template selection approach which takes into account both the template quality and the semantic relatedness between the input concept and its sibling concepts , performs the best ."
"253","Summarization of archived and shared personal photo collections The volume of personal photos hosted on photo archives and social sharing platforms has been increasing exponentially . It is difficult to get an overview of a large collection of personal photos without browsing though the entire database manually . In this research , we propose a framework to generate representative subset summaries from photo collections hosted on web archives or social networks . We define salient properties of an effective photo summary and model summarization as an optimization of these properties , given the size constraints . We also introduce metrics for evaluating photo summaries based on their information content and the ability to satisfy user 's information needs . Our experiments show that our summarization framework performs better than baseline algorithms ."
"254","SmartInt : using mined attribute dependencies to integrate fragmented web databases Many web databases can be seen as providing partial and overlapping information about entities in the world . To answer queries effectively , we need to integrate the information about the individual entities that are fragmented over multiple sources . At first blush this is just the inverse of traditional database normalization problem - rather than go from a universal relation to normalized tables , we want to reconstruct the universal relation given the tables ( sources ) . The standard way of reconstructing the entities will involve joining the tables . Unfortunately , because of the autonomous and decentralized way in which the sources are populated , they often do not have Primary Key - Foreign Key relations . While tables do share attributes , direct joins over these shared attributes can result in reconstruction of many spurious entities thus seriously compromising precision . We present a unified approach that supports intelligent retrieval over fragmented web databases by mining and using inter-table dependencies . Experiments with the prototype implementation , SmartInt , show that its retrieval strikes a good balance between precision and recall ."
"255","Video summarization via transferrable structured learning It is well-known that textual information such as video transcripts and video reviews can significantly enhance the performance of video summarization algorithms . Unfortunately , many videos on the Web such as those from the popular video sharing site YouTube do not have useful textual information . The goal of this paper is to propose a transfer learning framework for video summarization : in the training process both the video features and textual features are exploited to train a summarization algorithm while for summarizing a new video only its video features are utilized . The basic idea is to explore the transferability between videos and their corresponding textual information . Based on the assumption that video features and textual features are highly correlated with each other , we can transfer textual information into knowledge on summarization using video information only . In particular , we formulate the video summarization problem as that of learning a mapping from a set of shots of a video to a subset of the shots using the general framework of SVM-based structured learning . Textual information is transferred by encoding them into a set of constraints used in the structured learning process which tend to provide a more detailed and accurate characterization of the different subsets of shots . Experimental results show significant performance improvement of our approach and demonstrate the utility of textual information for enhancing video summarization ."
"256","Here , there , and everywhere : correlated online behaviors can lead to overestimates of the effects of advertising Measuring the causal effects of online advertising ( adfx ) on user behavior is important to the health of the WWW publishing industry . In this paper , using three controlled experiments , we show that observational data frequently lead to incorrect estimates of adfx . The reason , which we label `` activity bias , '' comes from the surprising amount of time-based correlation between the myriad activities that users undertake online . In Experiment 1 , users who are exposed to an ad on a given day are much more likely to engage in brand-relevant search queries as compared to their recent history for reasons that had nothing do with the advertisement . In Experiment 2 , we show that activity bias occurs for page views across diverse websites . In Experiment 3 , we track account sign-ups at a competitor 's ( of the advertiser ) website and find that many more people sign-up on the day they saw an advertisement than on other days , but that the true `` competitive effect '' was minimal . In all three experiments , exposure to a campaign signals doing `` more of everything '' in given period of time , making it difficult to find a suitable `` matched control '' using prior behavior . In such cases , the `` match '' is fundamentally different from the exposed group , and we show how and why observational methods lead to a massive overestimate of adfx in such circumstances ."
"257","Highly efficient algorithms for structural clustering of large websites In this paper , we present a highly scalable algorithm for structurally clustering webpages for extraction . We show that , using only the URLs of the webpages and simple content features , it is possible to cluster webpages effectively and efficiently . At the heart of our techniques is a principled framework , based on the principles of information theory , that allows us to effectively leverage the URLs , and combine them with content and structural properties . Using an extensive evaluation over several large full websites , we demonstrate the effectiveness of our techniques , at a scale unattainable by previous techniques ."
"258","Web scale NLP : a case study on url word breaking This paper uses the URL word breaking task as an example to elaborate what we identify as crucial in designing statistical natural language processing ( NLP ) algorithms for Web scale applications : ( 1 ) rudimentary multilingual capabilities to cope with the global nature of the Web , ( 2 ) multi-style modeling to handle diverse language styles seen in the Web contents , ( 3 ) fast adaptation to keep pace with the dynamic changes of the Web , ( 4 ) minimal heuristic assumptions for generalizability and robustness , and ( 5 ) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost . We first show that the state-of-the-art word breaking techniques can be unified and generalized under the Bayesian minimum risk ( BMR ) framework that , using a Web scale N-gram , can meet the first three requirements . We discuss how the existing techniques can be viewed as introducing additional assumptions to the basic BMR framework , and describe a generic yet efficient implementation called word synchronous beam search . Testing the framework and its implementation on a series of large scale experiments reveals the following . First , the language style used to build the model plays a critical role in the word breaking task , and the most suitable for the URL word breaking task appears to be that of the document title where the best performance is obtained . Models created from other language styles , such as from document body , anchor text , and even queries , exhibit varying degrees of mismatch . Although all styles benefit from increasing modeling power which , in our experiments , corresponds to the use of a higher order N-gram , the gain is most recognizable for the title model . The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models , but are less effective and , in many cases , lead to poorer performance than the matched model with minimal assumptions . For the matched model based on document titles , an accuracy rate of 97.18 % can already be achieved using simple trigram without any heuristics ."
"259","A game theoretic formulation of the service provisioning problem in cloud systems Cloud computing is an emerging paradigm which allows the on-demand delivering of software , hardware , and data as services . As cloud-based services are more numerous and dynamic , the development of efficient service provisioning policies become increasingly challenging . Game theoretic approaches have shown to gain a thorough analytical understanding of the service provisioning problem . In this paper we take the perspective of Software as a Service ( SaaS ) providers which host their applications at an Infrastructure as a Service ( IaaS ) provider . Each SaaS needs to comply with quality of service requirements , specified in Service Level Agreement ( SLA ) contracts with the end-users , which determine the revenues and penalties on the basis of the achieved performance level . SaaS providers want to maximize their revenues from SLAs , while minimizing the cost of use of resources supplied by the IaaS provider . Moreover , SaaS providers compete and bid for the use of infrastructural resources . On the other hand , the IaaS wants to maximize the revenues obtained providing virtualized resources . In this paper we model the service provisioning problem as a Generalized Nash game , and we propose an efficient algorithm for the run time management and allocation of IaaS resources to competing SaaSs ."
"260","Survivability-oriented self-tuning of web systems Running in a highly uncertain and changing environment , Web systems can not always provide full set of services with optim ¬ al quality , especially when the workload is high or failures in subsys-tems occur frequently . It is thus desirable to continuously maintain a high satisfaction level of the system value proposition , hereafter survivability assurance , while relaxing\/sacrificing certain quality\/functional requirements that are not crucial to the survival of the Web systems . In this paper , we propose a requirements-driven self-tuning method for survivability assurance of Web systems . Using a value-based feedback controller plus a requirements-oriented reasoner , our method makes both quality and functional requirements tradeoffs decisions at runtime ."
"261","OntoTrix : a hybrid visualization for populated ontologies Most Semantic Web data visualization tools structure the representation according to the concept definitions and interrelations that constitute the ontology 's vocabulary . Instances are often treated as somewhat peripheral information , when considered at all . These instances , that populate ontologies , represent an essential part of any knowledge base , and are often orders of magnitude more numerous than the concept definitions that give them machine-processable meaning . We present a visualization technique designed to enable users to visualize large instance sets and the relations that connect them . This hybrid visualization uses both node-link and adjacency matrix representations of graphs to visualize different parts of the data depending on their semantic and local structural properties , exploiting ontological knowledge to drive the graph layout . The representation is embedded in an environment that features advanced interaction techniques for easy navigation , including support for smooth continuous zooming and coordinated views ."
"262","Information spreading in context Information spreading processes are central to human interactions . Despite recent studies in online domains , little is known about factors that could affect the dissemination of a single piece of information . In this paper , we address this challenge by combining two related but distinct datasets , collected from a large scale privacy-preserving distributed social sensor system . We find that the social and organizational context significantly impacts to whom and how fast people forward information . Yet the structures within spreading processes can be well captured by a simple stochastic branching model , indicating surprising independence of context . Our results build the foundation of future predictive models of information flow and provide significant insights towards design of communication platforms ."
"263","A user-tunable approach to marketplace search The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results . Online search engines have used other features beyond pure IR features to return relevant matching documents . However , over-emphasis on relevance could lead to redundancy in search results . In document search , diversity is simply the variety of documents that span the result set . In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options . For such a marketplace , in order to minimize query abandonment and the risk of dissatisfaction to the average user , several factors like diversity , trust and value need to be taken into account . Previous work in this field ( 4 ) has shown an impossibility result that there exists no such function that can optimize for all these factors . Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user . In this paper we describe an interface which enables users to have more control over the optimization function used to present the results . We demonstrate this for search on eBay - one of the largest online marketplaces with a vibrant user community and dynamic inventory . We use an algorithm based on bounded greedy selection ( 5 ) to construct the result set based on parameters specified by the user ."
"264","Dynamic learning-based mechanism design for dependent valued exchange economies Learning private information from multiple strategic agents poses challenge in many Internet applications . Sponsored search auctions , crowdsourcing , Amazon 's mechanical turk , various online review forums are examples where we are interested in learning true values of the advertisers or true opinion of the reviewers . The common thread in these decision problems is that the optimal outcome depends on the private information of all the agents , while the decision of the outcome can be chosen only through reported information which may be manipulated by the strategic agents . The other important trait of these applications is their dynamic nature . The advertisers in an online auction or the users of mechanical turk arrive and depart , and when present , interact with the system repeatedly , giving the opportunity to learn their types . Dynamic mechanisms , which learn from the past interactions and make present decisions depending on the expected future evolution of the game , has been shown to improve performance over repeated versions of static mechanisms . In this paper , we will survey the past and current state-of-the-art dynamic mechanisms and analyze a new setting where the agents consist of buyers and sellers , known as exchange economies , and agents having value interdependency , which are relevant in applications illustrated through examples . We show that known results of dynamic mechanisms with independent value settings can not guarantee certain desirable properties in this new significantly different setting . In the future work , we propose to analyze similar settings with dynamic types and population ."
"265","Identifying enrichment candidates in textbooks Many textbooks written in emerging countries lack clear and adequate coverage of important concepts . We propose a technological solution for algorithmically identifying those sections of a book that are not well written and could benefit from better exposition . We provide a decision model based on the syntactic complexity of writing and the dispersion of key concepts . The model parameters are learned using a tune set which is algorithmically generated using a versioned authoritative web resource as a proxy . We evaluate the proposed methodology over a corpus of Indian textbooks which demonstrates its effectiveness in identifying enrichment candidates ."
"266","Two-stream indexing for spoken web search This paper presents two-stream processing of audio to index the audio content for Spoken Web search . The first stream indexes the meta-data associated with a particular audio document . The meta-data is usually very sparse , but accurate . This therefore results in a high-precision , low-recall index . The second stream uses a novel language-independent speech recognition to generate text to be indexed . Owing to the multiple languages and the noise in user generated content on the Spoken Web , the speech recognition accuracy of such systems is not high , thus they result in a low-precision , high-recall index . The paper attempts to use these two complementary streams to generate a combined index to increase the precision-recall performance in audio content search . The problem of audio content search is motivated by the real world implication of the Web in developing regions , where due to literacy and affordability issues , people use Spoken Web which consists of interconnected VoiceSites , which have content in audio . The experiments are based on more than 20,000 audio documents spanning over seven live VoiceSites and four different languages . The results suggest significant improvement over a meta-data-only or a speech-recognitiononly system , thus justifying the two-stream processing approach . Audio content search is a growing problem area and this paper wishes to be a first step to solving this at a large scale , across languages , in a Web context ."
"267","Harnessing the wisdom of crowds : video event detection based on synchronous comments With the recent explosive growth of the number of videos on the Web , it becomes more important to facilitate users ' demand for locating their preferred event clips in the lengthy and voluminous programs . Although there has been a great deal of study on generic event detection in recent years , the performance of existing approaches is still far from satisfactory . In this paper , we propose an integrated framework for general event detection . The key idea is that we utilize the synchronous comments to segment the video into clips with semantic text analysis , while taking into account the relationship between the users who write the comments . By borrowing the power of `` the wisdom of crowds '' , we experimentally demonstrate that our approach can effectively detect video events ."
"268","CONQUER : a system for efficient context-aware query suggestions Many of today 's search engines provide autocompletion while the user is typing a query string . This type of dynamic query suggestion can help users to formulate queries that better represent their search intent during Web search interactions . In this paper , we demonstrate our query suggestion system called CONQUER , which allows to efficiently suggest queries for a given partial query and a number of available query context observations . The context-awareness allows for suggesting queries tailored to a given context , e.g. , the user location or the time of day . CONQUER uses a suggestion model that is based on the combined probabilities of sequential query patterns and context observations . For this , the weight of a context in a query suggestion can be adjusted online , for example , based on the learned user behavior or user profiles . We demonstrate the functionality of CONQUER based on 6 million queries from an AOL query log using the time of day and the country domain of the clicked URLs in the search result as context observations ."
"269","Understanding the functions of business accounts on Twitter This paper performs an initial exploration of business Twitter accounts in order to start understanding how businesses interact with their users and viceversa . We provide an analysis of business tweet types and topics and show that specific business tweet classes such as deals and events can be reliably identified for customer use ."
"270","A classification based framework for concept summarization In this paper we propose a novel classification based framework for finding a small number of images summarizing a concept . Our method exploits metadata information available with the images to get the category information using Latent Dirichlet Allocation . We modify the import vector machine formulation based on kernel logistic regression to solve the underlying classification problem . We show that the import vectors provide a good summary satisfying important properties such as coverage , diversity and balance . Furthermore , the framework allows users to specify desired distributions over category , time etc , that a summary should satisfy . Experimental results show that the proposed method performs better than state-of-the-art summarization methods in terms of satisfying important visual and semantic properties ."
"271","Choreography conformance via synchronizability Choreography analysis has been a crucial problem in service oriented computing . Interactions among services involve message exchanges across organizational boundaries in a distributed computing environment , and in order to build such systems in a reliable manner , it is necessary to develop techniques for analyzing such interactions . Choreography conformance involves verifying that a set of services behave according to a given choreography specification that characterizes their interactions . Unfortunately this is an undecidable problem when services interact with asynchronous communication . In this paper we present techniques that identify if the interaction behavior for a set of services remain the same when asynchronous communication is replaced with synchronous communication . This is called the synchronizability problem and determining the synchronizability of a set of services has been an open problem for several years . We solve this problem in this paper . Our results can be used to identify synchronizable services for which choreography conformance can be checked efficiently . Our results on synchronizability are applicable to any software infrastructure that supports message-based interactions ."
"272","Relational duality : unsupervised extraction of semantic relations between entities on the web Extracting semantic relations among entities is an important first step in various tasks in Web mining and natural language processing such as information extraction , relation detection , and social network mining . A relation can be expressed extensionally by stating all the instances of that relation or intensionally by defining all the paraphrases of that relation . For example , consider the ACQUISITION relation between two companies . An extensional definition of ACQUISITION contains all pairs of companies in which one company is acquired by another ( e.g. ( YouTube , Google ) or ( Powerset , Microsoft ) ) . On the other hand we can intensionally define ACQUISITION as the relation described by lexical patterns such as X is acquired by Y , or Y purchased X , where X and Y denote two companies . We use this dual representation of semantic relations to propose a novel sequential co-clustering algorithm that can extract numerous relations efficiently from unlabeled data . We provide an efficient heuristic to find the parameters of the proposed coclustering algorithm . Using the clusters produced by the algorithm , we train an L1 regularized logistic regression model to identify the representative patterns that describe the relation expressed by each cluster . We evaluate the proposed method in three different tasks : measuring relational similarity between entity pairs , open information extraction ( Open IE ) , and classifying relations in a social network system . Experiments conducted using a benchmark dataset show that the proposed method improves existing relational similarity measures . Moreover , the proposed method significantly outperforms the current state-of-the-art Open IE systems in terms of both precision and recall . The proposed method correctly classifies 53 relation types in an online social network containing 470 ; 671 nodes and 35 ; 652 ; 475 edges , thereby demonstrating its efficacy in real-world relation detection tasks ."
"273","Who says what to whom on twitter We study several longstanding questions in media communications research , in the context of the microblogging service Twitter , regarding the production , flow , and consumption of information . To do so , we exploit a recently introduced feature of Twitter known as `` lists '' to distinguish between elite users - by which we mean celebrities , bloggers , and representatives of media outlets and other formal organizations - and ordinary users . Based on this classification , we find a striking concentration of attention on Twitter , in that roughly 50 % of URLs consumed are generated by just 20K elite users , where the media produces the most information , but celebrities are the most followed . We also find significant homophily within categories : celebrities listen to celebrities , while bloggers listen to bloggers etc ; however , bloggers in general rebroadcast more information than the other categories . Next we re-examine the classical `` two-step flow '' theory of communications , finding considerable support for it on Twitter . Third , we find that URLs broadcast by different categories of users or containing different types of content exhibit systematically different lifespans . And finally , we examine the attention paid by the different user categories to different news topics ."
"274","Piazza : data management infrastructure for semantic web applications The Semantic Web envisions a World Wide Web in which data is described with rich semantics and applications can pose complex queries . To this point , researchers have defined new languages for specifying meanings for concepts and developed techniques for reasoning about them , using RDF as the data model . To flourish , the Semantic Web needs to be able to accommodate the huge amounts of existing data and the applications operating on them . To achieve this , we are faced with two problems . First , most of the world 's data is available not in RDF but in XML ; XML and the applications consuming it rely not only on the domain structure of the data , but also on its document structure . Hence , to provide interoperability between such sources , we must map between both their domain structures and their document structures . Second , data management practitioners often prefer to exchange data through local point-to-point data translations , rather than mapping to common mediated schemas or ontologies . This paper describes the Piazza system , which addresses these challenges . Piazza offers a language for mediating between data sources on the Semantic Web , which maps both the domain structure and document structure . Piazza also enables interoperation of XML data with RDF data that is accompanied by rich OWL ontologies . Mappings in Piazza are provided at a local scale between small sets of nodes , and our query answering algorithm is able to chain sets mappings together to obtain relevant data from across the Piazza network . We also describe an implemented scenario in Piazza and the lessons we learned from it ."
"275","Detecting web page structure for adaptive viewing on small form factor devices Mobile devices have already been widely used to access the Web . However , because most available web pages are designed for desktop PC in mind , it is inconvenient to browse these large web pages on a mobile device with a small screen . In this paper , we propose a new browsing convention to facilitate navigation and reading on a small-form-factor device . A web page is organized into a two level hierarchy with a thumbnail representation at the top level for providing a global view and index to a set of sub-pages at the bottom level for detail information . A page adaptation technique is also developed to analyze the structure of an existing web page and split it into small and logically related units that fit into the screen of a mobile device . For a web page not suitable for splitting , auto-positioning or scrolling-by-block is used to assist the browsing as an alterative . Our experimental results show that our proposed browsing convention and developed page adaptation scheme greatly improve the user 's browsing experiences on a device with a small display ."
"276","Analysis of community structure in Wikipedia We present the results of a community detection analysis of the Wikipedia graph . Distinct communities in Wikipedia contain semantically closely related articles . The central topic of a community can be identified using PageRank . Extracted communities can be organized hierarchically similar to manually created Wikipedia category structure ."
"277","Generalized fact-finding Once information retrieval has located a document , and information extraction has provided its contents , how do we know whether we should actually believe it ? Fact-finders are a state-of-the-art class of algorithms that operate in a manner analogous to Kleinberg 's Hubs and Authorities , iteratively computing the trustworthiness of an information source as a function of the believability of the claims it makes , and the believability of a claim as a function of the trustworthiness of those sources asserting it . However , as fact-finders consider only `` who claims what '' , they ignore a great deal of relevant background and contextual information . We present a framework for `` lifting '' ( generalizing ) the fact-finding process , allowing us to elegantly incorporate knowledge such as the confidence of the information extractor and the attributes of the information sources . Experiments demonstrate that leveraging this information significantly improves performance over existing , `` unlifted '' fact-finding algorithms ."
"278","Unified analysis of streaming news News clustering , categorization and analysis are key components of any news portal . They require algorithms capable of dealing with dynamic data to cluster , interpret and to temporally aggregate news articles . These three tasks are often solved separately . In this paper we present a unified framework to group incoming news articles into temporary but tightly-focused storylines , to identify prevalent topics and key entities within these stories , and to reveal the temporal structure of stories as they evolve . We achieve this by building a hybrid clustering and topic model . To deal with the available wealth of data we build an efficient parallel inference algorithm by sequential Monte Carlo estimation . Time and memory costs are nearly constant in the length of the history , and the approach scales to hundreds of thousands of documents . We demonstrate the efficiency and accuracy on the publicly available TDT dataset and data of a major internet news site ."
"279","Talking about data : sharing richly structured information through blogs and wikis The web has dramatically enhanced people 's ability to communicate ideas , knowledge , and opinions . But the authoring tools that most people understand , blogs and wikis , primarily guide users toward authoring text . In this work , we show that substantial gains in expressivity and communication would accrue if people could easily share richly structured information in meaningful visualizations . We then describe several extensions we have created for blogs and wikis that enable users to publish , share , and aggregate such structured information using the same workflows they apply to text . In particular , we aim to preserve those attributes that make blogs and wikis so effective : one-click access to the information , one-click publishing of content , natural authoring interfaces , and the ability to easily copy-and-paste information and visualizations from other sources ."
"280","Stop thinking , start tagging : tag semantics emerge from collaborative verbosity Recent research provides evidence for the presence of emergent semantics in collaborative tagging systems . While several methods have been proposed , little is known about the factors that influence the evolution of semantic structures in these systems . A natural hypothesis is that the quality of the emergent semantics depends on the pragmatics of tagging : Users with certain usage patterns might contribute more to the resulting semantics than others . In this work , we propose several measures which enable a pragmatic differentiation of taggers by their degree of contribution to emerging semantic structures . We distinguish between categorizers , who typically use a small set of tags as a replacement for hierarchical classification schemes , and describers , who are annotating resources with a wealth of freely associated , descriptive keywords . To study our hypothesis , we apply semantic similarity measures to 64 different partitions of a real-world and large-scale folksonomy containing different ratios of categorizers and describers . Our results not only show that `` verbose '' taggers are most useful for the emergence of tag semantics , but also that a subset containing only 40 % of the most ` verbose ' taggers can produce results that match and even outperform the semantic precision obtained from the whole dataset . Moreover , the results suggest that there exists a causal link between the pragmatics of tagging and resulting emergent semantics . This work is relevant for designers and analysts of tagging systems interested ( i ) in fostering the semantic development of their platforms , ( ii ) in identifying users introducing `` semantic noise '' , and ( iii ) in learning ontologies ."
"281","The paths more taken : matching DOM trees to search logs for accurate webpage clustering An unsupervised clustering of the webpages on a website is a primary requirement for most wrapper induction and automated data extraction methods . Since page content can vary drastically across pages of one cluster ( e.g. , all product pages on amazon.com ) , traditional clustering methods typically use some distance function between the DOM trees representing a pair of webpages . However , without knowing which portions of the DOM tree are `` important , '' such distance functions might discriminate between similar pages based on trivial features ( e.g. , differing number of reviews on two product pages ) , or club together distinct types of pages based on superficial features present in the DOM trees of both ( e.g. , matching footer\/copyright ) , leading to poor clustering performance . We propose using search logs to automatically find paths in the DOM trees that mark out important portions of pages , e.g. , the product title in a product page . Such paths are identified via a global analysis of the entire website , whereby search data for popular pages can be used to infer good paths even for other pages that receive little or no search traffic . The webpages on the website are then clustered using these `` key '' paths . Our algorithm only requires information on search queries , and the webpages clicked in response to them ; there is no need for human input , and it does not need to be told which portion of a webpage the user found interesting . The resulting clusterings achieve an adjusted RAND score of over 0.9 on half of the websites ( a score of 1 indicating a perfect clustering ) , and 59 % better scores on average than competing algorithms . Besides leading to refined clusterings , these key paths can be useful in the wrapper induction process itself , as shown by the high degree of match between the key paths and the manually identified paths used in existing wrappers for these sites ( 90 % average precision ) ."
"282","Protocol-aware matching of web service interfaces for adapter development With the rapid growth in the number of online Web services , the problem of service adaptation has received significant attention . In matching and adaptation , the functional description of services including interface and data as well as behavioral descriptions are important . Existing work on matching and adaptation focuses only on one aspect . In this paper , we present a semi-automated matching approach that considers both service descriptions . We introduce two protocol-aware service interface matching algorithms , i.e. depth-based interface matching and iterative reference-based interface matching . These algorithms refine the results of interface matching by incorporating the ordering constraints imposed by business protocol definitions on service operations . We have implemented a prototype and performed experiments using the specification of synthetic and real-world Web services . Experiments show that the proposed approaches lead to a significant improvement in the quality of matching between services ."
"283","Randomization tests for distinguishing social influence and homophily effects Relational autocorrelation is ubiquitous in relational domains . This observed correlation between class labels of linked instances in a network ( e.g. , two friends are more likely to share political beliefs than two randomly selected people ) can be due to the effects of two different social processes . If social influence effects are present , instances are likely to change their attributes to conform to their neighbor values . If homophily effects are present , instances are likely to link to other individuals with similar attribute values . Both these effects will result in autocorrelated attribute values . When analyzing static relational networks it is impossible to determine how much of the observed correlation is due each of these factors . However , the recent surge of interest in social networks has increased the availability of dynamic network data . In this paper , we present a randomization technique for temporal network data where the attributes and links change over time . Given data from two time steps , we measure the gain in correlation and assess whether a significant portion of this gain is due to influence and\/or homophily . We demonstrate the efficacy of our method on semi-synthetic data and then apply the method to a real-world social networks dataset , showing the impact of both influence and homophily effects ."
"284","Tag ranking Social media sharing web sites like Flickr allow users to annotate images with free tags , which significantly facilitate Web image search and organization . However , the tags associated with an image generally are in a random order without any importance or relevance information , which limits the effectiveness of these tags in search and other applications . In this paper , we propose a tag ranking scheme , aiming to automatically rank the tags associated with a given image according to their relevance to the image content . We first estimate initial relevance scores for the tags based on probability density estimation , and then perform a random walk over a tag similarity graph to refine the relevance scores . Experimental results on a 50 , 000 Flickr photo collection show that the proposed tag ranking method is both effective and efficient . We also apply tag ranking into three applications : ( 1 ) tag-based image search , ( 2 ) tag recommendation , and ( 3 ) group recommendation , which demonstrates that the proposed tag ranking approach really boosts the performances of social-tagging related applications ."
"285","Hybrid keyword search auctions Search auctions have become a dominant source of revenue generation on the Internet . Such auctions have typically used per-click bidding and pricing . We propose the use of hybrid auctions where an advertiser can make a per-impression as well as a per-click bid , and the auctioneer then chooses one of the two as the pricing mechanism . We assume that the advertiser and the auctioneer both have separate beliefs ( called priors ) on the click-probability of an advertisement . We first prove that the hybrid auction is truthful , assuming that the advertisers are risk-neutral . We then show that this auction is superior to the existing per-click auction in multiple ways : We show that risk-seeking advertisers will choose only a per-impression bid whereas risk-averse advertisers will choose only a per-click bid , and argue that both kind of advertisers arise naturally . Hence , the ability to bid in a hybrid fashion is important to account for the risk characteristics of the advertisers . For obscure keywords , the auctioneer is unlikely to have a very sharp prior on the click-probabilities . In such situations , we show that having the extra information from the advertisers in the form of a per-impression bid can result in significantly higher revenue . An advertiser who believes that its click-probability is much higher than the auctioneer 's estimate can use per-impression bids to correct the auctioneer 's prior without incurring any extra cost . The hybrid auction can allow the advertiser and auctioneer to implement complex dynamic programming strategies to deal with the uncertainty in the click-probability using the same basic auction . The per-click and per-impression bidding schemes can only be used to implement two extreme cases of these strategies . As Internet commerce matures , we need more sophisticated pricing models to exploit all the information held by each of the participants . We believe that hybrid auctions could be an important step in this direction . The hybrid auction easily extends to multiple slots , and is also applicable to scenarios where the hybrid bidding is per-impression and per-action ( i.e. CPM and CPA ) , or per-click and per-action ( i.e. CPC and CPA ) ."
"286","Search shortcuts : driving users towards their goals Giving suggestions to users of Web-based services is a common practice aimed at enhancing their navigation experience . Major Web Search Engines usually provide `` Suggestions '' under the form of queries that are , to some extent , related to the current query typed by the user , and the knowledge learned from the past usage of the system . In this work we introduce `` Search Shortcuts '' as `` Successful '' queries allowed , in the past , users to satisfy their information needs . Differently from conventional suggestion techniques , our search shortcuts allows to evaluate effectiveness by exploiting a simple train-and-test approach . We have applied several Collaborative Filtering algorithms to this problem , evaluating them on a real query log data . We generate the shortcuts from all user sessions belonging to the testing set , and measure the quality of the shortcuts suggested by considering the similarity between them and the navigational user behavior ."
"287","Building term suggestion relational graphs from collective intelligence This paper proposes an effective approach to provide relevant search terms for conceptual Web search . ` Semantic Term Suggestion ' function has been included so that users can find the most appropriate query term to what they really need . Conventional approaches for term suggestion involve extracting frequently occurring key terms from retrieved documents . They must deal with term extraction difficulties and interference from irrelevant documents . In this paper , we propose a semantic term suggestion function called Collective Intelligence based Term Suggestion ( CITS ) . CITS provides a novel social-network based framework for relevant terms suggestion with a semantic graph of the search term without limiting to the specific query term . A visualization of semantic graph is presented to the users to help browsing search results from related terms in the semantic graph . The search results are ranked each time according to their relevance to the related terms in the entire query session . Comparing to two popular commercial search engines , a user study of 18 users on 50 search terms showed better user satisfactions and indicated the potential usefulness of proposed method in real-world search applications ."
"288","WPBench : a benchmark for evaluating the client-side performance of web 2.0 applications In this paper , a benchmark called WPBench is reported to evaluate the responsiveness of Web browsers for modern Web 2.0 applications . In WPBench , variations of servers and networks are removed and the benchmark result is the closest to what Web users would perceive . To achieve these , WPBench records users ' interactions with typical Web 2.0 applications , and then replays Web navigations when benchmarking browsers . The replay mechanism can emulate the actual user interactions and the characteristics of the servers and the networks in a consistent way independent of browsers so that any browser compliant to the standards can be benchmarked fairly . In addition to describing the design and generation of WPBench , we also report the WPBench comparison results on the responsiveness performance for three popular Web browsers : Internet Explorer , Firefox and Chrome ."
"289","Web 2.0 : blind to an accessible new world With the advent of Web 2.0 technologies , websites have evolved from static pages to dynamic , interactive Web-based applications with the ability to replicate common desktop functionality . However , for blind and visually impaired individuals who rely upon screen readers , Web 2.0 applications force them to adapt to an inaccessible use model . Many technologies , including WAI-ARIA , AJAX , and improved screen reader support , are rapidly evolving to improve this situation . However , simply combining them does not solve the problems of screen reader users . The main contributions of this paper are two models of interaction for screen reader users , for both traditional websites and Web 2.0 applications . Further contributions are a discussion of accessibility difficulties screen reader users encounter when interacting with Web 2.0 applications , a user workflow design model for improving Web 2.0 accessibility , and a set of design requirements for developers to ease the user 's burden and increase accessibility . These models , accessibility difficulties , and design implications are based directly on responses and lessons learned from usability research focusing on Web 2.0 usage and screen reader users . Without the conscious effort of Web engineers and designers , most blind and visually impaired users will shy away from using new Web 2.0 technology in favor of desktop based applications ."
"290","Buzz-based recommender system In this paper , we describe a buzz-based recommender system based on a large source of queries in an eCommerce application . The system detects bursts in query trends . These bursts are linked to external entities like news and inventory information to find the queries currently in-demand which we refer to as buzz queries . The system follows the paradigm of limited quantity merchandising , in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity , and improving activity and stickiness on the site . A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood ."
"291","How much can behavioral targeting help online advertising ? Behavioral Targeting ( BT ) is a technique used by online advertisers to increase the effectiveness of their campaigns , and is playing an increasingly important role in the online advertising market . However , it is underexplored in academia when looking at how much BT can truly help online advertising in commercial search engines . To answer this question , in this paper we provide an empirical study on the click-through log of advertisements collected from a commercial search engine . From the comprehensively experiment results on the sponsored search log of the commercial search engine over a period of seven days , we can draw three important conclusions : ( 1 ) Users who clicked the same ad will truly have similar behaviors on the Web ; ( 2 ) Click-Through Rate ( CTR ) of an ad can be averagely improved as high as 670 % by properly segmenting users for behavioral targeted advertising in a sponsored search ; ( 3 ) Using the short term user behaviors to represent users is more effective than using the long term user behaviors for BT . The statistical t-test verifies that all conclusions drawn in the paper are statistically significant . To the best of our knowledge , this work is the first empirical study for BT on the click-through log of real world ads ."
"292","Threshold selection for web-page classification with highly skewed class distribution We propose a novel cost-efficient approach to threshold selection for binary web-page classification problems with imbalanced class distributions . In many binary-classification tasks the distribution of classes is highly skewed . In such problems , using uniform random sampling in constructing sample sets for threshold setting requires large sample sizes in order to include a statistically sufficient number of examples of the minority class . On the other hand , manually labeling examples is expensive and budgetary considerations require that the size of sample sets be limited . These conflicting requirements make threshold selection a challenging problem . Our method of sample-set construction is a novel approach based on stratified sampling , in which manually labeled examples are expanded to reflect the true class distribution of the web-page population . Our experimental results show that using false positive rate as the criterion for threshold setting results in lower-variance threshold estimates than using other widely used accuracy measures such as F1 and precision ."
"293","Thumbs-up : a game for playing to rank search results Human computation is an effective way to channel human effort spent playing games to solving computational problems that are easy for humans but difficult for computers to automate . We propose Thumbs-Up , a new game for human computation with the purpose of playing to rank search result . Our experience from users shows that Thumbs-Up is not only fun to play , but produces more relevant rankings than both a major search engine and optimal rank aggregation using the Kemeny rule ."
"294","iRIN : image retrieval in image-rich information networks In this demo , we present a system called iRIN designed for performing image retrieval in image-rich information networks . We first introduce MoK-SimRank to significantly improve the speed of SimRank , one of the most popular algorithms for computing node similarity in information networks . Next , we propose an algorithm called SimLearn to ( 1 ) extend MoK-SimRank to heterogeneous image-rich information network , and ( 2 ) account for both link-based and content-based similarities by seamlessly integrating reinforcement learning with feature learning ."
"295","Deriving music theme annotations from user tags Music theme annotations would be really beneficial for supporting retrieval , but are often neglected by users while annotating . Thus , in order to support users in tagging and to fill the gaps in the tag space , in this paper we develop algorithms for recommending theme annotations . Our methods exploit already existing user tags , the lyrics of music tracks , as well as combinations of both . We compare the results for our recommended theme annotations against genre and style recommendations - a much easier and already studied task . We evaluate the quality of our recommended tags against an expert ground truth data set . Our results are promising and provide interesting insights into possible extensions for music tagging systems to support music search ."
"296","RuralCafe : web search in the rural developing world The majority of people in rural developing regions do not have access to the World Wide Web . Traditional network connectivity technologies have proven to be prohibitively expensive in these areas . The emergence of new long-range wireless technologies provide hope for connecting these rural regions to the Internet . However , the network connectivity provided by these new solutions are by nature intermittent due to high network usage rates , frequent power-cuts and the use of delay tolerant links . Typical applications , especially interactive applications like web search , do not tolerate intermittent connectivity . In this paper , we present the design and implementation of RuralCafe , a system intended to support efficient web search over intermittent networks . RuralCafe enables users to perform web search asynchronously and find what they are looking for in one round of intermittency as opposed to multiple rounds of search\/downloads . RuralCafe does this by providing an expanded search query interface which allows a user to specify additional query terms to maximize the utility of the results returned by a search query . Given knowledge of the limited available network resources , RuralCafe performs optimizations to prefetch pages to best satisfy a search query based on a user 's search preferences . In addition , RuralCafe does not require modifications to the web browser , and can provide single round search results tailored to various types of networks and economic constraints . We have implemented and evaluated the effectiveness of RuralCafe using queries from logs made to a large search engine , queries made by users in an intermittent setting , and live queries from a small testbed deployment . We have also deployed a prototype of RuralCafe in Kerala , India ."
"297","A class-feature-centroid classifier for text categorization Automated text categorization is an important technique for many web applications , such as document indexing , document filtering , and cataloging web resources . Many different approaches have been proposed for the automated text categorization problem . Among them , centroid-based approaches have the advantages of short training time and testing time due to its computational efficiency . As a result , centroid-based classifiers have been widely used in many web applications . However , the accuracy of centroid-based classifiers is inferior to SVM , mainly because centroids found during construction are far from perfect locations . We design a fast Class-Feature-Centroid ( CFC ) classifier for multi-class , single-label text categorization . In CFC , a centroid is built from two important class distributions : inter-class term index and inner-class term index . CFC proposes a novel combination of these indices and employs a denormalized cosine measure to calculate the similarity score between a text vector and a centroid . Experiments on the Reuters-21578 corpus and 20-newsgroup email collection show that CFC consistently outperforms the state-of-the-art SVM classifiers on both micro-F1 and macro-F1 scores . Particularly , CFC is more effective and robust than SVM when data is sparse ."
"298","Graffiti : node labeling in heterogeneous networks We introduce a multi-label classification model and algorithm for labeling heterogeneous networks , where nodes belong to different types and different types have different sets of classification labels . We present a graph-based approach which models the mutual influence between nodes in the network as a random walk . When viewing class labels as `` colors '' , the random surfer is `` spraying '' different node types with different color palettes ; hence the name Graffiti . We demonstrate the performance gains of our method by comparing it to three state-of-the-art techniques for graph-based classification ."
"299","Analyzing seller practices in a Brazilian marketplace E-commerce is growing at an exponential rate . In the last decade , there has been an explosion of online commercial activity enabled by World Wide Web ( WWW ) . These days , many consumers are less attracted to online auctions , preferring to buy merchandise quickly using fixed-price negotiations . Sales at Amazon.com , the leader in online sales of fixed-price goods , rose 37 % in the first quarter of 2008 . At eBay , where auctions make up 58 % of the site 's sales , revenue rose 14 % . In Brazil , probably by cultural influence , online auctions are not been popular . This work presents a characterization and analysis of fixed-price online negotiations . Using actual data from a Brazilian marketplace , we analyze seller practices , considering seller profiles and strategies . We show that different sellers adopt strategies according to their interests , abilities and experience . Moreover , we confirm that choosing a selling strategy is not simple , since it is important to consider the seller 's characteristics to evaluate the applicability of a strategy . The work also provides a comparative analysis of some selling practices in Brazil with popular worldwide marketplaces ."
"300","Releasing search queries and clicks privately The question of how to publish an anonymized search log was brought to the forefront by a well-intentioned , but privacy-unaware AOL search log release . Since then a series of ad-hoc techniques have been proposed in the literature , though none are known to be provably private . In this paper , we take a major step towards a solution : we show how queries , clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy . Our algorithm is decidedly simple to state , but non-trivial to analyze . On the opposite side of privacy is the question of whether the data we can safely publish is of any use . Our findings offer a glimmer of hope : we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log . In addition , we select an application , keyword generation , and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data ."
"301","Adaptive bidding for display advertising Motivated by the emergence of auction-based marketplaces for display ads such as the Right Media Exchange , we study the design of a bidding agent that implements a display advertising campaign by bidding in such a marketplace . The bidding agent must acquire a given number of impressions with a given target spend , when the highest external bid in the marketplace is drawn from an unknown distribution P. The quantity and spend constraints arise from the fact that display ads are usually sold on a CPM basis . We consider both the full information setting , where the winning price in each auction is announced publicly , and the partially observable setting where only the winner obtains information about the distribution ; these differ in the penalty incurred by the agent while attempting to learn the distribution . We provide algorithms for both settings , and prove performance guarantees using bounds on uniform closeness from statistics , and techniques from online learning . We experimentally evaluate these algorithms : both algorithms perform very well with respect to both target quantity and spend ; further , our algorithm for the partially observable case performs nearly as well as that for the fully observable setting despite the higher penalty incurred during learning ."
"302","Measuring the similarity between implicit semantic relations from the web Measuring the similarity between semantic relations that hold among entities is an important and necessary step in various Web related tasks such as relation extraction , information retrieval and analogy detection . For example , consider the case in which a person knows a pair of entities ( e.g. Google , YouTube ) , between which a particular relation holds ( e.g. acquisition ) . The person is interested in retrieving other such pairs with similar relations ( e.g. Microsoft , Powerset ) . Existing keyword-based search engines can not be applied directly in this case because , in keyword-based search , the goal is to retrieve documents that are relevant to the words used in a query -- not necessarily to the relations implied by a pair of words . We propose a relational similarity measure , using a Web search engine , to compute the similarity between semantic relations implied by two pairs of words . Our method has three components : representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns , clustering the extracted lexical patterns to identify the different patterns that express a particular semantic relation , and measuring the similarity between semantic relations using a metric learning approach . We evaluate the proposed method in two tasks : classifying semantic relations between named entities , and solving word-analogy questions . The proposed method outperforms all baselines in a relation classification task with a statistically significant average precision score of 0.74 . Moreover , it reduces the time taken by Latent Relational Analysis to process 374 word-analogy questions from 9 days to less than 6 hours , with an SAT score of 51 % ."
"303","SOFIE : a self-organizing framework for information extraction This paper presents SOFIE , a system for automated ontology extension . SOFIE can parse natural language documents , extract ontological facts from them and link the facts into an ontology . SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning , to reason on the meaning of text patterns and to take into account world knowledge axioms . This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology . The framework of SOFIE unites the paradigms of pattern matching , word sense disambiguation and ontological reasoning in one unified model . Our experiments show that SOFIE delivers high-quality output , even from unstructured Internet documents ."
"304","REST-based management of loosely coupled services Applications increasingly make use of the distributed platform that the World Wide Web provides - be it as a Software-as-a-Service such as salesforce.com , an application infrastructure such as facebook.com , or a computing infrastructure such as a `` cloud '' . A common characteristic of applications of this kind is that they are deployed on infrastructure or make use of components that reside in different management domains . Current service management approaches and systems , however , often rely on a centrally managed configuration management database ( CMDB ) , which is the basis for centrally orchestrated service management processes , in particular change management and incident management . The distribution of management responsibility of WWW based applications requires a decentralized approach to service management . This paper proposes an approach of decentralized service management based on distributed configuration management and service process co-ordination , making use RESTful access to configuration information and ATOM-based distribution of updates as a novel foundation for service management processes ."
"305","Retaining personal expression for social search Web is being extensively used for personal expression , which includes ratings , reviews , recommendations , blogs . This user created content , e.g. book review on Amazon.com , becomes the property of the website , and the user often does not have easy access to it . In some cases , user 's feedback may get averaged with feedback from other users e.g. ratings of a video . We argue that the creator of such content needs to be able to retain ( a link to ) her created content . We introduce the concept of MEB which is a user controlled store of such retained links . A MEB allows a user to access\/share all the reviews she has given on different websites . With this capability users can allow their friends to search through their feedback . Searching through one 's social network allows harnessing the power of social networks where known relationships provide the context & trust necessary to interpret feedback ."
"306","Bucefalo : a tool for intelligent search and filtering for web-based personal health records In this poster , a tool named BUCEFALO is presented . This tool is specially designed to improve the information retrieval tasks in web-based Personal Health Records ( PHR ) . This tool implements semantic and multilingual query expansion techniques and information filtering algorithms in order to help users find the most valuable information about a specific clinical case . The filtering model is based on fuzzy prototypes based filtering , data quality measures , user profiles and healthcare ontologies . The first experimental results illustrate the feasibility of this tool ."
"307","Instance-based probabilistic reasoning in the semantic web Most of the approaches for dealing with uncertainty in the Semantic Web rely on the principle that this uncertainty is already asserted . In this paper , we propose a new approach to learn and reason about uncertainty in the Semantic Web . Using instance data , we learn the uncertainty of an OWL ontology , and use that information to perform probabilistic reasoning on it . For this purpose , we use Markov logic , a new representation formalism that combines logic with probabilistic graphical models ."
"308","Crawling English-Japanese person-name transliterations from the web Automatic compilation of lexicon is a dream of lexicon compilers as well as lexicon users . This paper proposes a system that crawls English-Japanese person-name transliterations from the Web , which works a back-end collector for automatic compilation of bilingual person-name lexicon . Our crawler collected 561K transliterations in five months . From them , an English-Japanese person-name lexicon with 406K entries has been compiled by an automatic post processing . This lexicon is much larger than other similar resources including English-Japanese lexicon of HeiNER obtained from Wikipedia ."
"309","Bid optimization for broad match ad auctions Ad auctions in sponsored search support `` broad match '' that allows an advertiser to target a large number of queries while bidding only on a limited number . While giving more expressiveness to advertisers , this feature makes it challenging to optimize bids to maximize their returns : choosing to bid on a query as a broad match because it provides high profit results in one bidding for related queries which may yield low or even negative profits . We abstract and study the complexity of the -LCB- \ em bid optimization problem -RCB- which is to determine an advertiser 's bids on a subset of keywords ( possibly using broad match ) so that her profit is maximized . In the query language model when the advertiser is allowed to bid on all queries as broad match , we present a linear programming ( LP ) - based polynomial-time algorithm that gets the optimal profit . In the model in which an advertiser can only bid on keywords , ie. , a subset of keywords as an exact or broad match , we show that this problem is not approximable within any reasonable approximation factor unless P = NP . To deal with this hardness result , we present a constant-factor approximation when the optimal profit significantly exceeds the cost . This algorithm is based on rounding a natural LP formulation of the problem . Finally , we study a budgeted variant of the problem , and show that in the query language model , one can find two budget constrained ad campaigns in polynomial time that implement the optimal bidding strategy . Our results are the first to address bid optimization under the broad match feature which is common in ad auctions ."
"310","Matchbox : large scale online bayesian recommendations We present a probabilistic model for generating personalised recommendations of items to users of a web service . The Matchbox system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user . Users and items are represented by feature vectors which are mapped into a low-dimensional ` trait space ' in which similarity is measured in terms of inner products . The model can be trained from different types of feedback in order to learn user-item preferences . Here we present three alternatives : direct observation of an absolute rating each user gives to some items , observation of a binary preference ( like \/ do n't like ) and observation of a set of ordinal ratings on a user-specific scale . Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation ( EP ) and Variational Message Passing . We also include a dynamics model which allows an item 's popularity , a user 's taste or a user 's personal rating scale to drift over time . By using Assumed-Density Filtering ( ADF ) for training , the model requires only a single pass through the training data . This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences . We evaluate the performance of the algorithm on the MovieLens and Netflix data sets consisting of approximately 1,000,000 and 100,000,000 ratings respectively . This demonstrates that training the model using the on-line ADF approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple EP passes over the training data ."
"311","A game based approach to assign geographical relevance to web images Geographical context is very important for images . Millions of images on the Web have been already assigned latitude and longitude information . Due to the rapid proliferation of such images with geographical context , it is still difficult to effectively search and browse them , since we do not have ways to decide their relevance . In this paper , we focus on the geographical relevance of images , which is defined as to what extent the main objects in an image match landmarks at the location where the image was taken . Recently , researchers have proposed to use game based approaches to label large scale data such as Web images . However , previous works have not shown the quality of collected game logs in detail and how the logs can improve existing applications . To answer these questions , we design and implement a Web-based and multi-player game to collect human knowledge while people are enjoying the game . Then we thoroughly analyze the game logs obtained during a three week study with 147 participants and propose methods to determine the image geographical relevance . In addition , we conduct an experiment to compare our methods with a commercial search engine . Experimental results show that our methods dramatically improve image search relevance . Furthermore , we show that we can derive geographically relevant objects and their salient portion in images , which is valuable for a number of applications such as image location recognition ."
"312","Combining anchor text categorization and graph analysis for paid link detection In order to artificially boost the rank of commercial pages in search engine results , search engine optimizers pay for links to these pages on other websites . Identifying paid links is important for a web search engine to produce highly relevant results . In this paper we introduce a novel method of identifying such links . We start with training a classifier of anchor text topics and analyzing web pages for diversity of their outgoing commercial links . Then we use this information and analyze link graph of the Russian Web to find pages that sell links and sites that buy links and to identify the paid links . Testing on manually marked samples showed high efficiency of the algorithm ."
"313","Predicting click through rate for job listings Click Through Rate ( CTR ) is an important metric for ad systems , job portals , recommendation systems . CTR impacts publisher 's revenue , advertiser 's bid amounts in `` pay for performance '' business models . We learn regression models using features of the job , optional click history of job , features of `` related '' jobs . We show that our models predict CTR much better than predicting avg . CTR for all job listings , even in absence of the click history for the job listing ."
"314","Unsupervised query categorization using automatically-built concept graphs Automatic categorization of user queries is an important component of general purpose ( Web ) search engines , particularly for triggering rich , query-specific content and sponsored links . We propose an unsupervised learning scheme that reduces dramatically the cost of setting up and maintaining such a categorizer , while retaining good categorization power . The model is stored as a graph of concepts where graph edges represent the cross-reference between the concepts . Concepts and relations are extracted from query logs by an offline Web mining process , which uses a search engine as a powerful summarizer for building a concept graph . Empirical evaluation indicates that the system compares favorably on publicly available data sets ( such as KDD Cup 2005 ) as well as on portions of the current query stream of Yahoo ! Search , where it is already changing the experience of millions of Web search users ."
"315","A P2P based distributed services network for next generation mobile internet communications In this poster , we present a novel P2P ( Peer to Peer ) based distributed services network ( DSN ) , which is a next generation operable and manageable distributed core network architecture and functional structure , proposed by China Mobile for telecommunication services and wireless Internet . Our preliminary implementations of P2P VoIP ( Voice over Internet Protocol ) system over DSN platform demonstrate its effectiveness and promising future ."
"316","Keyword extraction for social snippets Today , a huge amount of text is being generated for social purposes on social networking services on the Web . Unlike traditional documents , such text is usually extremely short and tends to be informal . Analysis of such text benefit many applications such as advertising , search , and content filtering . In this work , we study one traditional text mining task on such new form of text , that is extraction of meaningful keywords . We propose several intuitive yet useful features and experiment with various classification models . Evaluation is conducted on Facebook data . Performances of various features and models are reported and compared ."
"317","The slashdot zoo : mining a social network with negative edges We analyze the corpus of user relationships of the Slashdot technology news site . The data was collected from the Slashdot Zoo feature where users of the website can tag other users as friends and foes , providing positive and negative endorsements . We adapt social network analysis techniques to the problem of negative edge weights . In particular , we consider signed variants of global network characteristics such as the clustering coefficient , node-level characteristics such as centrality and popularity measures , and link-level characteristics such as distances and similarity measures . We evaluate these measures on the task of identifying unpopular users , as well as on the task of predicting the sign of links and show that the network exhibits multiplicative transitivity which allows algebraic methods based on matrix multiplication to be used . We compare our methods to traditional methods which are only suitable for positively weighted edges ."
"318","Web service derivatives Web service development and usage has shifted from simple information processing services to high-value business services that are crucial to productivity and success . In order to deal with an increasing risk of unavailability or failure of mission-critical Web services we argue the need for advanced reservation of services in the form of derivatives . The contribution of this paper is twofold : First we provide an abstract model of a market design that enables the trade of derivatives for mission-critical Web services . Our model satisfies requirements that result from service characteristics such as intangibility and the impossibility to inventor services in order to meet fluctuating demand . It comprehends principles from models of incomplete markets such as the absence of a tradeable underlying and consistent arbitrage-free derivative pricing . Furthermore we provide an architecture for a Web service market that implements our model and describes the strategy space and interaction of market participants in the trading process of service derivatives . We compare the underlying pricing processes to existing derivative models in energy exchanges , discuss eventual shortcomings , and apply Wavelets to analyze actual data and extract long - and short-term trends ."
"319","Answering approximate queries over autonomous web databases To deal with the problem of empty or too little answers returned from a Web database in response to a user query , this paper proposes a novel approach to provide relevant and ranked query results . Based on the user original query , we speculate how much the user cares about each specified attribute and assign a corresponding weight to it . This original query is then rewritten as an approximate query by relaxing the query criteria range . The relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights . For the approximate query results , we generate users ' contextual preferences from database workload and use them to create a priori orders of tuples in an off-line preprocessing step . Only a few representative orders are saved , each corresponding to a set of contexts . Then , these orders and associated contexts are used at query time to expeditiously provide ranked answers . Results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user 's preferences effectively . The efficiency and effectiveness of our approach is also demonstrated by experimental result ."
"320","RankCompete : simultaneous ranking and clustering of web photos With the explosive growth of digital cameras and online media , it has become crucial to design efficient methods that help users browse and search large image collections . The recent VisualRank algorithm ( 4 ) employs visual similarity to represent the link structure in a graph so that the classic PageRank algorithm can be applied to select the most relevant images . However , measuring visual similarity is difficult when there exist diversified semantics in the image collection , and the results from VisualRank can not supply good visual summarization with diversity . This paper proposes to rank the images in a structural fashion , which aims to discover the diverse structure embedded in photo collections , and rank the images according to their similarity among local neighborhoods instead of across the entire photo collection . We design a novel algorithm named RankCompete , which generalizes the PageRank algorithm for the task of simultaneous ranking and clustering . The experimental results show that RankCompete outperforms VisualRank and provides an efficient but effective tool for organizing web photos ."
"321","Detecting image spam using local invariant features and pyramid match kernel Image spam is a new obfuscating method which spammers invented to more effectively bypass conventional text based spam filters . In this paper , we extract local invariant features of images and run a one-class SVM classifier which uses the pyramid match kernel as the kernel function to detect image spam . Experimental results demonstrate that our algorithm is effective for fighting image spam ."
"322","Topic initiator detection on the world wide web In this paper we introduce a new Web mining and search technique - Topic Initiator Detection ( TID ) on the Web . Given a topic query on the Internet and the resulting collection of time-stamped web documents which contain the query keywords , the task of TID is to automatically return which web document ( or its author ) initiated the topic or was the first to discuss about the topic . To deal with the TID problem , we design a system framework and propose algorithm InitRank ( Initiator Ranking ) to rank the web documents by their possibility to be the topic initiator . We first extract features from the web documents and design several topic initiator indicators . Then , we propose a TCL graph which integrates the Time , Content and Link information and design an optimization framework over the graph to compute InitRank . Experiments show that compared with baseline methods , such as direct time sorting , well-known link based ranking algorithms PageRank and HITS , InitRank achieves the best overall performance with high effectiveness and robustness . In case studies , we successfully detected ( 1 ) the first web document related to a famous rumor of an Australia product banned in USA and ( 2 ) the pre-release of IBM and Google Cloud Computing collaboration before the official announcement ."
"323","User-centric content freshness metrics for search engines In order to return relevant search results , a search engine must keep its local repository synchronized to the Web , but it is usually impossible to attain perfect freshness . Hence , it is vital for a production search engine continually to monitor and improve repository freshness . Most previous freshness metrics , formulated in the context of developing better synchronization policies , focused on the web crawler while ignoring other parts of a search engine . But , the freshness of documents in a web crawler does not necessarily translate directly into the freshness of search results as seen by users . We propose metrics for measuring freshness from a user 's perspective , which take into account the latency between when documents are crawled and when they are viewed by users , as well as the variation in user click and view frequency among different documents . We also describe a practical implementation of these metrics that were used in a production search engine ."
"324","Semantic wiki aided business process specification This paper formulates a collaborative system for modeling business application . The system uses a Semantic Wiki to enable collaboration between the various stakeholders involved in the design of the system and translates the captured intelligence into business models which are used for designing a business system ."
"325","Mining cultural differences from a large number of geotagged photos We propose a novel method to detect cultural differences over the world automatically by using a large amount of geotagged images on the photo sharingWeb sites such as Flickr . We employ the state-of-the-art object recognition technique developed in the research community of computer vision to mine representative photos of the given concept for representative local regions from a large-scale unorganized collection of consumer-generated geotagged photos . The results help us understand how objects , scenes or events corresponding to the same given concept are visually different depending on local regions over the world ."
"326","Co-browsing dynamic web pages Collaborative browsing , or co-browsing , is the co-navigation of the web with other people at-a-distance , supported by software that takes care of synchronizing the browsers . Current state-of-the-art solutions are able to do co-browsing of `` static web pages '' , and do not support the synchronization of JavaScript interactions . However , currently many web pages use JavaScript and Ajax techniques to create highly dynamic and interactive web applications . In this paper , we describe two approaches for co-browsing that both support the synchronization of the JavaScript and Ajax interactions of dynamic web pages . One approach is based on synchronizing the output of the JavaScript engine by sending over the changes made on the DOM tree . The other approach is based on synchronizing the input of the JavaScript engine by synchronizing UI events and incoming data . Since the latter solution offers a better user experience and is more scalable , it is elaborated in more detail . An important aspect of both approaches is that they operate at the DOM level . Therefore , the client-side can be implemented in JavaScript and no browser extensions are required . To the best of the authors ' knowledge this is the first DOM-level co-browsing solution that also enables co-browsing of the dynamic interaction parts of web pages . The presented co-browsing solution has been implemented in a research demonstrator which allows users to do co-browsing of web-applications on browser-based networked televisions ."
"327","A messaging API for inter-widgets communication Widget containers are used everywhere on the Web , for instance as customizable start pages to Web desktops . In this poster , we describe the extension of a widget container with an inter-widgets communication layer , as well as the subsequent application programming interfaces ( APIs ) added to the Widget object to support this feature . We present the benefits of a drag and drop facility within widgets and conclude by a call for standardization of inter-widgets communication on the Web ."
"328","Towards lightweight and efficient DDOS attacks detection for web server In this poster , based on our previous work in building a lightweight DDoS ( Distributed Denial-of-Services ) attacks detection mechanism for web server using TCM-KNN ( Transductive Confidence Machines for K-Nearest Neighbors ) and genetic algorithm based instance selection methods , we further propose a more efficient and effective instance selection method , named E-FCM ( Extend Fuzzy C-Means ) . By using this method , we can obtain much cheaper training time for TCM-KNN while ensuring high detection performance . Therefore , the optimized mechanism is more suitable for lightweight DDoS attacks detection in real network environment ."
"329","Web image retrieval reranking with multi-view clustering General image retrieval is often carried out by a text-based search engine , such as Google Image Search . In this case , natural language queries are used as input to the search engine . Usually , the user queries are quite ambiguous and the returned results are not well-organized as the ranking often done by the popularity of an image . In order to address these problems , we propose to use both textual and visual contents of retrieved images to reRank web retrieved results . In particular , a machine learning technique , a multi-view clustering algorithm is proposed to reorganize the original results provided by the text-based search engine . Preliminary results validate the effectiveness of the proposed framework ."
"330","Sitemaps : above and beyond the crawl of duty Comprehensive coverage of the public web is crucial to web search engines . Search engines use crawlers to retrieve pages and then discover new ones by extracting the pages ' outgoing links . However , the set of pages reachable from the publicly linked web is estimated to be significantly smaller than the invisible web , the set of documents that have no incoming links and can only be retrieved through web applications and web forms . The Sitemaps protocol is a fast-growing web protocol supported jointly by major search engines to help content creators and search engines unlock this hidden data by making it available to search engines . In this paper , we perform a detailed study of how `` classic '' discovery crawling compares with Sitemaps , in key measures such as coverage and freshness over key representative websites as well as over billions of URLs seen at Google . We observe that Sitemaps and discovery crawling complement each other very well , and offer different tradeoffs ."
"331","Automatically filling form-based web interfaces with free text inputs On the web of today the most prevalent solution for users to interact with data-intensive applications is the use of form-based interfaces composed by several data input fields , such as text boxes , radio buttons , pull-down lists , check boxes , etc. . Although these interfaces are popular and effective , in many cases , free text interfaces are preferred over form-based ones . In this paper we discuss the proposal and the implementation of a novel IR-based method for using data rich free text to interact with form-based interfaces . Our solution takes a free text as input , extracts implicitly data values from it and fills appropriate fields using them . For this task , we rely on values of previous submissions for each field , which are freely obtained from the usage of form-based interfaces"
"332","Searching for events in the blogosphere Over the last few years , blogs ( web logs ) have gained massive popularity and have become one of the most influential web social media in our times . Every blog post in the Blogosphere has a well defined timestamp , which is not taken into account by search engines . By conducting research regarding this feature of the Blogosphere , we can attempt to discover bursty terms and correlations between them during a time interval . We apply Kleinberg 's automaton on extracted titles of blog posts to discover bursty terms , we introduce a novel representation of a term 's burstiness evolution called State Series and we employ a Euclidean-based distance metric to discover potential correlations between terms without taking into account their context . We evaluate the results trying to match them with real life events . Finally , we propose some ideas for further evaluation techniques and future research in the field ."
"333","Compressed web indexes Web search engines use indexes to efficiently retrieve pages containing specified query terms , as well as pages linking to specified pages . The problem of compressed indexes that permit such fast retrieval has a long history . We consider the problem : assuming that the terms in ( or links to ) a page are generated from a probability distribution , how well compactly can we build such indexes that allow fast retrieval ? Of particular interest is the case when the probability distribution is Zipfian ( or a similar power law ) , since these are the distributions that arise on the web . We obtain sharp bounds on the space requirement of Boolean indexes for text documents that follow Zipf 's law . In the process we develop a general technique that applies to any probability distribution , not necessarily a power law ; this is the first analysis of compression in indexes under arbitrary distributions . Our bounds lead to quantitative versions of rules of thumb that are folklore in indexing . Our experiments on several document collections show that the distribution of terms appears to follow a double-Pareto law rather than Zipf 's law . Despite widely varying sets of documents , the index sizes observed in the experiments conform well to our theoretical predictions ."
"334","Efficient overlap and content reuse detection in blogs and online news articles The use of blogs to track and comment on real world ( political , news , entertainment ) events is growing . Similarly , as more individuals start relying on the Web as their primary information source and as more traditional media outlets try reaching consumers through alternative venues , the number of news sites on the Web is also continuously increasing . Content-reuse , whether in the form of extensive quotations or content borrowing across media outlets , is very common in blogs and news entries outlets tracking the same real-world event . Knowledge about which web entries re-use content from which others can be an effective asset when organizing these entries for presentation . On the other hand , this knowledge is not cheap to acquire : considering the size of the related space web entries , it is essential that the techniques developed for identifying re-use are fast and scalable . Furthermore , the dynamic nature of blog and news entries necessitates incremental processing for reuse detection . In this paper , we develop a novel qSign algorithm that efficiently and effectively analyze the blogosphere for quotation and reuse identification . Experiment results show that with qSign processing time gains from 10X to 100X are possible while maintaining reuse detection rates of upto 90 % . Furthermore , processing time gains can be pushed multiple orders of magnitude ( from 100X to 1000X ) for 70 % recall ."
"335","SGPS : a semantic scheme for web service similarity Today 's Web becomes a platform for services to be dynamically interconnected to produce a desired outcome . It is important to formalize the semantics of the contextual elements of web services . In this paper , we propose a novel technique called Semantic Genome Propagation Scheme ( SGPS ) for measuring similarity between semantic concepts . We show how SGPS is used to compute a multi-dimensional similarity between two services . We evaluate the SGPS similarity measurement in terms of the similarity performance and scalability ."
"336","Dataplorer : a scalable search engine for the data web More and more structured information in the form of semantic data is nowadays available . It offers a wide range of new possibilities especially for semantic search and Web data integration . However , their effective exploitation still brings about a number of challenges , e.g. usability , scalability and uncertainty . In this paper , we present Dataplorer , a solution designed to address these challenges . We consider the usability through the use of hybrid queries and faceted search , while still preserving the scalability thanks to an extension of inverted index to support this type of query . Moreover , Dataplorer deals with uncertainty by means of a powerful ranking scheme to find relevant results . Our experimental results show that our proposed approach is promising and it makes us believe that it is possible to extend the current IR infrastructure to query and search the Web of data ."
"337","Spatio-temporal models for estimating click-through rate We propose novel spatio-temporal models to estimate click-through rates in the context of content recommendation . We track article CTR at a fixed location over time through a dynamic Gamma-Poisson model and combine information from correlated locations through dynamic linear regressions , significantly improving on per-location model . Our models adjust for user fatigue through an exponential tilt to the first-view CTR ( probability of click on first article exposure ) that is based only on user-specific repeat-exposure features . We illustrate our approach on data obtained from a module ( Today Module ) published regularly on Yahoo ! Front Page and demonstrate significant improvement over commonly used baseline methods . Large scale simulation experiments to study the performance of our models under different scenarios provide encouraging results . Throughout , all modeling assumptions are validated via rigorous exploratory data analysis ."
"338","OpenRuleBench : an analysis of the performance of rule engines The Semantic Web initiative has led to an upsurge of the interest in rules as a general and powerful way of processing , combining , and analyzing semantic information . Since several of the technologies underlying rule-based systems are already quite mature , it is important to understand how such systems might perform on the Web scale . OpenRuleBench is a suite of benchmarks for analyzing the performance and scalability of different rule engines . Currently the study spans five different technologies and eleven systems , but OpenRuleBench is an open community resource , and contributions from the community are welcome . In this paper , we describe the tested systems and technologies , the methodology used in testing , and analyze the results ."
"339","Smart Miner : a new framework for mining large scale web usage data In this paper , we propose a novel framework called Smart-Miner for web usage mining problem which uses link information for producing accurate user sessions and frequent navigation patterns . Unlike the simple session concepts in the time and navigation based approaches , where sessions are sequences of web pages requested from the server or viewed in the browser , Smart Miner sessions are set of paths traversed in the web graph that corresponds to users ' navigations among web pages . We have modeled session construction as a new graph problem and utilized a new algorithm , Smart-SRA , to solve this problem efficiently . For the pattern discovery phase , we have developed an efficient version of the Apriori-All technique which uses the structure of web graph to increase the performance . From the experiments that we have performed on both real and simulated data , we have observed that Smart-Miner produces at least 30 % more accurate web usage patterns than other approaches including previous session construction methods . We have also studied the effect of having the referrer information in the web server logs to show that different versions of Smart-SRA produce similar results . Our another contribution is that we have implemented distributed version of the Smart Miner framework by employing Map\/Reduce Paradigm . We conclude that we can efficiently process terabytes of web server logs belonging to multiple web sites by our scalable framework ."
"340","Rapid prototyping of semantic mash-ups through semantic web pipes The use of RDF data published on the Web for applications is still a cumbersome and resource-intensive task due to the limited software support and the lack of standard programming paradigms to deal with everyday problems such as combination of RDF data from dierent sources , object identifier consolidation , ontology alignment and mediation , or plain querying and filtering tasks . In this paper we present a framework , Semantic Web Pipes , that supports fast implementation of Semantic data mash-ups while preserving desirable properties such as abstraction , encapsulation , component-orientation , code re-usability and maintainability which are common and well supported in other application areas ."
"341","Crosslanguage blog mining and trend visualisation People use weblogs to express thoughts , present ideas and share knowledge , therefore weblogs are extraordinarily valuable resources , amongs others , for trend analysis . Trends are derived from the chronological sequence of blog post count per topic . The comparison with a reference corpus allows qualitative statements over identified trends . We propose a crosslanguage blog mining and trend visualisation system to analyze blogs across languages and topics . The trend visualisation facilitates the identification of trends and the comparison with the reference news article corpus . To prove the correctness of our system we computed the correlation between trends in blogs and news articles for a subset of blogs and topics . The evaluation corroborated our hypothesis of a high correlation coefficient for these subsets and therefore the correctness of our system for different languages and topics is proven ."
"342","An experimental study of large-scale mobile social network Mobile social network is a typical social network where one or more individuals of similar interests or commonalities , conversing and connecting with one another using the mobile phone . Our works in this paper focus on the experimental study for this kind of social network with the support of large-scale real mobile call data . The main contributions can be summarized as three-fold : firstly , a large-scale real mobile phone call log of one city has been extracted from a mobile phone carrier in China to construct mobile social network ; secondly , common features of traditional social networks , such as power law distribution and small diameter etc , have been experimented , with which we confirm that the mobile social network is a typical scale-free network and has small-world phenomenon ; lastly , different from traditional analytical methods , important properties of the actors , such as gender and age , have been introduced into our experiments with some interesting findings about human behavior , for example , the middle-age people are more active than the young and old people , and the female is unusual more active than the male while in the old age ."
"343","Detecting soft errors by redirection classification A soft error redirection is a URL redirection to a page that returns the HTTP status code 200 ( OK ) but has actually no relevant content to the client request . Since such redirections degrade the performance of web search engines in many ways , it is highly desirable to remove as many of them as possible . We propose a novel approach to detect soft error redirections by analyzing redirection logs collected during crawling operation . Experimental results on huge crawl data show that our measure can classify soft error redirections effectively ."
"344","Nearest-neighbor caching for content-match applications Motivated by contextual advertising systems and other web applications involving efficiency-accuracy tradeoffs , we study similarity caching . Here , a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item . We study two objectives that dictate the efficiency-accuracy tradeoff and provide our caching policies for these objectives . By conducting extensive experiments on real data we show similarity caching can significantly improve the efficiency of contextual advertising systems , with minimal impact on accuracy . Inspired by the above , we propose a simple generative model that embodies two fundamental characteristics of page requests arriving to advertising systems , namely , long-range dependences and similarities . We provide theoretical bounds on the gains of similarity caching in this model and demonstrate these gains empirically by fitting the actual data to the model ."
"345","XQuery in the browser Since the invention of the Web , the browser has become more and more powerful . By now , it is a programming and execution environment in itself . The predominant language to program applications in the browser today is JavaScript . With browsers becoming more powerful , JavaScript has been extended and new layers have been added ( e.g. , DOM-Support and XPath ) . Today , JavaScript is very successful and applications and GUI features implemented in the browser have become increasingly complex . The purpose of this paper is to improve the programmability of Web browsers by enabling the execution of XQuery programs in the browser . Although it has the potential to ideally replace JavaScript , it is possible to run it in addition to JavaScript for more flexibility . Furthermore , it allows instant code migration from the server to the client and vice-versa . This enables a significant simplification of the technology stack . The intuition is that programming the browser involves mostly XML ( i.e. , DOM ) navigation and manipulation , and the XQuery family of W3C standards were designed exactly for that purpose . The paper proposes extensions to XQuery for Web browsers and gives a number of examples that demonstrate the usefulness of XQuery for the development of AJAX-style applications . Furthermore , the paper presents the design of an XQuery plug-in for Microsoft 's Internet Explorer . The paper also gives examples of applications which were developed with the help of this plug-in ."
"346","Raise semantics at the user level for dynamic and interactive SOA-based portals In this paper , we describe the fully dynamic semantic portal we implemented , integrating Semantic Web technologies and Service Oriented Architecture ( SOA ) . The goals of the portal are twofold : first it helps administrators to easily propose new features in the portal using semantics to ease the orchestration process ; secondly it automatically generates a customized user interface for these scenarios . This user interface takes into account different devices and assists end-users in the use of the portal taking benefit of context awareness . All the added-value of this portal is based on a core semantics defined by an ontology . We present here the main features of this portal and how it was implemented using state-of-the-art technologies and frameworks ."
"347","Search result re-ranking based on gap between search queries and social tags Both search engine click-through log and social annotation have been utilized as user feedback for search result re-ranking . However , to our best knowledge , no previous study has explored the correlation between these two factors for the task of search result ranking . In this paper , we show that the gap between search queries and social tags of the same Web page can well reflect its user preference score . Motivated by this observation , we propose a novel algorithm , called Query-Tag-Gap ( QTG ) , to re-rank search results for better user satisfaction . Intuitively , on one hand , the search users ' intentions are generally described by their queries before they read the search results . On the other hand , the Web annotators semantically tag Web pages after they read the content of the pages . The difference between users ' recognition of the same page before and after they read it is a good reflection of user satisfaction . In this extended abstract , we formally define the query set and tag set of the same page as users ' pre - and post - knowledge respectively . We empirically show the strong correlation between user satisfaction and user 's knowledge gap before and after reading the page . Based on this gap , experiments have shown outstanding performance of our proposed QTG algorithm in search result re-ranking ."
"348","Content hole search in community-type content In community-type content such as blogs and SNSs , we call the user 's unawareness of information as a `` content hole '' and the search for this information as a `` content hole search . '' A content hole search differs from similarity searching and has a variety of types . In this paper , we propose different types of content holes and define each type . We also propose an analysis of dialogue related to community-type content and introduce content hole search by using Wikipedia as an example ."
"349","Exploiting web search to generate synonyms for entities Tasks recognizing named entities such as products , people names , or locations from documents have recently received significant attention in the literature . Many solutions to these tasks assume the existence of reference entity tables . An important challenge that needs to be addressed in the entity extraction task is that of ascertaining whether or not a candidate string approximately matches with a named entity in a given reference table . Prior approaches have relied on string-based similarity which only compare a candidate string and an entity it matches with . In this paper , we exploit web search engines in order to define new similarity functions . We then develop efficient techniques to facilitate approximate matching in the context of our proposed similarity functions . In an extensive experimental evaluation , we demonstrate the accuracy and efficiency of our techniques ."
"350","Automated construction of web accessibility models from transaction click-streams Screen readers , the dominant assistive technology used by visually impaired people to access the Web , function by speaking out the content of the screen serially . Using screen readers for conducting online transactions can cause considerable information overload , because transactions , such as shopping and paying bills , typically involve a number of steps spanning several web pages . One can combat this overload by using a transaction model for web accessibility that presents only fragments of web pages that are needed for doing transactions . We can realize such a model by coupling a process automaton , encoding states of a transaction , with concept classifiers that identify page fragments `` relevant '' to a particular state of the transaction . In this paper we present a fully automated process that synergistically combines several techniques for transforming unlabeled click-stream data generated by transactions into a transactionmodel . These techniques include web content analysis to partition a web page into segments consisting of semantically related content , contextual analysis of data surrounding clickable objects in a page , and machine learning methods , such as clustering of page segments based on contextual analysis , statistical classification , and automata learning . The use of unlabeled click streams in building transaction models has important benefits : ( i ) visually impaired users do not have to depend on sighted users for creating manually labeled training data to construct the models ; ( ii ) it is possible to mine personalized models from unlabeled transaction click-streams associated with sites that visually impaired users visit regularly ; ( iii ) since unlabeled data is relatively easy to obtain , it is feasible to scale up the construction of domain-specific transaction models ( e.g. , separate models for shopping , airline reservations , bill payments , etc. ) ; ( iv ) adjusting the performance of deployed models over timtime with new training data is also doable . We provide preliminary experimental evidence of the practical effectiveness of both domain-specific , as well as personalized accessibility transaction models built using our approach . Finally , this approach is applicable for building transaction models for mobile devices with limited-size displays , as well as for creating wrappers for information extraction from web sites ."
"351","Rare item detection in e-commerce site As the largest online marketplace in the world , eBay has a huge inventory where there are plenty of great rare items with potentially large , even rapturous buyers . These items are obscured in long tail of eBay item listing and hard to find through existing searching or browsing methods . It is observed that there are great rarity demands from users according to eBay query log . To keep up with the demands , the paper proposes a method to automatically detect rare items in eBay online listing . A large set of features relevant to the task are investigated to filter items and further measure item rareness . The experiments on the most rarity-demand-intensitive domains show that the method may effectively detect rare items ( ) 90 % precision ) ."
"352","Fast dynamic reranking in large graphs In this paper we consider the problem of re-ranking search results by incorporating user feedback . We present a graph theoretic measure for discriminating irrelevant results from relevant results using a few labeled examples provided by the user . The key intuition is that nodes relatively closer ( in graph topology ) to the relevant nodes than the irrelevant nodes are more likely to be relevant . We present a simple sampling algorithm to evaluate this measure at specific nodes of interest , and an efficient branch and bound algorithm to compute the top k nodes from the entire graph under this measure . On quantifiable prediction tasks the introduced measure outperforms other diffusion-based proximity measures which take only the positive relevance feedback into account . On the Entity-Relation graph built from the authors and papers of the entire DBLP citation corpus ( 1.4 million nodes and 2.2 million edges ) our branch and bound algorithm takes about 1.5 seconds to retrieve the top 10 nodes w.r.t. this measure with 10 labeled nodes ."
"353","Community gravity : measuring bidirectional effects by trust and rating on online social networks Several attempts have been made to analyze customer behavior on online E-commerce sites . Some studies particularly emphasize the social networks of customers . Users ' reviews and ratings of a product exert effects on other consumers ' purchasing behavior . Whether a user refers to other users ' ratings depends on the trust accorded by a user to the reviewer . On the other hand , the trust that is felt by a user for another user correlates with the similarity of two users ' ratings . This bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online communities because it suggests clustering of similar users and the evolution of strong communities . This paper presents a theoretical model along with analyses of an actual online E-commerce site . We analyzed a large community site in Japan : @cosme . The noteworthy characteristics of @cosme are that users can bookmark their trusted users ; in addition , they can post their own ratings of products , which facilitates our analyses of the ratings ' bidirectional effects on trust and ratings . We describe an overview of the data in @cosme , analyses of effects from trust to rating and vice versa , and our proposition of a measure of community gravity , which measures how strongly a user might be attracted to a community . Our study is based on the @cosme dataset in addition to the Epinions dataset . It elucidates important insights and proposes a potentially important measure for mining online social networks ."
"354","A flight meta-search engine with metamorph We demonstrate a flight meta-search engine that is based on the Metamorph framework . Metamorph provides mechanisms to model web forms together with the interactions which are needed to fulfil a request , and can generate interaction sequences that pose queries using these web forms and collect the results . In this paper , we discuss an interesting new feature that makes use of the forms themselves as an information source . We show how data can be extracted from web forms ( rather than the data behind web forms ) to generate a graph of flight connections between cities . The flight connection graph allows us to vastly reduce the number of queries that the engine sends to airline websites in the most interesting search scenarios ; those that involve the controversial practice of creative ticketing , in which agencies attempt to find lower price fares by using more than one airline for a journey . We describe a system which attains data from a number of websites to identify promising routes and prune the search tree . Heuristics that make use of geographical information and an estimation of cost based on historical data are employed . The results are then made available to improve the quality of future search requests ."
"355","The value of socially tagged urls for a search engine Social bookmarking has emerged as a growing source of human generated content on the web . In essence , bookmarking involves URLs and tags on them . In this paper , we perform a large scale study of the usefulness of bookmarked URLs from the top social bookmarking site Delicious . Instead of focusing on the dimension of tags , which has been covered in the previous work , we explore social bookmarking from the dimension of URLs . More specifically , we investigate the Delicious URLs and their content to quantify their value to a search engine . For their value in leading to good content , we show that the Delicious URLs have higher quality content and more external outlinks . For their value in satisfying users , we show that the Delicious URLs have more clicked URLs as well as get more clicks . We suggest that based on their value , the Delicious URLs should be used as another source of seed URLs for crawlers ."
"356","A geographical analysis of knowledge production in computer science We analyze knowledge production in Computer Science by means of coauthorship networks . For this , we consider 30 graduate programs of different regions of the world , being 8 programs in Brazil , 16 in North America ( 3 in Canada and 13 in the United States ) , and 6 in Europe ( 2 in France , 1 in Switzerland and 3 in the United Kingdom ) . We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues . The results obtained for different metrics of collaboration social networks indicate the process of knowledge creation has changed differently for each region . Research is increasingly done in teams across different fields of Computer Science . The size of the giant component indicates the existence of isolated collaboration groups in the European network , contrasting to the degree of connectivity found in the Brazilian and North-American counterparts . We also analyzed the temporal evolution of the social networks representing the three regions . The number of authors per paper experienced an increase in a time span of 12 years . We observe that the number of collaborations between authors grows faster than the number of authors , benefiting from the existing network structure . The temporal evolution shows differences between well-established fields , such as Databases and Computer Architecture , and emerging fields , like Bioinformatics and Geoinformatics . The patterns of collaboration analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database ."
"357","Visual diversification of image search results Due to the reliance on the textual information associated with an image , image search engines on the Web lack the discriminative power to deliver visually diverse search results . The textual descriptions are key to retrieve relevant results for a given user query , but at the same time provide little information about the rich image content . In this paper we investigate three methods for visual diversification of image search results . The methods deploy lightweight clustering techniques in combination with a dynamic weighting function of the visual features , to best capture the discriminative aspects of the resulting set of images that is retrieved . A representative image is selected from each cluster , which together form a diverse result set . Based on a performance evaluation we find that the outcome of the methods closely resembles human perception of diversity , which was established in an extensive clustering experiment carried out by human assessors ."
"358","Two birds with one stone : a graph-based framework for disambiguating and tagging people names in web search The ever growing volume of Web data makes it increasingly challenging to accurately find relevant information about a specific person on the Web . To address the challenge caused by name ambiguity in Web people search , this paper explores a novel graph-based framework to both disambiguate and tag people entities in Web search results . Experimental results demonstrate the effectiveness of the proposed framework in tag discovery and name disambiguation ."
"359","The web of nations In this paper , we report on a large-scale study of structural differences among the national webs . The study is based on a web-scale crawl conducted in the summer 2008 . More specifically , we study two graphs derived from this crawl , the nation graph , with nodes corresponding to nations and edges - to links among nations , and the host graph , with nodes corresponding to hosts and edges - to hyperlinks among pages on the hosts . Contrary to some of the previous work ( 2 ) , our results show that webs of different nations are often very different from each other , both in terms of their internal structure , and in terms of their connectivity with other nations ."
"360","Discovering the staring people from social networks In this paper , we study a novel problem of staring people discovery from social networks , which is concerned with finding people who are not only authoritative but also sociable in the social network . We formalize this problem as an optimization programming problem . Taking the co-author network as a case study , we define three objective functions and propose two methods to combine these objective functions . A genetic algorithm based method is further presented to solve this problem . Experimental results show that the proposed solution can effectively find the staring people from social networks ."
"361","Reliability analysis using weighted combinational models for web-based software In the past , some researches suggested that engineers can use combined software reliability growth models ( SRGMs ) to obtain more accurate reliability prediction during testing . In this paper , three weighted combinational models , namely , equal , linear , and nonlinear weight , are proposed for reliability estimation of web-based software . We further investigate the estimation accuracy of using genetic algorithm to determine the weight assignment for the proposed models . Preliminary result shows that the linearly and nonlinearly weighted combinational models have better prediction capability than single SRGM and equally weighted combinational model for web-based software ."
"362","A general framework for adaptive and online detection of web attacks Detection of web attacks is an important issue in current defense-in-depth security framework . In this paper , we propose a novel general framework for adaptive and online detection of web attacks . The general framework can be based on any online clustering methods . A detection model based on the framework is able to learn online and deal with `` concept drift '' in web audit data streams . Str-DBSCAN that we extended DBSCAN to streaming data as well as StrAP are both used to validate the framework . The detection model based on the framework automatically labels the web audit data and adapts to normal behavior changes while identifies attacks through dynamical clustering of the streaming data . A very large size of real HTTP Log data collected in our institute is used to validate the framework and the model . The preliminary testing results demonstrated its effectiveness ."
"363","Mining for personal name aliases on the web We propose a novel approach to find aliases of a given name from the web . We exploit a set of known names and their aliases as training data and extract lexical patterns that convey information related to aliases of names from text snippets returned by a web search engine . The patterns are then used to find candidate aliases of a given name . We use anchor texts and hyperlinks to design a word co-occurrence model and define numerous ranking scores to evaluate the association between a name and its candidate aliases . The proposed method outperforms numerous baselines and previous work on alias extraction on a dataset of personal names , achieving a statistically significant mean reciprocal rank of 0.6718 . Moreover , the aliases extracted using the proposed method improve recall by 20 % in a relation-detection task ."
"364","Social and semantics analysis via non-negative matrix factorization Social media such as Web forum often have dense interactions between user and content where network models are often appropriate for analysis . Joint non-negative matrix factorization model of participation and content data can be viewed as a bipartite graph model between users and media and is proposed for analysis social media . The factorizations allow simultaneous automatic discovery of leaders and sub-communities in the Web forum as well as the core latent topics in the forum . Results on topic detection of Web forums and cluster analysis show that social features are highly effective for forum analysis ."
"365","Winner takes all : competing viruses or ideas on fair-play networks Given two competing products ( or memes , or viruses etc. ) spreading over a given network , can we predict what will happen at the end , that is , which product will ` win ' , in terms of highest market share ? One may naively expect that the better product ( stronger virus ) will just have a larger footprint , proportional to the quality ratio of the products ( or strength ratio of the viruses ) . However , we prove the surprising result that , under realistic conditions , for any graph topology , the stronger virus completely wipes-out the weaker one , thus not merely ` winning ' but ` taking it all ' . In addition to the proofs , we also demonstrate our result with simulations over diverse , real graph topologies , including the social-contact graph of the city of Portland OR ( about 31 million edges and 1 million nodes ) and internet AS router graphs . Finally , we also provide real data about competing products from Google-Insights , like Facebook-Myspace , and we show again that they agree with our analysis ."
"366","Merkle tree authentication of HTTP responses We propose extensions to existing web protocols that allow proofs of authenticity of HTTP server responses , whether or not the HTTP server is under the control of the publisher . These extensions protect users from content that may be substituted by malicious servers , and therefore have immediate applications in improving the security of web caching , mirroring , and relaying systems that rely on untrusted machines ( 2,4 ) . Our proposal relies on Merkle trees to support 200 and 404 response authentication while requiring only a single cryptographic hash of trusted data per repository . While existing web protocols such as HTTPS can provide authenticity guarantees ( in addition to confidentiality ) , HTTPS consumes significantly more computational resources , and requires that the hosting server act without malice in generating responses and in protecting the publisher 's private key ."
"367","Document hierarchies from text and links Hierarchical taxonomies provide a multi-level view of large document collections , allowing users to rapidly drill down to fine-grained distinctions in topics of interest . We show that automatically induced taxonomies can be made more robust by combining text with relational links . The underlying mechanism is a Bayesian generative model in which a latent hierarchical structure explains the observed data -- thus , finding hierarchical groups of documents with similar word distributions and dense network connections . As a nonparametric Bayesian model , our approach does not require pre-specification of the branching factor at each non-terminal , but finds the appropriate level of detail directly from the data . Unlike many prior latent space models of network structure , the complexity of our approach does not grow quadratically in the number of documents , enabling application to networks with more than ten thousand nodes . Experimental results on hypertext and citation network corpora demonstrate the advantages of our hierarchical , multimodal approach ."
"368","Trust analysis with clustering Web provides rich information about a variety of objects . Trustability is a major concern on the web . Truth establishment is an important task so as to provide the right information to the user from the most trustworthy source . Trustworthiness of information provider and the confidence of the facts it provides are inter-dependent on each other and hence can be expressed iteratively in terms of each other . However , a single information provider may not be the most trustworthy for all kinds of information . Every information provider has its own area of competence where it can perform better than others . We derive a model that can evaluate trustability on objects and information providers based on clusters ( groups ) . We propose a method which groups the set of objects for which similar set of providers provide `` good '' facts , and provides better accuracy in addition to high quality object clusters ."
"369","Design and implementation of contextual information portals This paper presents a system for enabling offline web use to satisfy the information needs of disconnected communities . We describe the design , implementation , evaluation , and pilot deployment of an automated mechanism to construct Contextual Information Portals ( CIPs ) . CIPs are large searchable information repositories of web pages tailored to the information needs of a target population . We combine an efficient classifier with a focused crawler to gather the web pages for the portal for any given topic . Given a set of topics of interest , our system constructs a CIP containing the most relevant pages from the web across these topics . Using several secondary school course syllabi , we demonstrate the effectiveness of our system for constructing CIPs for use as an education resource . We evaluate our system across several metrics : classification accuracy , crawl scalability , crawl accuracy and harvest rate . We describe the utility and usability of our system based on a preliminary deployment study at an after-school program in India , and also outline our ongoing larger-scale pilot deployment at five schools in Kenya ."
"370","Simplifying friendlist management Online social networks like Facebook allow users to connect , communicate , and share content . The popularity of these services has lead to an information overload for their users ; the task of simply keeping track of different interactions has become daunting . To reduce this burden , sites like Facebook allows the user to group friends into specific lists , known as friendlists , aggregating the interactions and content from all friends in each friendlist . While this approach greatly reduces the burden on the user , it still forces the user to create and populate the friendlists themselves and , worse , makes the user responsible for maintaining the membership of their friendlists over time . We show that friendlists often have a strong correspondence to the structure of the social network , implying that friendlists may be automatically inferred by leveraging the social network structure . We present a demonstration of Friendlist Manager , a Facebook application that proposes friendlists to the user based on the structure of their local social network , allows the user to tweak the proposed friendlists , and then automatically creates the friendlists for the user ."
"371","Measuring the web crawler ethics Web crawlers are highly automated and seldom regulated manually . The diversity of crawler activities often leads to ethical problems such as spam and service attacks . In this research , quantitative models are proposed to measure the web crawler ethics based on their behaviors on web servers . We investigate and define rules to measure crawler ethics , referring to the extent to which web crawlers respect the regulations set forth in robots . txt configuration files . We propose a vector space model to represent crawler behavior and measure the ethics of web crawlers based on the behavior vectors . The results show that ethicality scores vary significantly among crawlers . Most commercial web crawlers ' behaviors are ethical . However , many commercial crawlers still consistently violate or misinterpret certain robots . txt rules . We also measure the ethics of big search engine crawlers in terms of return on investment . The results show that Google has a higher score than other search engines for a US website but has a lower score than Baidu for Chinese websites ."
"372","Spotting fake reviewer groups in consumer reviews Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making . However , due to the reason of profit or fame , people try to game the system by opinion spamming ( e.g. , writing fake reviews ) to promote or demote some target products . For reviews to reflect genuine user experiences and opinions , such spam reviews should be detected . Prior works on opinion spam focused on detecting fake reviews and individual fake reviewers . However , a fake reviewer group ( a group of reviewers who work collaboratively to write fake reviews ) is even more damaging as they can take total control of the sentiment on the target product due to its size . This paper studies spam detection in the collaborative setting , i.e. , to discover fake reviewer groups . The proposed method first uses a frequent itemset mining method to find a set of candidate groups . It then uses several behavioral models derived from the collusion phenomenon among fake reviewers and relation models based on the relationships among groups , individual reviewers , and products they reviewed to detect fake reviewer groups . Additionally , we also built a labeled dataset of fake reviewer groups . Although labeling individual fake reviews and reviewers is very hard , to our surprise labeling fake reviewer groups is much easier . We also note that the proposed technique departs from the traditional supervised learning approach for spam detection because of the inherent nature of our problem which makes the classic supervised learning approach less effective . Experimental results show that the proposed method outperforms multiple strong baselines including the state-of-the-art supervised classification , regression , and learning to rank algorithms ."
"373","Understanding and combating link farming in the twitter social network Recently , Twitter has emerged as a popular platform for discovering real-time information on the Web , such as news stories and people 's reaction to them . Like the Web , Twitter has become a target for link farming , where users , especially spammers , try to acquire large numbers of follower links in the social network . Acquiring followers not only increases the size of a user 's direct audience , but also contributes to the perceived influence of the user , which in turn impacts the ranking of the user 's tweets by search engines . In this paper , we first investigate link farming in the Twitter network and then explore mechanisms to discourage the activity . To this end , we conducted a detailed analysis of links acquired by over 40,000 spammer accounts suspended by Twitter . We find that link farming is wide spread and that a majority of spammers ' links are farmed from a small fraction of Twitter users , the social capitalists , who are themselves seeking to amass social capital and links by following back anyone who follows them . Our findings shed light on the social dynamics that are at the root of the link farming problem in Twitter network and they have important implications for future designs of link spam defenses . In particular , we show that a simple user ranking scheme that penalizes users for connecting to spammers can effectively address the problem by disincentivizing users from linking with other users simply to gain influence ."
"374","On measuring the quality of Wikipedia articles This paper discusses an approach to modeling and measuring information quality of Wikipedia articles . The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models . We report on our initial study , which involved two categories of Wikipedia articles : `` stabilized '' ( those , whose content has not undergone major changes for a significant period of time ) and `` controversial '' ( the articles , which have undergone vandalism , revert wars , or whose content is subject to internal discussions between Wikipedia editors ) . We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users . Our experiment shows , that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles ."
"375","Consideration set generation in commerce search In commerce search , the set of products returned by a search engine often forms the basis for all user interactions leading up to a potential transaction on the web . Such a set of products is known as the consideration set . In this study , we consider the problem of generating consideration set of products in commerce search so as to maximize user satisfaction . One of the key features of commerce search that we exploit in our study is the association of a set of important attributes with the products and a set of specified attributes with the user queries . Those important attributes not used in the query are treated as unspecified . The attribute space admits a natural definition of user satisfaction via user preferences on the attributes and their values , viz . require that the surfaced products be close to the specified attribute values in the query , and diverse with respect to the unspecified attributes . We model this as a general Max-Sum Dispersion problem wherein we are given a set of n nodes in a metric space and the objective is to select a subset of nodes with total cost at most a given budget , and maximize the sum of the pairwise distances between the selected nodes . In our setting , each node denotes a product , the cost of a node being inversely proportional to its relevance with respect to specified attributes . The distance between two nodes quantifies the diversity with respect to the unspecified attributes . The problem is NP-hard and a 2-approximation was previously known only when all the nodes have unit cost . In our setting , we do not make any assumptions on the cost . We label this problem as the General Max-Sum Dispersion problem . We give the first constant factor approximation algorithm for this problem , achieving an approximation ratio of 2 . Further , we perform extensive empirical analysis on real-world data to show the effectiveness of our algorithm ."
"376","Modeling the temporal dynamics of social rating networks using bidirectional effects of social relations and rating patterns A social rating network ( SRN ) is a social network in which edges represent social relationships and users ( nodes ) express ratings on some of the given items . Such networks play an increasingly important role in reviewing websites such as Epinions.com or online sharing websites like Flickr.com . In this paper , we first observe and analyze the temporal behavior of users in a social rating network , who express ratings and create social relations . Then , we model the temporal dynamics of an SRN based on our observations , using the bidirectional effects of ratings and social relations . While existing models for other types of social networks have captured some of the effects , our model is the first one to represent all four effects , i.e. social relations-on-ratings ( social influence ) , social relations-on-social relations ( transitivity ) , ratings-on-social relations ( selection ) , and ratings-on-ratings ( correlational influence ) . Existing works consider these effects as static and constant throughout the evolution of an SRN , however our observations reveal that these effects are actually dynamic . We propose a probabilistic generative model for SRNs , which models the strength and dynamics of each effect throughout the network evolution . This model can serve for the prediction of future links , ratings or community structures . Due to the sensitive nature of SRNs , another motivation for our work is the generation of synthetic SRN data sets for research purposes . Our experimental studies on two real life datasets ( Epinions and Flickr ) demonstrate that the proposed model produces social rating networks that agree with real world data on a comprehensive set of evaluation criteria ."
"377","Economics of BitTorrent communities Over the years , private file-sharing communities built on the BitTorrent protocol have developed their own policies and mechanisms for motivating members to share content and contribute resources . By requiring members to maintain a minimum ratio between uploads and downloads , private communities effectively establish credit systems , and with them full-fledged economies . We report on a half-year-long measurement study of DIME -- a community for sharing live concert recordings -- that sheds light on the economic forces affecting users in such communities . A key observation is that while the download of files is priced only according to the size of the file , the rate of return for seeding new files is significantly greater than for seeding old files . We find via a natural experiment that users react to such differences in resale value by preferentially consuming older files during a ` free leech ' period . We consider implications of these finding on a user 's ability to earn credits and meet ratio enforcements , focusing in particular on the relationship between visitation frequency and wealth and on low bandwidth users . We then share details from an interview with DIME moderators , which highlights the goals of the community based on which we make suggestions for possible improvement ."
"378","Periodic transfers in mobile applications : network-wide origin , impact , and optimization Cellular networks employ a specific radio resource management policy distinguishing them from wired and Wi-Fi networks . A lack of awareness of this important mechanism potentially leads to resource-inefficient mobile applications . We perform the first network-wide , large-scale investigation of a particular type of application traffic pattern called periodic transfers where a handset periodically exchanges some data with a remote server every t seconds . Using packet traces containing 1.5 billion packets collected from a commercial cellular carrier , we found that periodic transfers are very prevalent in today 's smartphone traffic . However , they are extremely resource-inefficient for both the network and end-user devices even though they predominantly generate very little traffic . This somewhat counter-intuitive behavior is a direct consequence of the adverse interaction between such periodic transfer patterns and the cellular network radio resource management policy . For example , for popular smartphone applications such as Facebook , periodic transfers account for only 1.7 % of the overall traffic volume but contribute to 30 % of the total handset radio energy consumption . We found periodic transfers are generated for various reasons such as keep-alive , polling , and user behavior measurements . We further investigate the potential of various traffic shaping and resource control algorithms . Depending on their traffic patterns , applications exhibit disparate responses to optimization strategies . Jointly using several strategies with moderate aggressiveness can eliminate almost all energy impact of periodic transfers for popular applications such as Facebook and Pandora ."
"379","A unified approach to learning task-specific bit vector representations for fast nearest neighbor search Fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval , nearest neighbor classification and nearest neighbor regression . Recently a number of machine learning algorithms have been proposed for representing the data to be searched as ( short ) bit vectors and then using hashing to do rapid search . These algorithms have been limited in their applicability in that they are suited for only one type of task -- e.g. Spectral Hashing learns bit vector representations for retrieval , but not say , classification . In this paper we present a unified approach to learning bit vector representations for many applications that use nearest neighbor search . The main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand . This broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval . We propose a learning-to-rank formulation to learn the bit vector representation of the data . LambdaRank algorithm is used for learning a function that computes a task-specific bit vector from an input data vector . Our approach outperforms state-of-the-art nearest neighbor methods on a number of real world text and image classification and retrieval datasets . It is scalable and learns a 32-bit representation on 1.46 million training cases in two days ."
"380","Estimating the impressionrank of web pages The ImpressionRank of a web page ( or , more generally , of a web site ) is the number of times users viewed the page while browsing search results . ImpressionRank captures the visibility of pages and sites in search engines and is thus an important measure , which is of interest to web site owners , competitors , market analysts , and end users . All previous approaches to estimating the ImpressionRank of a page rely on privileged access to private data sources , like the search engine 's query log . In this paper we present the first external algorithm for estimating the ImpressionRank of a web page . This algorithm relies on access to three public data sources : the search engine , the query suggestion service of the search engine , and the web . In addition , the algorithm is local and uses modest resources . It can therefore be used by almost any party to estimate the ImpressionRank of any page on any search engine . En route to estimating the ImpressionRank of a page , our algorithm solves a novel variant of the keyword extraction problem : it finds the most popular search keywords that drive impressions of a page . Empirical analysis of the algorithm on the Google and Yahoo ! search engines indicates that it is accurate and provides interesting insights about sites and search queries ."
"381","Classifying web sites In this paper , we present a novel method for the classification of Web sites . This method exploits both structure and content of Web sites in order to discern their functionality . It allows for distinguishing between eight of the most relevant functional classes of Web sites . We show that a pre-classification of Web sites utilizing structural properties considerably improves a subsequent textual classification with standard techniques . We evaluate this approach on a dataset comprising more than 16,000 Web sites with about 20 million crawled and 100 million known Web pages . Our approach achieves an accuracy of 92 % for the coarse-grained classification of these Web sites ."
"382","Human wayfinding in information networks Navigating information spaces is an essential part of our everyday lives , and in order to design efficient and user-friendly information systems , it is important to understand how humans navigate and find the information they are looking for . We perform a large-scale study of human wayfinding , in which , given a network of links between the concepts of Wikipedia , people play a game of finding a short path from a given start to a given target concept by following hyperlinks . What distinguishes our setup from other studies of human Web-browsing behavior is that in our case people navigate a graph of connections between concepts , and that the exact goal of the navigation is known ahead of time . We study more than 30,000 goal-directed human search paths and identify strategies people use when navigating information spaces . We find that human wayfinding , while mostly very efficient , differs from shortest paths in characteristic ways . Most subjects navigate through high-degree hubs in the early phase , while their search is guided by content features thereafter . We also observe a trade-off between simplicity and efficiency : conceptually simple solutions are more common but tend to be less efficient than more complex ones . Finally , we consider the task of predicting the target a user is trying to reach . We design a model and an efficient learning algorithm . Such predictive models of human wayfinding can be applied in intelligent browsing interfaces ."
"383","Generalized link suggestions via web site clustering Proactive link suggestion leads to improved user experience by allowing users to reach relevant information with fewer clicks , fewer pages to read , or simply faster because the right pages are prefetched just in time . In this paper we tackle two new scenarios for link suggestion , which were not covered in prior work owing to scarcity of historical browsing data . In the web search scenario , we propose a method for generating quick links - additional entry points into Web sites , which are shown for top search results for navigational queries - for tail sites , for which little browsing statistics is available . Beyond Web search , we also propose a method for link suggestion in general web browsing , effectively anticipating the next link to be followed by the user . Our approach performs clustering of Web sites in order to aggregate information across multiple sites , and enables relevant link suggestion for virtually any site , including tail sites and brand new sites for which little historical data is available . Empirical evaluation confirms the validity of our method using editorially labeled data as well as real-life search and browsing data from a major US search engine ."
"384","A data-driven sketch of Wikipedia editors Who edits Wikipedia ? We attempt to shed light on this question by using aggregated log data from Yahoo ! 's browser toolbar in order to analyze Wikipedians ' editing behavior in the context of their online lives beyond Wikipedia . We broadly characterize editors by investigating how their online behavior differs from that of other users ; e.g. , we find that Wikipedia editors search more , read more news , play more games , and , perhaps surprisingly , are more immersed in pop culture . Then we inspect how editors ' general interests relate to the articles to which they contribute ; e.g. , we confirm the intuition that editors show more expertise in their active domains than average users . Our results are relevant as they illuminate novel aspects of what has become many Web users ' prevalent source of information and can help in recruiting new editors ."
"385","Framework and algorithms for network bucket testing Bucket testing , also known as split testing , A\/B testing , or 0\/1 testing , is a widely used method for evaluating users ' satisfaction with new features , products , or services . In order not to expose the whole user base to the new service , the mean user satisfaction rate is estimated by exposing the service only to a few uniformly chosen random users . In a recent work , Backstrom and Kleinberg , defined the notion of network bucket testing for social services . In this context , users ' interactions are only valid for measurement if some minimal number of their friends are also given the service . The goal is to estimate the mean user satisfaction rate while providing the service to the least number of users . This constraint makes uniform sampling , which is optimal for the traditional case , grossly inefficient . In this paper we introduce a simple general framework for designing and evaluating sampling techniques for network bucket testing . The framework is constructed in a way that sampling algorithms are only required to generate sets of users to which the service should be provided . Given an algorithm , the framework produces an unbiased user satisfaction rate estimator and a corresponding variance bound for any network and any user satisfaction function . Furthermore , we present several simple sampling algorithms that are evaluated using both synthetic and real social networks . Our experiments corroborate the theoretical results and demonstrate the effectiveness of the proposed framework and algorithms ."
"386","Online team formation in social networks We study the problem of online team formation . We consider a setting in which people possess different skills and compatibility among potential team members is modeled by a social network . A sequence of tasks arrives in an online fashion , and each task requires a specific set of skills . The goal is to form a new team upon arrival of each task , so that ( i ) each team possesses all skills required by the task , ( ii ) each team has small communication overhead , and ( iii ) the workload of performing the tasks is balanced among people in the fairest possible way . We propose efficient algorithms that address all these requirements : our algorithms form teams that always satisfy the required skills , provide approximation guarantees with respect to team communication overhead , and they are online-competitive with respect to load balancing . Experiments performed on collaboration networks among film actors and scientists , confirm that our algorithms are successful at balancing these conflicting requirements . This is the first paper that simultaneously addresses all these aspects . Previous work has either focused on minimizing coordination for a single task or balancing the workload neglecting coordination costs ."
"387","Trains of thought : generating information maps When information is abundant , it becomes increasingly difficult to fit nuggets of knowledge into a single coherent picture . Complex stories spaghetti into branches , side stories , and intertwining narratives . In order to explore these stories , one needs a map to navigate unfamiliar territory . We propose a methodology for creating structured summaries of information , which we call metro maps . Our proposed algorithm generates a concise structured set of documents maximizing coverage of salient pieces of information . Most importantly , metro maps explicitly show the relations among retrieved pieces in a way that captures story development . We first formalize characteristics of good maps and formulate their construction as an optimization problem . Then we provide efficient methods with theoretical guarantees for generating maps . Finally , we integrate user interaction into our framework , allowing users to alter the maps to better reflect their interests . Pilot user studies with a real-world dataset demonstrate that the method is able to produce maps which help users acquire knowledge efficiently ."
"388","Active objects : actions for entity-centric search We introduce an entity-centric search experience , called Active Objects , in which entity-bearing queries are paired with actions that can be performed on the entities . For example , given a query for a specific flashlight , we aim to present actions such as reading reviews , watching demo videos , and finding the best price online . In an annotation study conducted over a random sample of user query sessions , we found that a large proportion of queries in query logs involve actions on entities , calling for an automatic approach to identifying relevant actions for entity-bearing queries . In this paper , we pose the problem of finding actions that can be performed on entities as the problem of probabilistic inference in a graphical model that captures how an entity bearing query is generated . We design models of increasing complexity that capture latent factors such as entity type and intended actions that determine how a user writes a query in a search box , and the URL that they click on . Given a large collection of real-world queries and clicks from a commercial search engine , the models are learned efficiently through maximum likelihood estimation using an EM algorithm . Given a new query , probabilistic inference enables recommendation of a set of pertinent actions and hosts . We propose an evaluation methodology for measuring the relevance of our recommended actions , and show empirical evidence of the quality and the diversity of the discovered actions ."
"389","Using proximity to predict activity in social networks The structure of a social network contains information useful for predicting its evolution . We show that structural information also helps predict activity . People who are `` close '' in some sense in a social network are more likely to perform similar actions than more distant people . We use network proximity to capture the degree to which people are `` close '' to each other . In addition to standard proximity metrics used in the link prediction task , such as neighborhood overlap , we introduce new metrics that model different types of interactions that take place between people . We study this claim empirically using data about URL forwarding activity on the social media sites Digg and Twitter . We show that structural proximity of two users in the follower graph is related to similarity of their activity , i.e. , how many URLs they both forward . We also show that given friends ' activity , knowing their proximity to the user can help better predict which URLs the user will forward . We compare the performance of different proximity metrics on the activity prediction task and find that metrics that take into account the attention-limited nature of interactions in social media lead to substantially better predictions ."
"390","Optimizing budget allocation among channels and influencers Brands and agencies use marketing as a tool to influence customers . One of the major decisions in a marketing plan deals with the allocation of a given budget among media channels in order to maximize the impact on a set of potential customers . A similar situation occurs in a social network , where a marketing budget needs to be distributed among a set of potential influencers in a way that provides high-impact . We introduce several probabilistic models to capture the above scenarios . The common setting of these models consists of a bipartite graph of source and target nodes . The objective is to allocate a fixed budget among the source nodes to maximize the expected number of influenced target nodes . The concrete way in which source nodes influence target nodes depends on the underlying model . We primarily consider two models : a source-side influence model , in which a source node that is allocated a budget of k makes k independent trials to influence each of its neighboring target nodes , and a target-side influence model , in which a target node becomes influenced according to a specified rule that depends on the overall budget allocated to its neighbors . Our main results are an optimal ( 1-1 \/ e ) - approximation algorithm for the source-side model , and several inapproximability results for the target-side model , establishing that influence maximization in the latter model is provably harder ."
"391","On revenue in the generalized second price auction The Generalized Second Price ( GSP ) auction is the primary auction used for selling sponsored search advertisements . In this paper we consider the revenue of this auction at equilibrium . We prove that if agent values are drawn from identical regular distributions , then the GSP auction paired with an appropriate reserve price generates a constant fraction ( 1\/6th ) of the optimal revenue . In the full-information game , we show that at any Nash equilibrium of the GSP auction obtains at least half of the revenue of the VCG mechanism excluding the payment of a single participant . This bound holds also with any reserve price , and is tight . Finally , we consider the tradeoff between maximizing revenue and social welfare . We introduce a natural convexity assumption on the click-through rates and show that it implies that the revenue-maximizing equilibrium of GSP in the full information model will necessarily be envy-free . In particular , it is always possible to maximize revenue and social welfare simultaneously when click-through rates are convex . Without this convexity assumption , however , we demonstrate that revenue may be maximized at a non-envy-free equilibrium that generates a socially inefficient allocation ."
"392","Document recommendation in social tagging services Social tagging services allow users to annotate various online resources with freely chosen keywords ( tags ) . They not only facilitate the users in finding and organizing online resources , but also provide meaningful collaborative semantic data which can potentially be exploited by recommender systems . Traditional studies on recommender systems focused on user rating data , while recently social tagging data is becoming more and more prevalent . How to perform resource recommendation based on tagging data is an emerging research topic . In this paper we consider the problem of document ( e.g. Web pages , research papers ) recommendation using purely tagging data . That is , we only have data containing users , tags , documents and the relationships among them . We propose a novel graph-based representation learning algorithm for this purpose . The users , tags and documents are represented in the same semantic space in which two related objects are close to each other . For a given user , we recommend those documents that are sufficiently close to him\/her . Experimental results on two data sets crawled from Del. icio . us and CiteULike show that our algorithm can generate promising recommendations and outperforms traditional recommendation algorithms ."
"393","From actors , politicians , to CEOs : domain adaptation of relational extractors using a latent relational mapping We propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision . Our proposed method comprises two stages : learning a lower-dimensional projection between different relations , and learning a relational classifier for the target relation type with instance sampling . We evaluate the proposed method using a dataset that contains 2000 instances for 20 different relation types . Our experimental results show that the proposed method achieves a statistically significant macro-average F-score of 62.77 . Moreover , the proposed method outperforms numerous baselines and a previously proposed weakly-supervised relation extraction method ."
"394","Modeling and predicting behavioral dynamics on the web User behavior on the Web changes over time . For example , the queries that people issue to search engines , and the underlying informational goals behind the queries vary over time . In this paper , we examine how to model and predict this temporal user behavior . We develop a temporal modeling framework adapted from physics and signal processing that can be used to predict time-varying user behavior using smoothing and trends . We also explore other dynamics of Web behaviors , such as the detection of periodicities and surprises . We develop a learning procedure that can be used to construct models of users ' activities based on features of current and historical behaviors . The results of experiments indicate that by using our framework to predict user behavior , we can achieve significant improvements in prediction compared to baseline models that weight historical evidence the same for all queries . We also develop a novel learning algorithm that explicitly learns when to apply a given prediction model among a set of such models . Our improved temporal modeling of user behavior can be used to enhance query suggestions , crawling policies , and result ranking ."
"395","k-Centralities : local approximations of global measures based on shortest paths A lot of centrality measures have been developed to analyze different aspects of importance . Some of the most popular centrality measures ( e.g. betweenness centrality , closeness centrality ) are based on the calculation of shortest paths . This characteristic limits the applicability of these measures for larger networks . In this article we elaborate on the idea of bounded-distance shortest paths calculations . We claim criteria for k-centrality measures and we introduce one algorithm for calculating both betweenness and closeness based centralities . We also present normalizations for these measures . We show that k-centrality measures are good approximations for the corresponding centrality measures by achieving a tremendous gain of calculation time and also having linear calculation complexity O ( n ) for networks with constant average degree . This allows researchers to approximate centrality measures based on shortest paths for networks with millions of nodes or with high frequency in dynamically changing networks ."
"396","An expressive mechanism for auctions on the web Auctions are widely used on the Web . Applications range from internet advertising to platforms such as eBay . In most of these applications the auctions in use are single\/multi-item auctions with unit demand . The main drawback of standard mechanisms for this type of auctions , such as VCG and GSP , is the limited expressiveness that they offer to the bidders . The General Auction Mechanism ( GAM ) of ( 1 ) is taking a first step towards addressing the problem of limited expressiveness by computing a bidder optimal , envy free outcome for linear utility functions with identical slopes and a single discontinuity per bidder-item pair . We show that in many practical situations this does not suffice to adequately model the preferences of the bidders , and we overcome this problem by presenting the first mechanism for piece-wise linear utility functions with non-identical slopes and multiple discontinuities . Our mechanism runs in polynomial time . Like GAM it is incentive compatible for inputs that fulfill a certain non-degeneracy requirement , but our requirement is more general than the requirement of GAM . For discontinuous utility functions that are non-degenerate as well as for continuous utility functions the outcome of our mechanism is a competitive equilibrium . We also show how our mechanism can be used to compute approximately bidder optimal , envy free outcomes for a general class of continuous utility functions via piece-wise linear approximation . Finally , we prove hardness results for even more expressive settings ."
"397","The million song dataset challenge We introduce the Million Song Dataset Challenge : a large-scale , personalized music recommendation challenge , where the goal is to predict the songs that a user will listen to , given both the user 's listening history and full information ( including meta-data and content analysis ) for all songs . We explain the taste profile data , our goals and design choices in creating the challenge , and present baseline results using simple , off-the-shelf recommendation algorithms ."
"398","The social honeypot project : protecting online communities from spammers We present the conceptual framework of the Social Honeypot Project for uncovering social spammers who target online communities and initial empirical results from Twitter and MySpace . Two of the key components of the Social Honeypot Project are : ( 1 ) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities ; and ( 2 ) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers ."
"399","Design of a crawler with bounded bandwidth This paper presents an algorithm to bound the bandwidth of a Web crawler . The crawler collects statistics on the transfer rate of each server to predict the expected bandwidth use for future downloads . The prediction allows us to activate the optimal number of fetcher threads in order to exploit the assigned bandwidth . The experimental results show the effectiveness of the proposed technique ."
"400","Serf and turf : crowdturfing for fun and profit Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses using crowd-sourcing systems . However , crowd-sourcing systems can also pose a real challenge to existing security mechanisms deployed to protect Internet services . Many of these security techniques rely on the assumption that malicious activity is generated automatically by automated programs . Thus they would perform poorly or be easily bypassed when attacks are generated by real users working in a crowd-sourcing system . Through measurements , we have found surprising evidence showing that not only do malicious crowd-sourcing systems exist , but they are rapidly growing in both user base and total revenue . We describe in this paper a significant effort to study and understand these `` crowdturfing '' systems in today 's Internet . We use detailed crawls to extract data about the size and operational structure of these crowdturfing systems . We analyze details of campaigns offered and performed in these sites , and evaluate their end-to-end effectiveness by running active , benign campaigns of our own . Finally , we study and compare the source of workers on crowdturfing sites in different countries . Our results suggest that campaigns on these systems are highly effective at reaching users , and their continuing growth poses a concrete threat to online communities both in the US and elsewhere ."
"401","b-Bit minwise hashing This paper establishes the theoretical framework of b-bit minwise hashing . The original minwise hashing method has become a standard technique for estimating set similarity ( e.g. , resemblance ) with applications in information retrieval , data management , computational advertising , etc. . By only storing b bits of each hashed value ( e.g. , b = 1 or 2 ) , we gain substantial advantages in terms of storage space . We prove the basic theoretical results and provide an unbiased estimator of the resemblance for any b. We demonstrate that , even in the least favorable scenario , using b = 1 may reduce the storage space at least by a factor of 21.3 ( or 10.7 ) compared to b = 64 ( or b = 32 ) , if one is interested in resemblance ) 0.5 ."
"402","Crowdsourcing with endogenous entry We investigate the design of mechanisms to incentivize high quality outcomes in crowdsourcing environments with strategic agents , when entry is an endogenous , strategic choice . Modeling endogenous entry in crowdsourcing markets is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating , and indeed many sites based on crowdsourced content do not have adequate participation . We use a mechanism with monotone , rank-based , rewards in a model where agents strategically make participation and quality choices to capture a wide variety of crowdsourcing environments , ranging from conventional crowdsourcing contests with monetary rewards such as TopCoder , to crowdsourced content as in online Q&A forums . We begin by explicitly constructing the unique mixed-strategy equilibrium for such monotone rank-order mechanisms , and use the participation probability and distribution of qualities from this construction to address the question of designing incentives for two kinds of rewards that arise in the context of crowdsourcing . We first show that for attention rewards that arise in the crowdsourced content setting , the entire equilibrium distribution and therefore every increasing statistic including the maximum and average quality ( accounting for participation ) , improves when the rewards for every rank but the last are as high as possible . In particular , when the cost of producing the lowest possible quality content is low , the optimal mechanism displays all but the poorest contribution . We next investigate how to allocate rewards in settings where there is a fixed total reward that can be arbitrarily distributed amongst participants , as in crowdsourcing contests . Unlike models with exogenous entry , here the expected number of participants can be increased by subsidizing entry , which could potentially improve the expected value of the best contribution . However , we show that subsidizing entry does not improve the expected quality of the best contribution , although it may improve the expected quality of the average contribution . In fact , we show that free entry is dominated by taxing entry -- making all entrants pay a small fee , which is rebated to the winner along with whatever rewards were already assigned , can improve the quality of the best contribution over a winner-take-all contest with no taxes ."
"403","Implementing optimal outcomes in social computing : a game-theoretic approach In many social computing applications such as online Q&A forums , the best contribution for each task receives some high reward , while all remaining contributions receive an identical , lower reward irrespective of their actual qualities . Suppose a mechanism designer ( site owner ) wishes to optimize an objective that is some function of the number and qualities of received contributions . When potential contributors are -LCB- \ em strategic -RCB- agents , who decide whether to contribute or not to selfishly maximize their own utilities , is such a `` best contribution '' mechanism , Mb , adequate to implement an outcome that is optimal for the mechanism designer ? We first show that in settings where a contribution 's value is determined primarily by an agent 's expertise , and agents only strategically choose whether to contribute or not , contests can implement optimal outcomes : for any reasonable objective , the rewards for the best and remaining contributions in Mb can always be chosen so that the outcome in the unique symmetric equilibrium of Mb maximizes the mechanism designer 's utility . We also show how the mechanism designer can learn these optimal rewards when she does not know the parameters of the agents ' utilities , as might be the case in practice . We next consider settings where a contribution 's value depends on both the contributor 's expertise as well as her effort , and agents endogenously choose how much effort to exert in addition to deciding whether to contribute . Here , we show that optimal outcomes can never be implemented by contests if the system can rank the qualities of contributions perfectly . However , if there is noise in the contributions ' rankings , then the mechanism designer can again induce agents to follow strategies that maximize his utility . Thus imperfect rankings can actually help achieve implementability of optimal outcomes when effort is endogenous and influences quality ."
"404","New objective functions for social collaborative filtering This paper examines the problem of social collaborative filtering ( CF ) to recommend items of interest to users in a social network setting . Unlike standard CF algorithms using relatively simple user and item features , recommendation in social networks poses the more complex problem of learning user preferences from a rich and complex set of user profile and interaction information . Many existing social CF methods have extended traditional CF matrix factorization , but have overlooked important aspects germane to the social setting . We propose a unified framework for social CF matrix factorization by introducing novel objective functions for training . Our new objective functions have three key features that address main drawbacks of existing approaches : ( a ) we fully exploit feature-based user similarity , ( b ) we permit direct learning of user-to-user information diffusion , and ( c ) we leverage co-preference ( dis ) agreement between two users to learn restricted areas of common interest . We evaluate these new social CF objectives , comparing them to each other and to a variety of ( social ) CF baselines , and analyze user behavior on live user trials in a custom-developed Facebook App involving data collected over five months from over 100 App users and their 37,000 + friends ."
"405","Intelligent crawling of web applications for web archiving The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data . Tools used currently by Web archivists blindly crawl and store Web pages found while crawling , disregarding the kind of Web site currently accessed ( which leads to suboptimal crawling strategies ) and whatever structured content is contained in Web pages ( which results in page-level archives whose content is hard to exploit ) . We focus in this PhD work on the crawling and archiving of publicly accessible Web applications , especially those of the social Web . A Web application is any application that uses Web standards such as HTML and HTTP to publish information on the Web , accessible by Web browsers . Examples include Web forums , social networks , geolocation services , etc. . We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed , allowing it to refine the list of URLs to process , and to annotate the archive with information about the structure of crawled content . We add adaptive characteristics to an archival Web crawler : being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology ."
"406","Leveraging interlingual classification to improve web search In this paper we address the problem of improving accuracy of web search in a smaller , data-limited search market ( search language ) using behavioral data from a larger , data-rich market ( assist language ) . Specifically , we use interlingual classification to infer the search language query 's intent using the assist language click-through data . We use these improved estimates of query intent , along with the query intent based on the search language data , to compute features that encode the similarity between a search result ( URL ) and the query . These features are subsequently fed into the ranking model to improve the relevance ranking of the documents . Our experimental results on German and French languages show the effectiveness of using assist language behavioral data especially , when the search language queries have small click-through data ."
"407","Modeling click-through based word-pairs for web search Statistical translation models and latent semantic analysis ( LSA ) are two effective approaches to exploit click-through data for web search ranking . This paper presents two document ranking models that combine both approaches by explicitly modeling word-pairs . The first model , called PairModel , is a monolingual ranking model based on word pairs that are derived from click-through data . It maps queries and documents into a concept space spanned by these word pairs . The second model , called Bilingual Paired Topic Model ( BPTM ) , uses bilingual word pairs and jointly models a bilingual query-document collection . This model maps queries and documents in multiple languages into a lower dimensional semantic subspace . Experimental results on web search task show that they significantly outperform the state-of-the-art baseline models , and the best result is obtained by interpolating PairModel and BPTM ."
"408","Information integration over time in unreliable and uncertain environments Often an interesting true value such as a stock price , sports score , or current temperature is only available via the observations of noisy and potentially conflicting sources . Several techniques have been proposed to reconcile these conflicts by computing a weighted consensus based on source reliabilities , but these techniques focus on static values . When the real-world entity evolves over time , the noisy sources can delay , or even miss , reporting some of the real-world updates . This temporal aspect introduces two key challenges for consensus-based approaches : ( i ) due to delays , the mapping between a source 's noisy observation and the real-world update it observes is unknown , and ( ii ) missed updates may translate to missing values for the consensus problem , even if the mapping is known . To overcome these challenges , we propose a formal approach that models the history of updates of the real-world entity as a hidden semi-Markovian process ( HSMM ) . The noisy sources are modeled as observations of the hidden state , but the mapping between a hidden state ( i.e. real-world update ) and the observation ( i.e. source value ) is unknown . We propose algorithms based on Gibbs Sampling and EM to jointly infer both the history of real-world updates as well as the unknown mapping between them and the source values . We demonstrate using experiments on real-world datasets how our history-based techniques improve upon history-agnostic consensus-based approaches ."
"409","Handling forecast errors while bidding for display advertising Most of the online advertising today is sold via an auction , which requires the advertiser to respond with a valid bid within a fraction of a second . As such , most advertisers employ bidding agents to submit bids on their behalf . The architecture of such agents typically has ( 1 ) an offline optimization phase which incorporates the bidder 's knowledge about the market and ( 2 ) an online bidding strategy which simply executes the offline strategy . The online strategy is typically highly dependent on both supply and expected price distributions , both of which are forecast using traditional machine learning methods . In this work we investigate the optimum strategy of the bidding agent when faced with incorrect forecasts . At a high level , the agent can invest resources in improving the forecasts , or can tighten the loop between successive offline optimization cycles in order to detect errors more quickly . We show analytically that the latter strategy , while simple , is extremely effective in dealing with forecast errors , and confirm this finding with experimental evaluations ."
"410","Max algorithms in crowdsourcing environments Our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environments . We first develop parameterized families of max algorithms , that take as input a set of items and output an item from the set that is believed to be the maximum . Such max algorithms could , for instance , select the best Facebook profile that matches a given person or the best photo that describes a given restaurant . Then , we propose strategies that select appropriate max algorithm parameters . Our framework supports various human error and cost models and we consider many of them for our experiments . We evaluate under many metrics , both analytically and via simulations , the tradeoff between three quantities : ( 1 ) quality , ( 2 ) monetary cost , and ( 3 ) execution time . Also , we provide insights on the effectiveness of the strategies in selecting appropriate max algorithm parameters and guidelines for choosing max algorithms and strategies for each application ."
"411","The role of social networks in information diffusion Online social networking technologies enable individuals to simultaneously share information with any number of peers . Quantifying the causal effect of these mediums on the dissemination of information requires not only identification of who influences whom , but also of whether individuals would still propagate information in the absence of social signals about that information . We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends ' information sharing among 253 million subjects in situ . Those who are exposed are significantly more likely to spread information , and do so sooner than those who are not exposed . We further examine the relative role of strong and weak ties in information propagation . We show that , although stronger ties are individually more influential , it is the more abundant weak ties who are responsible for the propagation of novel information . This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed ."
"412","Targeted disambiguation of ad-hoc , homogeneous sets of named entities In many entity extraction applications , the entities to be recognized are constrained to be from a list of `` target entities '' . In many cases , these target entities are ( i ) ad-hoc , i.e. , do not exist in a knowledge base and ( ii ) homogeneous ( e.g. , all the entities are IT companies ) . We study the following novel disambiguation problem in this unique setting : given the candidate mentions of all the target entities , determine which ones are true mentions of a target entity . Prior techniques only consider target entities present in a knowledge base and\/or having a rich set of attributes . In this paper , we develop novel techniques that require no knowledge about the entities except their names . Our main insight is to leverage the homogeneity constraint and disambiguate the candidate mentions collectively across all documents . We propose a graph-based model , called MentionRank , for that purpose . Furthermore , if additional knowledge is available for some or all of the entities , our model can leverage it to further improve quality . Our experiments demonstrate the effectiveness of our model . To the best of our knowledge , this is the first work on targeted entity disambiguation for ad-hoc entities ."
"413","Vertex collocation profiles : subgraph counting for link analysis and prediction We introduce the concept of a vertex collocation profile ( VCP ) for the purpose of topological link analysis and prediction . VCPs provide nearly complete information about the surrounding local structure of embedded vertex pairs . The VCP approach offers a new tool for domain experts to understand the underlying growth mechanisms in their networks and to analyze link formation mechanisms in the appropriate sociological , biological , physical , or other context . The same resolution that gives VCP its analytical power also enables it to perform well when used in supervised models to discriminate potential new links . We first develop the theory , mathematics , and algorithms underlying VCPs . Then we demonstrate VCP methods performing link prediction competitively with unsupervised and supervised methods across several different network families . We conclude with timing results that introduce the comparative performance of several existing algorithms and the practicability of VCP computations on large networks ."
"414","Partitioned multi-indexing : bringing order to social search To answer search queries on a social network rich with user-generated content , it is desirable to give a higher ranking to content that is closer to the individual issuing the query . Queries occur at nodes in the network , documents are also created by nodes in the same network , and the goal is to find the document that matches the query and is closest in network distance to the node issuing the query . In this paper , we present the `` Partitioned Multi-Indexing '' scheme , which provides an approximate solution to this problem . With m links in the network , after an offline ~ O ( m ) pre-processing time , our scheme allows for social index operations ( i.e. , social search queries , as well as insertion and deletion of words into and from a document at any node ) , all in time ~ O ( 1 ) . Further , our scheme can be implemented on open source distributed streaming systems such as Yahoo ! S4 or Twitter 's Storm so that every social index operation takes ~ O ( 1 ) processing time and network queries in the worst case , and just two network queries in the common case where the reverse index corresponding to the query keyword is much smaller than the memory available at any distributed compute node . Building on Das Sarma et al. 's approximate distance oracle , the worst-case approximation ratio of our scheme is ~ O ( 1 ) for undirected networks . Our simulations on the social network Twitter as well as synthetic networks show that in practice , the approximation ratio is actually close to 1 for both directed and undirected networks . We believe that this work is the first demonstration of the feasibility of social search with real-time text updates at large scales ."
"415","Learning causality for news events prediction The problem we tackle in this work is , given a present news event , to generate a plausible future event that can be caused by the given event . We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques . Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor . To obtain precise labeled causality examples , we mine 150 years of news articles , and apply semantic natural language modeling techniques to titles containing certain predefined causality patterns . For generalization , the model uses a vast amount of world knowledge ontologies mined from LinkedData , containing ~ 200 datasets with approximately 20 billion relations . Empirical evaluation on real news articles shows that our Pundit algorithm reaches a human-level performance ."
"416","On the analysis of cascading style sheets Developing and maintaining cascading style sheets ( CSS ) is an important issue to web developers as they suffer from the lack of rigorous methods . Most existing means rely on validators that check syntactic rules , and on runtime debuggers that check the behavior of a CSS style sheet on a particular document instance . However , the aim of most style sheets is to be applied to an entire set of documents , usually defined by some schema . To this end , a CSS style sheet is usually written w.r.t. a given schema . While usual debugging tools help reducing the number of bugs , they do not ultimately allow to prove properties over the whole set of documents to which the style sheet is intended to be applied . We propose a novel approach to fill this lack . We introduce ideas borrowed from the fields of logic and compile-time verification for the analysis of CSS style sheets . We present an original tool based on recent advances in tree logics . The tool is capable of statically detecting a wide range of errors ( such as empty CSS selectors and semantically equivalent selectors ) , as well as proving properties related to sets of documents ( such as coverage of styling information ) , in the presence or absence of schema information . This new tool can be used in addition to existing runtime debuggers to ensure a higher level of quality of CSS style sheets ."
"417","Discovering geographical topics in the twitter stream Micro-blogging services have become indispensable communication tools for online users for disseminating breaking news , eyewitness accounts , individual expression , and protest groups . Recently , Twitter , along with other online social networking services such as Foursquare , Gowalla , Facebook and Yelp , have started supporting location services in their messages , either explicitly , by letting users choose their places , or implicitly , by enabling geo-tagging , which is to associate messages with latitudes and longitudes . This functionality allows researchers to address an exciting set of questions : 1 ) How is information created and shared across geographical locations , 2 ) How do spatial and linguistic characteristics of people vary across regions , and 3 ) How to model human mobility . Although many attempts have been made for tackling these problems , previous methods are either complicated to be implemented or oversimplified that can not yield reasonable performance . It is a challenge task to discover topics and identify users ' interests from these geo-tagged messages due to the sheer amount of data and diversity of language variations used on these location sharing services . In this paper we focus on Twitter and present an algorithm by modeling diversity in tweets based on topical diversity , geographical diversity , and an interest distribution of the user . Furthermore , we take the Markovian nature of a user 's location into account . Our model exploits sparse factorial coding of the attributes , thus allowing us to deal with a large and diverse set of covariates efficiently . Our approach is vital for applications such as user profiling , content recommendation and topic tracking . We show high accuracy in location estimation based on our model . Moreover , the algorithm identifies interesting topics based on location and language ."
"418","Information transfer in social media Recent research has explored the increasingly important role of social media by examining the dynamics of individual and group behavior , characterizing patterns of information diffusion , and identifying influential individuals . In this paper we suggest a measure of causal relationships between nodes based on the information -- theoretic notion of transfer entropy , or information transfer . This theoretically grounded measure is based on dynamic information , captures fine -- grain notions of influence , and admits a natural , predictive interpretation . Networks inferred by transfer entropy can differ significantly from static friendship networks because most friendship links are not useful for predicting future dynamics . We demonstrate through analysis of synthetic and real-world data that transfer entropy reveals meaningful hidden network structures . In addition to altering our notion of who is influential , transfer entropy allows us to differentiate between weak influence over large groups and strong influence over small groups ."
"419","Echoes of power : language effects and power differences in social interaction Understanding social interaction within groups is key to analyzing online communities . Most current work focuses on structural properties : who talks to whom , and how such interactions form larger network structures . The interactions themselves , however , generally take place in the form of natural language -- either spoken or written -- and one could reasonably suppose that signals manifested in language might also provide information about roles , status , and other aspects of the group 's dynamics . To date , however , finding domain-independent language-based signals has been a challenge . Here , we show that in group discussions , power differentials between participants are subtly revealed by how much one individual immediately echoes the linguistic style of the person they are responding to . Starting from this observation , we propose an analysis framework based on linguistic coordination that can be used to shed light on power relationships and that works consistently across multiple types of power -- including a more `` static '' form of power based on status differences , and a more `` situational '' form of power in which one individual experiences a type of dependence on another . Using this framework , we study how conversational behavior can reveal power relationships in two very different settings : discussions among Wikipedians and arguments before the U. S. Supreme Court ."
"420","Mining photo-sharing websites to study ecological phenomena The popularity of social media websites like Flickr and Twitter has created enormous collections of user-generated content online . Latent in these content collections are observations of the world : each photo is a visual snapshot of what the world looked like at a particular point in time and space , for example , while each tweet is a textual expression of the state of a person and his or her environment . Aggregating these observations across millions of social sharing users could lead to new techniques for large-scale monitoring of the state of the world and how it is changing over time . In this paper we step towards that goal , showing that by analyzing the tags and image features of geo-tagged , time-stamped photos we can measure and quantify the occurrence of ecological phenomena including ground snow cover , snow fall and vegetation density . We compare several techniques for dealing with the large degree of noise in the dataset , and show how machine learning can be used to reduce errors caused by misleading tags and ambiguous visual content . We evaluate the accuracy of these techniques by comparing to ground truth data collected both by surface stations and by Earth-observing satellites . Besides the immediate application to ecology , our study gives insight into how to accurately crowd-source other types of information from large , noisy social sharing datasets ."
"421","Towards robust service compositions in the context of functionally diverse services Web service composition provides a means of customized and flexible integration of service functionalities . Quality-of-Service ( QoS ) optimization algorithms select services in order to adapt workflows to the non-functional requirements of the user . With increasing number of services in a workflow , previous approaches fail to achieve a sufficient reliability . Moreover , expensive ad-hoc replanning is required to deal with service failures . The major problem with such sequential application of planning and replanning is that it ignores the potential costs during the initial planning and they consequently are hidden from the decision maker . Our basic idea to overcome this substantial problem is to compute a QoS optimized selection of service clusters that includes a sufficient number of backup services for each service employed . To support the human decision maker in the service selection task , our approach considers the possible repair costs directly in the initial composition . On the basis of a multi-objective approach and using a suitable service selection interface , the decision maker can select compositions in line with his\/her personal risk preferences ."
"422","Learning from the past : answering new questions with past answers Community-based Question Answering sites , such as Yahoo ! Answers or Baidu Zhidao , allow users to get answers to complex , detailed and personal questions from other users . However , since answering a question depends on the ability and willingness of users to address the asker 's needs , a significant fraction of the questions remain unanswered . We measured that in Yahoo ! Answers , this fraction represents 15 % of all incoming English questions . At the same time , we discovered that around 25 % of questions in certain categories are recurrent , at least at the question-title level , over a period of one year . We attempt to reduce the rate of unanswered questions in Yahoo ! Answers by reusing the large repository of past resolved questions , openly available on the site . More specifically , we estimate the probability whether certain new questions can be satisfactorily answered by a best answer from the past , using a statistical model specifically trained for this task . We leverage concepts and methods from query-performance prediction and natural language processing in order to extract a wide range of features for our model . The key challenge here is to achieve a level of quality similar to the one provided by the best human answerers . We evaluated our algorithm on offline data extracted from Yahoo ! Answers , but more interestingly , also on online data by using three `` live '' answering robots that automatically provide past answers to new questions when a certain degree of confidence is reached . We report the success rate of these robots in three active Yahoo ! Answers categories in terms of both accuracy , coverage and askers ' satisfaction . This work presents a first attempt , to the best of our knowledge , of automatic question answering to questions of social nature , by reusing past answers of high quality ."
"423","Structured query suggestion for specialization and parallel movement : effect on search behaviors Query suggestion , which enables the user to revise a query with a single click , has become one of the most fundamental features of Web search engines . However , it is often difficult for the user to choose from a list of query suggestions , and to understand the relation between an input query and suggested ones . In this paper , we propose a new method to present query suggestions to the user , which has been designed to help two popular query reformulation actions , namely , specialization ( e.g. from `` nikon '' to `` nikon camera '' ) and parallel movement ( e.g. from `` nikon camera '' to `` canon camera '' ) . Using a query log collected from a popular commercial Web search engine , our prototype called SParQS classifies query suggestions into automatically generated categories and generates a label for each category . Moreover , SParQS presents some new entities as alternatives to the original query ( e.g. `` canon '' in response to the query `` nikon '' ) , together with their query suggestions classified in the same way as the original query 's suggestions . We conducted a task-based user study to compare SParQS with a traditional `` flat list '' query suggestion interface . Our results show that the SParQS interface enables subjects to search more successfully than the flat list case , even though query suggestions presented were exactly the same in the two interfaces . In addition , the subjects found the query suggestions more helpful when they were presented in the SParQS interface rather than in a flat list ."
"424","Are web users really Markovian ? User modeling on the Web has rested on the fundamental assumption of Markovian behavior -- a user 's next action depends only on her current state , and not the history leading up to the current state . This forms the underpinning of PageRank web ranking , as well as a number of techniques for targeting advertising to users . In this work we examine the validity of this assumption , using data from a number of Web settings . Our main result invokes statistical order estimation tests for Markov chains to establish that Web users are not , in fact , Markovian . We study the extent to which the Markovian assumption is invalid , and derive a number of avenues for further research ."
"425","Lightweight automatic face annotation in media pages Labeling human faces in images contained in Web media stories enables enriching the user experience offered by media sites . We propose a lightweight framework for automatic image annotation that exploits named entities mentioned in the article to significantly boost the accuracy of face recognition . While previous works in the area labor to train comprehensive offline visual models for a pre-defined universe of candidates , our approach models the people mentioned in a given story on the y , using a standard Web image search engine as an image sampling mechanism . We overcome multiple sources of noise introduced by this ad-hoc process , to build a fast and robust end-to-end system from off-the-shelf error-prone text analysis and machine vision components . In experiments conducted on approximately 900 faces depicted in 500 stories from a major celebrity news website , we were able to correctly label 81.5 % of the faces while mislabeling 14.8 % of them ."
"426","Community detection in incomplete information networks With the recent advances in information networks , the problem of community detection has attracted much attention in the last decade . While network community detection has been ubiquitous , the task of collecting complete network data remains challenging in many real-world applications . Usually the collected network is incomplete with most of the edges missing . Commonly , in such networks , all nodes with attributes are available while only the edges within a few local regions of the network can be observed . In this paper , we study the problem of detecting communities in incomplete information networks with missing edges . We first learn a distance metric to reproduce the link-based distance between nodes from the observed edges in the local information regions . We then use the learned distance metric to estimate the distance between any pair of nodes in the network . A hierarchical clustering approach is proposed to detect communities within the incomplete information networks . Empirical studies on real-world information networks demonstrate that our proposed method can effectively detect community structures within incomplete information networks ."
"427","Semantic navigation on the web of data : specification of routes , web fragments and actions The massive semantic data sources linked in the Web of Data give new meaning to old features like navigation ; introduce new challenges like semantic specification of Web fragments ; and make it possible to specify actions relying on semantic data . In this paper we introduce a declarative language to face these challenges . Based on navigational features , it is designed to specify fragments of the Web of Data and actions to be performed based on these data . We implement it in a centralized fashion , and show its power and performance . Finally , we explore the same ideas in a distributed setting , showing their feasibility , potentialities and challenges ."
"428","Counting beyond a Yottabyte , or how SPARQL 1.1 property paths will prevent adoption of the standard SPARQL - the standard query language for querying RDF - provides only limited navigational functionalities , although these features are of fundamental importance for graph data formats such as RDF . This has led the W3C to include the property path feature in the upcoming version of the standard , SPARQL 1.1 . We tested several implementations of SPARQL 1.1 handling property path queries , and we observed that their evaluation methods for this class of queries have a poor performance even in some very simple scenarios . To formally explain this fact , we conduct a theoretical study of the computational complexity of property paths evaluation . Our results imply that the poor performance of the tested implementations is not a problem of these particular systems , but of the specification itself . In fact , we show that any implementation that adheres to the SPARQL 1.1 specification ( as of November 2011 ) is doomed to show the same behavior , the key issue being the need for counting solutions imposed by the current specification . We provide several intractability results , that together with our empirical results , provide strong evidence against the current semantics of SPARQL 1.1 property paths . Finally , we put our results in perspective , and propose a natural alternative semantics with tractable evaluation , that we think may lead to a wide adoption of the language by practitioners , developers and theoreticians ."
"429","Joint relevance and freshness learning from clickthroughs for news search In contrast to traditional Web search , where topical relevance is often the main selection criterion , news search is characterized by the increased importance of freshness . However , the estimation of relevance and freshness , and especially the relative importance of these two aspects , are highly specific to the query and the time when the query was issued . In this work , we propose a unified framework for modeling the topical relevance and freshness , as well as their relative importance , based on click logs . We use click statistics and content analysis techniques to define a set of temporal features , which predict the right mix of freshness and relevance for a given query . Experimental results on both historical click data and editorial judgments demonstrate the effectiveness of the proposed approach ."
"430","Multi-objective ranking of comments on web With the explosion of information on any topic , the need for ranking is becoming very critical . Ranking typically depends on several aspects . Products , for example , have several aspects like price , recency , rating , etc. . Product ranking has to bring the `` best '' product which is recent and highly rated . Hence ranking has to satisfy multiple objectives . In this paper , we explore multi-objective ranking of comments using Hodge decomposition . While Hodge decomposition produces a globally consistent ranking , a globally inconsistent component is also present . We propose an active learning strategy for the reduction of this component . Finally , we develop techniques for online Hodge decomposition . We experimentally validate the ideas presented in this paper ."
"431","Unsupervised extraction of template structure in web search queries Web search queries are an encoding of the user 's search intent and extracting structured information from them can facilitate central search engine operations like improving the ranking of search results and advertisements . Not surprisingly , this area has attracted a lot of attention in the research community in the last few years . The problem is , however , made challenging by the fact that search queries tend to be extremely succinct ; a condensation of user search needs to the bare-minimum set of keywords . In this paper we consider the problem of extracting , with no manual intervention , the hidden structure behind the observed search queries in a domain : the origins of the constituent keywords as well as the manner the individual keywords are assembled together . We formalize important properties of the problem and then give a principled solution based on generative models that satisfies these properties . Using manually labeled data we show that the query templates extracted by our solution are superior to those discovered by strong baseline methods . The query templates extracted by our approach have potential uses in many search engine tasks ; query answering , advertisement matching and targeting , to name a few . In this paper we study one such task , estimating Query-Advertisability , and empirically demonstrate that using extracted template information can improve performance over and above the current state-of-the-art ."
"432","Actions speak as loud as words : predicting relationships from social behavior data In recent years , new studies concentrating on analyzing user personality and finding credible content in social media have become quite popular . Most such work augments features from textual content with features representing the user 's social ties and the tie strength . Social ties are crucial in understanding the network the people are a part of . However , textual content is extremely useful in understanding topics discussed and the personality of the individual . We bring a new dimension to this type of analysis with methods to compute the type of ties individuals have and the strength of the ties in each dimension . We present a new genre of behavioral features that are able to capture the `` function '' of a specific relationship without the help of textual features . Our novel features are based on the statistical properties of communication patterns between individuals such as reciprocity , assortativity , attention and latency . We introduce a new methodology for determining how such features can be compared to textual features , and show , using Twitter data , that our features can be used to capture contextual information present in textual features very accurately . Conversely , we also demonstrate how textual features can be used to determine social attributes related to an individual ."
"433","Declarative platform for data sourcing games Harnessing a crowd of users for the collection of mass data ( data sourcing ) has recently become a wide-spread practice . One effective technique is based on games as a tool that attracts the crowd to contribute useful facts . We focus here on the data management layer of such games , and observe that the development of this layer involves challenges such as dealing with probabilistic data , combined with recursive manipulation of this data . These challenges are difficult to address using current declarative data management framework works , and we thus propose here a novel such framework , and demonstrate its usefulness in expressing different aspects in the data management of Trivia-like games . We have implemented a system prototype with our novel data management framework at its core , and we highlight key issues in the system design , as well as our experimentations that indicate the usefulness and scalability of the approach ."
"434","Distributed graph pattern matching Graph simulation has been adopted for pattern matching to reduce the complexity and capture the need of novel applications . With the rapid development of the Web and social networks , data is typically distributed over multiple machines . Hence a natural question raised is how to evaluate graph simulation on distributed data . To our knowledge , no such distributed algorithms are in place yet . This paper settles this question by providing evaluation algorithms and optimizations for graph simulation in a distributed setting . ( 1 ) We study the impacts of components and data locality on the evaluation of graph simulation . ( 2 ) We give an analysis of a large class of distributed algorithms , captured by a message-passing model , for graph simulation . We also identify three complexity measures : visit times , makespan and data shipment , for analyzing the distributed algorithms , and show that these measures are essentially controversial with each other . ( 3 ) We propose distributed algorithms and optimization techniques that exploit the properties of graph simulation and the analyses of distributed algorithms . ( 4 ) We experimentally verify the effectiveness and efficiency of these algorithms , using both real-life and synthetic data ."
"435","Mr. LDA : a flexible large scale topic modeling package using variational inference in MapReduce Latent Dirichlet Allocation ( LDA ) is a popular topic modeling technique for exploring document collections . Because of the increasing prevalence of large datasets , there is a need to improve the scalability of inference for LDA . In this paper , we introduce a novel and flexible large scale topic modeling package in MapReduce ( Mr. LDA ) . As opposed to other techniques which use Gibbs sampling , our proposed framework uses variational inference , which easily fits into a distributed environment . More importantly , this variational implementation , unlike highly tuned and specialized implementations based on Gibbs sampling , is easily extensible . We demonstrate two extensions of the models possible with this scalable framework : informed priors to guide topic discovery and extracting topics from a multilingual corpus . We compare the scalability of Mr. LDA against Mahout , an existing large scale topic modeling package . Mr. LDA out-performs Mahout both in execution speed and held-out likelihood ."
"436","QUBE : a quick algorithm for updating betweenness centrality The betweenness centrality of a vertex in a graph is a measure for the participation of the vertex in the shortest paths in the graph . The Betweenness centrality is widely used in network analyses . Especially in a social network , the recursive computation of the betweenness centralities of vertices is performed for the community detection and finding the influential user in the network . Since a social network graph is frequently updated , it is necessary to update the betweenness centrality efficiently . When a graph is changed , the betweenness centralities of all the vertices should be recomputed from scratch using all the vertices in the graph . To the best of our knowledge , this is the first work that proposes an efficient algorithm which handles the update of the betweenness centralities of vertices in a graph . In this paper , we propose a method that efficiently reduces the search space by finding a candidate set of vertices whose betweenness centralities can be updated and computes their betweenness centeralities using candidate vertices only . As the cost of calculating the betweenness centrality mainly depends on the number of vertices to be considered , the proposed algorithm significantly reduces the cost of calculation . The proposed algorithm allows the transformation of an existing algorithm which does not consider the graph update . Experimental results on large real datasets show that the proposed algorithm speeds up the existing algorithm 2 to 2418 times depending on the dataset ."
"437","On directly mapping relational databases to RDF and OWL Mapping relational databases to RDF is a fundamental problem for the development of the Semantic Web . We present a solution , inspired by draft methods defined by the W3C where relational databases are directly mapped to RDF and OWL . Given a relational database schema and its integrity constraints , this direct mapping produces an OWL ontology , which , provides the basis for generating RDF instances . The semantics of this mapping is defined using Datalog . Two fundamental properties are information preservation and query preservation . We prove that our mapping satisfies both conditions , even for relational databases that contain null values . We also consider two desirable properties : monotonicity and semantics preservation . We prove that our mapping is monotone and also prove that no monotone mapping , including ours , is semantic preserving . We realize that monotonicity is an obstacle for semantic preservation and thus present a non-monotone direct mapping that is semantics preserving ."
"438","Strategic formation of credit networks Credit networks are an abstraction for modeling trust between agents in a network . Agents who do not directly trust each other can transact through exchange of IOUs ( obligations ) along a chain of trust in the network . Credit networks are robust to intrusion , can enable transactions between strangers in exchange economies , and have the liquidity to support a high rate of transactions . We study the formation of such networks when agents strategically decide how much credit to extend each other . When each agent trusts a fixed set of other agents , and transacts directly only with those it trusts , the formation game is a potential game and all Nash equilibria are social optima . Moreover , the Nash equilibria of this game are equivalent in a very strong sense : the sequences of transactions that can be supported from each equilibrium credit network are identical . When we allow transactions over longer paths , the game may not admit a Nash equilibrium , and even when it does , the price of anarchy may be unbounded . Hence , we study two special cases . First , when agents have a shared belief about the trustworthiness of each agent , the networks formed in equilibrium have a star-like structure . Though the price of anarchy is unbounded , myopic best response quickly converges to a social optimum . Similar star-like structures are found in equilibria of heuristic strategies found via simulation . In addition , we simulate a second case where agents may have varying information about each others ' trustworthiness based on their distance in a social network . Empirical game analysis of these scenarios suggests that star structures arise only when defaults are relatively rare , and otherwise , credit tends to be issued over short social distances conforming to the locality of information ."
"439","Answering search queries with CrowdSearcher Web users are increasingly relying on social interaction to complete and validate the results of their search activities . While search systems are superior machines to get world-wide information , the opinions collected within friends and expert\/local communities can ultimately determine our decisions : human curiosity and creativity is often capable of going much beyond the capabilities of search systems in scouting `` interesting '' results , or suggesting new , unexpected search directions . Such personalized interaction occurs in most times aside of the search systems and processes , possibly instrumented and mediated by a social network ; when such interaction is completed and users resort to the use of search systems , they do it through new queries , loosely related to the previous search or to the social interaction . In this paper we propose CrowdSearcher , a novel search paradigm that embodies crowds as first-class sources for the information seeking process . CrowdSearcher aims at filling the gap between generalized search systems , which operate upon world-wide information - including facts and recommendations as crawled and indexed by computerized systems - with social systems , capable of interacting with real people , in real time , to capture their opinions , suggestions , emotions . The technical contribution of this paper is the discussion of a model and architecture for integrating computerized search with human interaction , by showing how search systems can drive and encapsulate social systems . In particular we show how social platforms , such as Facebook , LinkedIn and Twitter , can be used for crowdsourcing search-related tasks ; we demonstrate our approach with several prototypes and we report on our experiment upon real user communities ."
"440","CloudGenius : decision support for web server cloud migration Cloud computing is the latest computing paradigm that delivers hardware and software resources as virtualized services in which users are free from the burden of worrying about the low-level system administration details . Migrating Web applications to Cloud services and integrating Cloud services into existing computing infrastructures is non-trivial . It leads to new challenges that often require innovation of paradigms and practices at all levels : technical , cultural , legal , regulatory , and social . The key problem in mapping Web applications to virtualized Cloud services is selecting the best and compatible mix of software images ( e.g. , Web server image ) and infrastructure services to ensure that Quality of Service ( QoS ) targets of an application are achieved . The fact that , when selecting Cloud services , engineers must consider heterogeneous sets of criteria and complex dependencies between infrastructure services and software images , which are impossible to resolve manually , is a critical issue . To overcome these challenges , we present a framework ( called CloudGenius ) which automates the decision-making process based on a model and factors specifically for Web server migration to the Cloud . CloudGenius leverages a well known multi-criteria decision making technique , called Analytic Hierarchy Process , to automate the selection process based on a model , factors , and QoS parameters related to an application . An example application demonstrates the applicability of the theoretical CloudGenius approach . Moreover , we present an implementation of CloudGenius that has been validated through experiments ."
"441","Towards network-aware service composition in the cloud Service-Oriented Computing ( SOC ) enables the composition of loosely coupled services provided with varying Quality of Service ( QoS ) levels . Selecting a ( near - ) optimal set of services for a composition in terms of QoS is crucial when many functionally equivalent services are available . With the advent of Cloud Computing , both the number of such services and their distribution across the network are rising rapidly , increasing the impact of the network on the QoS of such compositions . Despite this , current approaches do not differentiate between the QoS of services themselves and the QoS of the network . Therefore , the computed latency differs substantially from the actual latency , resulting in suboptimal QoS for service compositions in the cloud . Thus , we propose a network-aware approach that handles the QoS of services and the QoS of the network independently . First , we build a network model in order to estimate the network latency between arbitrary services and potential users . Our selection algorithm then leverages this model to find compositions that will result in a low latency given an employed execution policy . In our evaluation , we show that our approach efficiently computes compositions with much lower latency than current approaches ."
"442","Practical end-to-end web content integrity Widespread growth of open wireless hotspots has made it easy to carry out man-in-the-middle attacks and impersonate web sites . Although HTTPS can be used to prevent such attacks , its universal adoption is hindered by its performance cost and its inability to leverage caching at intermediate servers ( such as CDN servers and caching proxies ) while maintaining end-to-end security . To complement HTTPS , we revive an old idea from SHTTP , a protocol that offers end-to-end web integrity without confidentiality . We name the protocol HTTPi and give it an efficient design that is easy to deploy for today 's web . In particular , we tackle several previously-unidentified challenges , such as supporting progressive page loading on the client 's browser , handling mixed content , and defining access control policies among HTTP , HTTPi , and HTTPS content from the same domain . Our prototyping and evaluation experience show that HTTPi incurs negligible performance overhead over HTTP , can leverage existing web infrastructure such as CDNs or caching proxies without any modifications to them , and can make many of the mixed-content problems in existing HTTPS web sites easily go away . Based on this experience , we advocate browser and web server vendors to adopt HTTPi ."
"443","Recommendations to boost content spread in social networks Content sharing in social networks is a powerful mechanism for discovering content on the Internet . The degree to which content is disseminated within the network depends on the connectivity relationships among network nodes . Existing schemes for recommending connections in social networks are based on the number of common neighbors , similarity of user profiles , etc. . However , such similarity-based connections do not consider the amount of content discovered . In this paper , we propose novel algorithms for recommending connections that boost content propagation in a social network without compromising on the relevance of the recommendations . Unlike existing work on influence propagation , in our environment , we are looking for edges instead of nodes , with a bound on the number of incident edges per node . We show that the content spread function is not submodular , and develop approximation algorithms for computing a near-optimal set of edges . Through experiments on real-world social graphs such as Flickr and Twitter , we show that our approximation algorithms achieve content spreads that are as much as 90 times higher compared to existing heuristics for recommending connections ."
"444","Leveraging user comments for aesthetic aware image search reranking The increasing number of images available online has created a growing need for efficient ways to search for relevant content . Text-based query search is the most common approach to retrieve images from the Web . In this approach , the similarity between the input query and the metadata of images is used to find relevant information . However , as the amount of available images grows , the number of relevant images also increases , all of them sharing very similar metadata but differing in other visual characteristics . This paper studies the influence of visual aesthetic quality in search results as a complementary attribute to relevance . By considering aesthetics , a new ranking parameter is introduced aimed at improving the quality at the top ranks when large amounts of relevant results exist . Two strategies for aesthetic rating inference are proposed : one based on visual content , another based on the analysis of user comments to detect opinions about the quality of images . The results of a user study with $ 58 $ participants show that the comment-based aesthetic predictor outperforms the visual content-based strategy , and reveals that aesthetic-aware rankings are preferred by users searching for photographs on the Web ."
"445","Beyond dwell time : estimating document relevance from cursor movements and other post-click searcher behavior Result clickthrough statistics and dwell time on clicked results have been shown valuable for inferring search result relevance , but the interpretation of these signals can vary substantially for different tasks and users . This paper shows that that post-click searcher behavior , such as cursor movement and scrolling , provides additional clues for better estimating document relevance . To this end , we identify patterns of examination and interaction behavior that correspond to viewing a relevant or non-relevant document , and design a new Post-Click Behavior ( PCB ) model to capture these patterns . To our knowledge , PCB is the first to successfully incorporate post-click searcher interactions such as cursor movements and scrolling on a landing page for estimating document relevance . We evaluate PCB on a dataset collected from a controlled user study that contains interactions gathered from hundreds of unique queries , result clicks , and page examinations . The experimental results show that PCB is significantly more effective than using page dwell time information alone , both for estimating the explicit judgments of each user , and for re-ranking the results using the estimated relevance ."
"446","A dual-mode user interface for accessing 3D content on the world wide web The Web evolved from a text-based system to the current rich and interactive medium that supports images , 2D graphics , audio and video . The major media type that is still missing is 3D graphics . Although various approaches have been proposed ( most notably VRML\/X3D ) , they have not been widely adopted . One reason for the limited acceptance is the lack of 3D interaction techniques that are optimal for the hypertext-based Web interface . We present a novel strategy for accessing integrated information spaces , where hypertext and 3D graphics data are simultaneously available and linked . We introduce a user interface that has two modes between which a user can switch anytime : the driven by simple hypertext-based interactions `` do n't - make-me-think '' mode , where a 3D scene is embedded in hypertext and the more immersive 3D `` take-me-to-the-Wonderland '' mode , which immerses the hypertextual annotations into the 3D scene . A user study is presented , which characterizes the user interface in terms of its efficiency and usability ."
"447","Collective context-aware topic models for entity disambiguation A crucial step in adding structure to unstructured data is to identify references to entities and disambiguate them . Such disambiguated references can help enhance readability and draw similarities across different pieces of running text in an automated fashion . Previous research has tackled this problem by first forming a catalog of entities from a knowledge base , such as Wikipedia , and then using this catalog to disambiguate references in unseen text . However , most of the previously proposed models either do not use all text in the knowledge base , potentially missing out on discriminative features , or do not exploit word-entity proximity to learn high-quality catalogs . In this work , we propose topic models that keep track of the context of every word in the knowledge base ; so that words appearing within the same context as an entity are more likely to be associated with that entity . Thus , our topic models utilize all text present in the knowledge base and help learn high-quality catalogs . Our models also learn groups of co-occurring entities thus enabling collective disambiguation . Unlike most previous topic models , our models are non-parametric and do not require the user to specify the exact number of groups present in the knowledge base . In experiments performed on an extract of Wikipedia containing almost 60,000 references , our models outperform SVM-based baselines by as much as 18 % in terms of disambiguation accuracy translating to an increment of almost 11,000 correctly disambiguated references ."
"448","Online modeling of proactive moderation system for auction fraud detection We consider the problem of building online machine-learned models for detecting auction frauds in e-commence web sites . Since the emergence of the world wide web , online shopping and online auction have gained more and more popularity . While people are enjoying the benefits from online trading , criminals are also taking advantages to conduct fraudulent activities against honest parties to obtain illegal profit . Hence proactive fraud-detection moderation systems are commonly applied in practice to detect and prevent such illegal and fraud activities . Machine-learned models , especially those that are learned online , are able to catch frauds more efficiently and quickly than human-tuned rule-based systems . In this paper , we propose an online probit model framework which takes online feature selection , coefficient bounds from human knowledge and multiple instance learning into account simultaneously . By empirical experiments on a real-world online auction fraud detection data we show that this model can potentially detect more frauds and significantly reduce customer complaints compared to several baseline models and the human-tuned rule-based system ."
"449","D2RQ\/update : updating relational data via virtual RDF D2RQ is a popular RDB-to-RDF mapping platform that supports mapping relational databases to RDF and posing SPARQL queries to these relational databases . However , D2RQ merely provides a read-only RDF view on relational databases . Thus , we introduce D2RQ\/Update -- an extension of D2RQ to enable executing SPARQL\/Update statements on the mapped data , and to facilitate the creation of a read-write Semantic Web ."
"450","LINDEN : linking named entities with knowledge base via semantic knowledge Integrating the extracted facts with an existing knowledge base has raised an urgent need to address the problem of entity linking . Specifically , entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base . However , this task is challenging due to name ambiguity , textual inconsistency , and lack of world knowledge in the knowledge base . Several methods have been proposed to tackle this problem , but they are largely based on the co-occurrence statistics of terms between the text around the entity mention and the document associated with the entity . In this paper , we propose LINDEN , a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet , by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base . We extensively evaluate the performance of our proposed LINDEN over two public data sets and empirical results show that LINDEN significantly outperforms the state-of-the-art methods in terms of accuracy ."
"451","Evaluating the effectiveness of search task trails In this paper , we introduce `` task trail '' as a new concept to understand user search behaviors . We define task to be an atomic user information need . Web search logs have been studied mainly at session or query level where users may submit several queries within one task and handle several tasks within one session . Although previous studies have addressed the problem of task identification , little is known about the advantage of using task over session and query for search applications . In this paper , we conduct extensive analyses and comparisons to evaluate the effectiveness of task trails in three search applications : determining user satisfaction , predicting user search interests , and query suggestion . Experiments are conducted on large scale datasets from a commercial search engine . Experimental results show that : ( 1 ) Sessions and queries are not as precise as tasks in determining user satisfaction . ( 2 ) Task trails provide higher web page utilities to users than other sources . ( 3 ) Tasks represent atomic user information needs , and therefore can preserve topic similarity between query pairs . ( 4 ) Task-based query suggestion can provide complementary results to other models . The findings in this paper verify the need to extract task trails from web search logs and suggest potential applications in search and recommendation systems ."
"452","WISER : a web-based interactive route search system for smartphones Many smartphones , nowadays , use GPS to detect the location of the user , and can use the Internet to interact with remote location-based services . These two capabilities support online navigation that incorporates search . In this demo we presents WISER -- a system for Web-based Interactive Search en Route . In the system , users perform route search by providing ( 1 ) a target location , and ( 2 ) search terms that specify types of geographic entities to be visited . The task is to find a route that minimizes the travel distance from the initial location of the user to the target , via entities of the specified types . However , planning a route under conditions of uncertainty requires the system to take into account the possibility that some visited entities will not satisfy the search requirements , so that the route may need to go via several entities of the same type . In an interactive search , the user provides feedback regarding her satisfaction with entities she visits during the travel , and the system changes the route , in real time , accordingly . The goal is to use the interaction for computing a route that is more effective than a route that is computed in a non-interactive fashion ."
"453","A flexible generative model for preference aggregation Many areas of study , such as information retrieval , collaborative filtering , and social choice face the preference aggregation problem , in which multiple preferences over objects must be combined into a consensus ranking . Preferences over items can be expressed in a variety of forms , which makes the aggregation problem difficult . In this work we formulate a flexible probabilistic model over pairwise comparisons that can accommodate all these forms . Inference in the model is very fast , making it applicable to problems with hundreds of thousands of preferences . Experiments on benchmark datasets demonstrate superior performance to existing methods"
"454","Evaluation with informational and navigational intents Given an ambiguous or underspecified query , search result diversification aims at accomodating different user intents within a single `` entry-point '' result page . However , some intents are informational , for which many relevant pages may help , while others are navigational , for which only one web page is required . We propose new evaluation metrics for search result diversification that considers this distinction , as well as a simple method for comparing the intuitiveness of a given pair of metrics quantitatively . Our main experimental findings are : ( a ) In terms of discriminative power which reflects statistical reliability , the proposed metrics , DIN #_# - nDCG and P+Q #_# , are comparable to intent recall and D #_# - nDCG , and possibly superior to α-nDCG ; ( b ) In terms of preference agreement with intent recall , P+Q #_# is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises diversity ; and ( c ) In terms of preference agreement with effective precision , DIN #_# - nDCG is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises relevance . Moreover , DIN #_# - nDCG may be the most intuitive as a metric that considers both diversity and relevance . In addition , we demonstrate that the randomised Tukey 's Honestly Significant Differences test that takes the entire set of available runs into account is substantially more conservative than the paired bootstrap test that only considers one run pair at a time , and therefore recommend the former approach for significance testing when a set of runs is available for evaluation ."
"455","Template-based question answering over RDF data As an increasing amount of RDF data is published as Linked Data , intuitive ways of accessing this data become more and more important . Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity . Most question answering systems translate questions into triples which are matched against the RDF data to retrieve an answer , typically relying on some similarity metric . However , in many cases , triples do not represent a faithful representation of the semantic structure of the natural language question , with the result that more expressive queries can not be answered . To circumvent this problem , we present a novel approach that relies on a parse of the question to produce a SPARQL template that directly mirrors the internal structure of the question . This template is then instantiated using statistical entity identification and predicate detection . We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches ."
"456","ZenCrowd : leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking We tackle the problem of entity linking for large collections of online pages ; Our system , ZenCrowd , identifies entities from natural language text using state of the art techniques and automatically connects them to the Linked Open Data cloud . We show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro-tasks on an online crowdsourcing platform . We develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers . We evaluate ZenCrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links , while limiting the amount of work performed by the crowd ."
"457","Understanding task-driven information flow in collaborative networks Collaborative networks are a special type of social network formed by members who collectively achieve specific goals , such as fixing software bugs and resolving customers ' problems . In such networks , information flow among members is driven by the tasks assigned to the network , and by the expertise of its members to complete those tasks . In this work , we analyze real-life collaborative networks to understand their common characteristics and how information is routed in these networks . Our study shows that collaborative networks exhibit significantly different properties compared with other complex networks . Collaborative networks have truncated power-law node degree distributions and other organizational constraints . Furthermore , the number of steps along which information is routed follows a truncated power-law distribution . Based on these observations , we developed a network model that can generate synthetic collaborative networks subject to certain structure constraints . Moreover , we developed a routing model that emulates task-driven information routing conducted by human beings in a collaborative network . Together , these two models can be used to study the efficiency of information routing for different types of collaborative networks -- a problem that is important in practice yet difficult to solve without the method proposed in this paper ."
"458","Three theses of representation in the semantic web The Semantic Web is vitally dependent on a formal meaning for the constructs of its languages . For Semantic Web languages to work well together their formal meanings must employ a common view ( or thesis ) of representation , otherwise it will not be possible to reconcile documents written in different languages . The thesis of representation underlying RDF and RDFS is particularly troublesome in this regard , as it has several unusual aspects , both semantic and syntactic . A more-standard thesis of representation would result in the ability to reuse existing results and tools in the Semantic Web ."
"459","Law-governed peer-to-peer auctions This paper proposes a flexible architecture for the creation of Internet auctions . It allows the custom definition of the auction parameters , and provides a decentralized control of the auction process . Auction policies are defined as laws in the Law Governed Interaction ( LGI ) paradigm . Each of these laws specifies not only the auction algorithm itself ( e.g. open-cry , dutch , etc. ) but also how to handle the other parameters usually involved in the online auctions , such as certification , auditioning , and treatment of complaints . LGI is used to enforce the rules established in the auction policy within the agents involved in the process . After the agents find out about the actions , they interact in a peer-to-peer communication protocol , reducing the role of the centralized auction room to an advertising registry , and taking profit of the distributed nature of the Internet to conduct the auction . The paper presents an example of an auction law , illustrating the use of the proposed architecture ."
"460","Hunter gatherer : interaction support for the creation and management of within-web-page collections Hunter Gatherer is an interface that lets Web users carry out three main tasks : ( 1 ) collect components from within Web pages ; ( 2 ) represent those components in a collection ; ( 3 ) edit those component collections . Our research shows that while the practice of making collections of content from within Web pages is common , it is not frequent , due in large part to poor interaction support in existing tools . We engaged with users in task analysis as well as iterative design reviews in order to understand the interaction issues that are part of within-Web-page collection making and to design an interaction that would support that process . We report here on that design development , as well as on the evaluations of the tool that evolved from that process , and the future work stemming from these results , in which our critical question is : what happens to users perceptions and expectations of web-based information ( their web-based information management practices ) when they can treat this information as harvestable , recontextualizable data , rather than as fixed pages ?"
"461","Searching the workplace web The social impact from the World Wide Web can not be underestimated , but technologies used to build the Web are also revolutionizing the sharing of business and government information within intranets . In many ways the lessons learned from the Internet carry over directly to intranets , but others do not apply . In particular , the social forces that guide the development of intranets are quite different , and the determination of a `` good answer '' for intranet search is quite different than on the Internet . In this paper we study the problem of intranet search . Our approach focuses on the use of rank aggregation , and allows us to examine the effects of different heuristics on ranking of search results ."
"462","SemTag and seeker : bootstrapping the semantic web via automated semantic annotation This paper describes Seeker , a platform for large-scale text analytics , and SemTag , an application written on the platform to perform automated semantic tagging of large corpora . We apply SemTag to a collection of approximately 264 million web pages , and generate approximately 434 million automatically disambiguated semantic tags , published to the web as a label bureau providing metadata regarding the 434 million annotations . To our knowledge , this is the largest scale semantic tagging effort to date . We describe the Seeker platform , discuss the architecture of the SemTag application , describe a new disambiguation algorithm specialized to support ontological disambiguation of large-scale data , evaluate the algorithm , and present our final results with information about acquiring and making use of the semantic tags . We argue that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web ."
"463","A software framework for matchmaking based on semantic web technology An important objective of the Semantic Web is to make Electronic Commerce interactions more flexible and automated . To achieve this , standardization of ontologies , message content and message protocols will be necessary . In this paper we investigate how Semantic and Web Services technologies can be used to support service advertisement and discovery in e-commerce . In particular , we describe the design and implementation of a service matchmaking prototype which uses a DAML-S based ontology and a Description Logic reasoner to compare ontology based service descriptions . We also present the results of initial experiments testing the performance of this prototype implementation in a realistic agent based e-commerce scenario ."
"464","Accelerated focused crawling through online relevance feedback The organization of HTML into a tag tree structure , which is rendered by browsers as roughly rectangular regions with embedded text and HREF links , greatly helps surfers locate and click on links that best satisfy their information need . Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t. an information need , based on information limited to the HREF source page ? Such a capability would be of great interest in focused crawling and resource discovery , because it can fine-tune the priority of unvisited URLs in the crawl frontier , and reduce the number of irrelevant pages which are fetched and discarded ."
"465","Implementing physical hyperlinks using ubiquitous identifier resolution Identifier resolution is presented as a way to link the physical world with virtual Web resources . In this paradigm , designed to support nomadic users , the user employs a handheld , wirelessly connected , sensor-equipped device to read identifiers associated with physical entities . The identifiers are resolved into virtual resources or actions related to the physical entities - as though the user ` clicked on a physical hyperlink ' . We have integrated identifier resolution with the Web so that it can be deployed as ubiquitously as the Web , in the infrastructure and on wirelessly connected handheld devices . We enable users to capture resolution services and applications as Web resources in their local context . We use the Web to invoke resolution services , with a model of ` physical ' Web form-filling . We propose a scheme for binding identifiers to resources , to promote services and applications linking the physical and virtual worlds ."
"466","Probabilistic question answering on the web Web-based search engines such as Google and NorthernLight return documents that are relevant to a user query , not answers to user questions . We have developed an architecture that augments existing search engines so that they support natural language question answering . The process entails five steps : query modulation , document retrieval , passage extraction , phrase extraction , and answer ranking . In this paper we describe some probabilistic approaches to the last three of these stages . We show how our techniques apply to a number of existing search engines and we also present results contrasting three different methods for question answering . Our algorithm , probabilistic phrase reranking ( PPR ) using proximity and question type features achieves a total reciprocal document rank of .20 on the TREC 8 corpus . Our techniques have been implemented as a Web-accessible system , called NSIR ."
"467","Application specific data replication for edge services The emerging edge services architecture promises to improve the availability and performance of web services by replicating servers at geographically distributed sites . A key challenge in such systems is data replication and consistency so that edge server code can manipulate shared data without incurring the availability and performance penalties that would be incurred by accessing a traditional centralized database . This paper explores using a distributed object architecture to build an edge service system for an e-commerce application , an online bookstore represented by the TPC-W benchmark . We take advantage of application specific semantics to design distributed objects to manage a specific subset of shared information using simple and effective consistency models . Our experimental results show that by slightly relaxing consistency within individual distributed objects , we can build an edge service system that is highly available and efficient . For example , in one experiment we find that our object-based edge server system provides a factor of five improvement in response time over a traditional centralized cluster architecture and a factor of nine improvement over an edge service system that distributes code but retains a centralized database ."
"468","Learning to tag Social tagging provides valuable and crucial information for large-scale web image retrieval . It is ontology-free and easy to obtain ; however , irrelevant tags frequently appear , and users typically will not tag all semantic objects in the image , which is also called semantic loss . To avoid noises and compensate for the semantic loss , tag recommendation is proposed in literature . However , current recommendation simply ranks the related tags based on the single modality of tag co-occurrence on the whole dataset , which ignores other modalities , such as visual correlation . This paper proposes a multi-modality recommendation based on both tag and visual correlation , and formulates the tag recommendation as a learning problem . Each modality is used to generate a ranking feature , and Rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities . Experiments on Flickr data demonstrate the effectiveness of this learning-based multi-modality recommendation strategy ."
"469","Clustering for opportunistic communication We describe ongoing work on I2I , a system aimed at fostering opportunistic communication among users viewing or manipulating content on the Web and in productivity applications . Unlike previous work in which the URLs of Web resources are used to group users visiting the same resource , we present a more general framework for clustering work contexts to group users together that accounts for dynamic content and distributional properties of Web accesses which can limit the utility URL based systems . In addition , we describe a method for scaffolding asynchronous communication in the context of an ongoing task that takes into account the ephemeral nature of the location of content on the Web . The techniques we describe also nicely cover local files in progress , in addition to publicly available Web content . We present the results of several evaluations that indicate systems that use the techniques we employ may be more useful than systems that are strictly URL based ."
"470","The Eigentrust algorithm for reputation management in P2P networks Peer-to-peer file-sharing networks are currently receiving much attention as a means of sharing and distributing information . However , as recent experience shows , the anonymous , open nature of these networks offers an almost ideal environment for the spread of self-replicating inauthentic files . We describe an algorithm to decrease the number of downloads of inauthentic files in a peer-to-peer file-sharing network that assigns each peer a unique global trust value , based on the peer 's history of uploads . We present a distributed and secure method to compute global trust values , based on Power iteration . By having peers use these global trust values to choose the peers from whom they download , the network effectively identifies malicious peers and isolates them from the network . In simulations , this reputation system , called EigenTrust , has been shown to significantly decrease the number of inauthentic files on the network , even under a variety of conditions where malicious peers cooperate in an attempt to deliberately subvert the system ."
"471","A smart hill-climbing algorithm for application server configuration The overwhelming success of the Web as a mechanism for facilitating information retrieval and for conducting business transactions has ledto an increase in the deployment of complex enterprise applications . These applications typically run on Web Application Servers , which assume the burden of managing many tasks , such as concurrency , memory management , database access , etc. , required by these applications . The performance of an Application Server depends heavily on appropriate configuration . Configuration is a difficult and error-prone task dueto the large number of configuration parameters and complex interactions between them . We formulate the problem of finding an optimal configuration for a given application as a black-box optimization problem . We propose a smart hill-climbing algorithm using ideas of importance sampling and Latin Hypercube Sampling ( LHS ) . The algorithm is efficient in both searching and random sampling . It consists of estimating a local function , and then , hill-climbing in the steepest descent direction . The algorithm also learns from past searches and restarts in a smart and selective fashion using the idea of importance sampling . We have carried out extensive experiments with an on-line brokerage application running in a WebSphere environment . Empirical results demonstrate that our algorithm is more efficient than and superior to traditional heuristic methods ."
"472","Post-processing inkml for random-access navigation of voluminous handwritten ink documents The goal of this research is the improvement of browsing voluminous InkML data in two areas : ease of rendering continuous ink-flow for replay-browsing , and ease of random access navigation in eLearning domains . The notion of real-time random access navigation in ink documents has not yet been fully exploited . Users of existing eLearning browsers are restricted to viewing static annotated slides that are inferior in quality when compared to actively replaying the same slides with sequenced ink-flow of the annotated freehand writings . We are developing a tool to investigate ways of managing massive InkML data for efficient `` active visible scrolling '' of recorded freehand writings in ink documents . This work will also develop and evaluate new post-processing techniques that take advantage of the relationship between ink volumes and active-rendering times for real-time random access navigation ."
"473","TCOZ approach to semantic web services design Complex Semantic Web ( SW ) services may have intricate data state , autonomous process behavior and concurrent interactions . The design of such SW service systems requires precise and powerful modelling techniques to capture not only the ontology domain properties but also the services ' process behavior and functionalities . In this paper we apply an integrated formal modeling language , Timed Communicating Object Z ( TCOZ ) , to design SW services . Furthermore , the paper presents the development of the systematic translation rules and tools which can automatically extract the SW ontology and services semantic markup from the formal TCOZ design model ."
"474","Analyzing client interactivity in streaming media This paper provides an extensive analysis of pre-stored streaming media workloads , focusing on the client interactive behavior . We analyze four workloads that fall into three different domains , namely , education , entertainment video and entertainment audio . Our main goals are : ( a ) to identify qualitative similarities and differences in the typical client behavior for the three workload classes and ( b ) to provide data for generating realistic synthetic workloads ."
"475","Lessons from a Gnutella-web gateway We present a gateway between the WWW and the Gnutella peer-to-peer network that permits searchers on one side to be able to search and retrieve files on the other side of the gateway . This work improvesthe accessibility of files across different delivery platforms , making it possible to use a single search modality . We outline our design and implementation , present access statistics from a test deployment and discuss lessons learned ."
"476","Structuring and presenting annotated media repositories We generate hypermedia presentations from annotated media repositories using simple document structure as an intermediate phase . This poster applies Web style technologies to this process . Results include style specification for accessing semantically annotated media repositories , for determining document structure from semantic structure and for applying this document structure to the final presentation ."
"477","Towards the self-annotating web The success of the Semantic Web depends on the availability of ontologies as well as on the proliferation of web pages annotated with metadata conforming to these ontologies . Thus , a crucial question is where to acquire these metadata from . In this paper wepropose PANKOW ( Pattern-based Annotation through Knowledge on theWeb ) , a method which employs an unsupervised , pattern-based approach to categorize instances with regard to an ontology . The approach is evaluated against the manual annotations of two human subjects . The approach is implemented in OntoMat , an annotation tool for the Semantic Web and shows very promising results ."
"478","Price modeling in standards for electronic product catalogs based on XML The fast spreading of electronic business-to-business procurement systems has led to the development of new standards for the exchange of electronic product catalogs ( e-catalogs ) . E-catalogs contain various information about products , essential is price information . Prices are used for buying decisions and following order transactions . While simple price models are often sufficient for the description of indirect goods ( e.g. office supplies ) , other goods and lines of business make higher demands . In this paper we examine what price information is contained in commercial XML standards for the exchange of product catalog data . For that purpose we bring the different implicit price models of the examined catalog standards together and provide a generalized model ."
"479","Aliasing on the world wide web : prevalence and performance implications Aliasing occurs in Web transactions when requests containing different URLs elicit replies containing identical data payloads . Conventional caches associate stored data with URLs and can therefore suffer redundant payload transfers due to aliasing and other causes . Existing research literature , however , says little about the prevalence of aliasing in user-initiated transactions , or about redundant payload transfers in conventional Web cache hierarchies . This paper quantifies the extent of aliasing and the performance impact of URL-indexed cache management using a large client trace from WebTV Networks . Fewer than 5 % of reply payloads are aliased ( referenced via multiple URLs ) but over 54 % of successful transactions involve aliased payloads . Aliased payloads account for under 3.1 % of the trace 's `` working set size '' ( sum of payload sizes ) but over 36 % of bytes transferred . For the WebTV workload , roughly 10 % of payload transfers to browser caches and 23 % of payload transfers to a shared proxy are redundant , assuming infinite-capacity conventional caches . Our analysis of a large proxy trace from Compaq Corporation yields similar results.URL-indexed caching does not entirely explain the large number of redundant proxy-to-browser payload transfers previously reported in the WebTV system . We consider other possible causes of redundant transfers ( e.g. , reply metadata and browser cache management policies ) and discuss a simple hop-by-hop protocol extension that completely eliminates all redundant transfers , regardless of cause ."
"480","Value-based web caching Despite traditional web caching techniques , redundant data is often transferred over HTTP links . These redundant transfers result from both resource modification and aliasing . Resource modification causes the data represented by a single URI to change ; often , in transferring the new data , some old data is retransmitted . Aliasing , in contrast , occurs when the same data is named by multiple URIs , often in the context of dynamic or advertising content . Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing . Despite traditional web caching techniques , redundant data is often transferred over HTTP links . These redundant transfers result from both resource modification and aliasing . Resource modification causes the data represented by a single URI to change ; often , in transferring the new data , some old data is retransmitted . Aliasing , in contrast , occurs when the same data is named by multiple URIs , often in the context of dynamic or advertising content . Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing ."
"481","Jena : implementing the semantic web recommendations The new Semantic Web recommendations for RDF , RDFS and OWL have , at their heart , the RDF graph . Jena2 , a second-generation RDF toolkit , is similarly centered on the RDF graph . RDFS and OWL reasoning are seen as graph-to-graph transforms , producing graphs of virtual triples . Rich APIs are provided . The Model API includes support for other aspects of the RDF recommendations , such as containers and reification . The Ontology API includes support for RDFS and OWL , including advanced OWL Full support . Jena includes the de facto reference RDF\/XML parser , and provides RDF\/XML output using the full range of the rich RDF\/XML grammar . N3 I\/O is supported . RDF graphs can be stored in-memory or in databases . Jena 's query language , RDQL , and the Web API are both offered for the next round of standardization ."
"482","Unparsing RDF\/XML It is difficult to serialize an RDF graph as a humanly readable RDF\/XML document . This paper describes the approach taken in Jena 1.2 , in which a design pattern of guarded procedures invoked using top down recursive descent is used . Each procedure corresponds to a grammar rule ; the guard makes the choice about the applicability of the production . This approach is seen to correspond closely to the design of an LL ( k ) parser , and a theoretical justification of this correspondence is found in universal algebra ."
"483","Personalization in distributed e-learning environments Personalized support for learners becomes even more important , when e-Learning takes place in open and dynamic learning and information networks . This paper shows how to realize personalized learning support in distributed learning environments based on Semantic Web technologies . Our approach fills the existing gap between current adaptive educational systems with well-established personalization functionality , and open , dynamic learning repository networks . We propose a service-based architecture for establishing personalized e-Learning , where personalization functionality is provided by various web-services . A Personal Learning Assistant integrates personalization services and other supporting services , and provides the personalized access to learning resources in an e-Learning network ."
"484","Mining the peanut gallery : opinion extraction and semantic classification of product reviews The web contains a wealth of product reviews , but sifting through them is a daunting task . Ideally , an opinion mining tool would process a set of search results for a given item , generating a list of product attributes ( quality , features , etc. ) and aggregating opinions about each of them ( poor , mixed , good ) . We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews . Our classifier draws on information retrieval techniques for feature extraction and scoring , and the results for various metrics and heuristics vary depending on the testing situation . The best methods work as well as or better than traditional machine learning . When operating on individual sentences collected from web searches , performance is limited due to noise and ambiguity . But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes , the results are qualitatively quite useful ."
"485","A convenient method for securely managing passwords Computer users are asked to generate , keep secret , and recall an increasing number of passwords for uses including host accounts , email servers , e-commerce sites , and online financial services . Unfortunately , the password entropy that users can comfortably memorize seems insufficient to store unique , secure passwords for all these accounts , and it is likely to remain constant as the number of passwords ( and the adversary 's computational power ) increases into the future . In this paper , we propose a technique that uses a strengthened cryptographic hash function to compute secure passwords for arbitrarily many accounts while requiring the user to memorize only a single short password . This mechanism functions entirely on the client ; no server-side changes are needed . Unlike previous approaches , our design is both highly resistant to brute force attacks and nearly stateless , allowing users to retrieve their passwords from any location so long as they can execute our program and remember a short secret . This combination of security and convenience will , we believe , entice users to adopt our scheme . We discuss the construction of our algorithm in detail , compare its strengths and weaknesses to those of related approaches , and present Password Multiplier , an implementation in the form of an extension to the Mozilla Firefox web browser ."
"486","XQuery containment in presence of variable binding dependencies Semantic caching is an important technology for improving the response time of future user queries specified over remote servers . This paper deals with the fundamental query containment problem in an XQuery-based semantic caching system . To our best knowledge , the impact of subtle differences in XQuery semantics caused by different ways of specifying variables on query containment has not yet been studied . We introduce the concept of variable binding dependencies for representing the hierarchical element dependencies preserved by an XQuery . We analyze the problem of XQuery containment in the presence of such dependencies . We propose a containment mapping technique for nested XQuery in presence of variable binding dependencies . The implication of the nested block structure on XQuery containment is also considered . We mention the performance gains achieved by a semantic caching system we build based on the proposed technique ."
"487","A method for modeling uncertainty in semantic web taxonomies We present a method for representing and reasoning with uncertainty in RDF ( S ) and OWL ontologies based on Bayesian networks ."
"488","The indexable web is more than 11.5 billion pages In this short paper we estimate the size of the public indexable web at 11.5 billion pages . We also estimate the overlap and the index size of Google , MSN , Ask\/Teoma and Yahoo !"
"489","DiTaBBu : automating the production of time-based hypermedia content We present DiTaBBu , Digital Talking Books Builder , a framework for automatic production of time-based hypermedia for the Web , focusing on the Digital Talking Books domain . Delivering Digital Talking Books collections to a wide range of users is an expensive task , as it must take into account each user profile 's different needs , therefore authoring should be dismissed in favor of automation . With DiTaBBu , we enable automated content delivery in several playback platforms , targeted to specific user needs , featuring powerful navigation capabilities over the content . DiTaBBu can also be used as testbed for prototyping novel capabilities , through its flexible extension mechanisms ."
"490","Accessibility : a Web engineering approach Currently , the vast majority of web sites do not support accessibility for visually impaired users . Usually , these users have to rely on screen readers : applications that sequentially read the content of a web page in audio . Unfortunately , screen readers are not able to detect the meaning of the different page objects , and thus the implicit semantic knowledge conveyed in the presentation of the page is lost . One approach described in literature to tackle this problem , is the Dante approach , which allows semantic annotation of web pages to provide screen readers with extra ( semantic ) knowledge to better facilitate the audio presentation of a web page . Until now , such annotations were done manually , and failed for dynamic pages . In this paper , we combine the Dante approach with a web design method , WSDM , to fully automate the generation of the semantic annotation for visually impaired users . To do so , the semantic knowledge gathered during the design process is exploited , and the annotations are generated as a by-product of the design process , requiring no extra effort from the designer ."
"491","Learning domain ontologies for Web service descriptions : an experiment in bioinformatics The reasoning tasks that can be performed with semantic web service descriptions depend on the quality of the domain ontologies used to create these descriptions . However , building such domain ontologies is a time consuming and difficult task . We describe an automatic extraction method that learns domain ontologies for web service descriptions from textual documentations attached to web services . We conducted our experiments in the field of bioinformatics by learning an ontology from the documentation of the web services used in myGrid , a project that supports biology experiments on the Grid . Based on the evaluation of the extracted ontology in the context of the project , we conclude that the proposed extraction method is a helpful tool to support the process of building domain ontologies for web service descriptions ."
"492","Securing web application code by static analysis and runtime protection Security remains a major roadblock to universal acceptance of the Web for many kinds of transactions , especially since the recent sharp increase in remotely exploitable vulnerabilities have been attributed to Web application bugs . Many verification tools are discovering previously unknown vulnerabilities in legacy C programs , raising hopes that the same success can be achieved with Web applications . In this paper , we describe a sound and holistic approach to ensuring Web application security . Viewing Web application vulnerabilities as a secure information flow problem , we created a lattice-based static analysis algorithm derived from type systems and typestate , and addressed its soundness . During the analysis , sections of code considered vulnerable are instrumented with runtime guards , thus securing Web applications in the absence of user intervention . With sufficient annotations , runtime overhead can be reduced to zero . We also created a tool named . WebSSARI ( Web application Security by Static Analysis and Runtime Inspection ) to test our algorithm , and used it to verify 230 open-source Web application projects on SourceForge.net , which were selected to represent projects of different maturity , popularity , and scale . 69 contained vulnerabilities . After notifying the developers , 38 acknowledged our findings and stated their plans to provide patches . Our statistics also show that static analysis reduced potential runtime overhead by 98.4 % ."
"493","Staging transformations for multimodal web interaction management Multimodal interfaces are becoming increasingly ubiquitous with the advent of mobile devices , accessibility considerations , and novel software technologies that combine diverse interaction media . In addition to improving access and delivery capabilities , such interfaces enable flexible and personalized dialogs with websites , much like a conversation between humans . In this paper , we present a software framework for multimodal web interaction management that supports mixed-initiative dialogs between users and websites . A mixed-initiative dialog is one where the user and the website take turns changing the flow of interaction . The framework supports the functional specification and realization of such dialogs using staging transformations -- a theory for representing and reasoning about dialogs based on partial input . It supports multiple interaction interfaces , and offers sessioning , caching , and co-ordination functions through the use of an interaction manager . Two case studies are presented to illustrate the promise of this approach ."
"494","Dynamic coordination of information management services for processing dynamic web content Dynamic Web content provides us with time-sensitive and continuously changing data . To glean up-to-date information , users need to regularly browse , collect and analyze this Web content . Without proper tool support this information management task is tedious , time-consuming and error prone , especially when the quantity of the dynamic Web content is large , when many information management services are needed to analyze it , and when underlying services\/network are not completely reliable . This paper describes a multi-level , lifecycle ( design-time and run-time ) coordination mechanism that enables rapid , efficient development and execution of information management applications that are especially useful for processing dynamic Web content . Such a coordination mechanism brings dynamism to coordinating independent , distributed information management services . Dynamic parallelism spawns\/merges multiple execution service branches based on available data , and dynamic run-time reconfiguration coordinates service execution to overcome faulty services and bottlenecks . These features enable information management applications to be more efficient in handling content and format changes in Web resources , and enable the applications to be evolved and adapted to process dynamic Web content ."
"495","To randomize or not to randomize : space optimal summaries for hyperlink analysis Personalized PageRank expresses link-based page quality around user selected pages . The only previous personalized PageRank algorithm that can serve on-line queries for an unrestricted choice of pages on large graphs is our Monte Carlo algorithm ( WAW 2004 ) . In this paper we achieve unrestricted personalization by combining rounding and randomized sketching techniques in the dynamic programming algorithm of Jeh and Widom ( WWW 2003 ) . We evaluate the precision of approximation experimentally on large scale real-world data and find significant improvement over previous results . As a key theoretical contribution we show that our algorithms use an optimal amount of space by also improving earlier asymptotic worst-case lower bounds . Our lower bounds and algorithms apply to the SimRank as well ; of independent interest is the reduction of the SimRank computation to personalized PageRank ."
"496","An XPath-based preference language for P3P The Platform for Privacy Preferences ( P3P ) is the most significant effort currently underway to enable web users to gain control over their private information . The designers of P3P simultaneously designed a preference language called APPEL to allow users to express their privacy preferences , thus enabling automatic matching of privacy preferences against P3P policies . Unfortunately subtle interactions between P3P and APPEL result in serious problems when using APPEL : Users can only directly specify what is unacceptable in a policy , not what is acceptable ; simple preferences are hard to express ; and writing APPEL preferences is error prone . We show that these problems follow from a fundamental design choice made by APPEL , and can not be solved without completely redesigning the language . Therefore we explore alternatives to APPEL that can overcome these problems . In particular , we show that XPath serves quite nicely as a preference language and solves all the above problems . We identify the minimal subset of XPath that is needed , thus allowing matching programs to potentially use a smaller memory footprint . We also give an APPEL to XPath translator that shows that XPath is as expressive as APPEL ."
"497","XJ : facilitating XML processing in Java The increased importance of XML as a data representation format has led to several proposals for facilitating the development of applications that operate on XML data . These proposals range from runtime API-based interfaces to XML-based programming languages . The subject of this paper is XJ , a research language that proposes novel mechanisms for the integration of XML as a first-class construct into Java ™ . The design goals of XJ distinguish it from past work on integrating XML support into programming languages -- specifically , the XJ design adheres to the XML Schema and XPath standards . Moreover , it supports in-place updates of XML data thereby keeping with the imperative nature of Java . We have built a prototype compiler for XJ , and our preliminary experiments demonstrate that the performance of XJ programs can approach that of traditional low-level API-based interfaces , while providing a higher level of abstraction ."
"498","An adaptive , fast , and safe XML parser based on byte sequences memorization XML ( Extensible Markup Language ) processing can incur significant runtime overhead in XML-based infrastructural middleware such as Web service application servers . This paper proposes a novel mechanism for efficiently processing similar XML documents . Given a new XML document as a byte sequence , the XML parser proposed in this paper normally avoids syntactic analysis but simply matches the document with previously processed ones , reusing those results . Our parser is adaptive since it partially parses and then remembers XML document fragments that it has not met before . Moreover , it processes safely since its partial parsing correctly checks the well-formedness of documents . Our implementation of the proposed parser complies with the JSR 63 standard of the Java API for XML Processing ( JAXP ) 1.1 specification . We evaluated Deltarser performance with messages using Google Web services . Comparing to Piccolo ( and Apache Xerces ) , it effectively parses 35 % ( 106 % ) faster in a server-side use-case scenario , and 73 % ( 126 % ) faster in a client-side use-case scenario ."
"499","Topic-sensitive PageRank In the original PageRank algorithm for improving the ranking of search-query results , a single PageRank vector is computed , using the link structure of the Web , to capture the relative `` importance '' of Web pages , independent of any particular search query . To yield more accurate search results , we propose computing a set of PageRank vectors , biased using a set of representative topics , to capture more accurately the notion of importance with respect to a particular topic . By using these ( precomputed ) biased PageRank vectors to generate query-specific importance scores for pages at query time , we show that we can generate more accurate rankings than with a single , generic PageRank vector . For ordinary keyword search queries , we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords . For searches done in context ( e.g. , when the search query is performed by highlighting words in a Web page ) , we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared ."
"500","Web taxonomy integration using support vector machines We address the problem of integrating objects from a source taxonomy into a master taxonomy . This problem is not only currently pervasive on the web , but also important to the emerging semantic web . A straightforward approach to automating this process would be to train a classifier for each category in the master taxonomy , and then classify objects from the source taxonomy into these categories . In this paper we attempt to use a powerful classification method , Support Vector Machine ( SVM ) , to attack this problem . Our key insight is that the availability of the source taxonomy data could be helpful to build better classifiers in this scenario , therefore it would be beneficial to do transductive learning rather than inductive learning , i.e. , learning to optimize classification performance on a particular set of test examples . Noticing that the categorizations of the master and source taxonomies often have some semantic overlap , we propose a method , Cluster Shrinkage ( CS ) , to further enhance the classification by exploiting such implicit knowledge . Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration ."
"501","Determining user interests about museum collections Currently , there is an increasing effort to provide various personalized services on museum web sites . This paper presents an approach for determining user interests in a museum collection with the help of an interactive dialog . It uses a semantically annotated collection of the Rijksmuseum Amsterdam to elicit specific user 's interests in artists , periods , genres and themes and uses these values to recommend relevant artefacts and related concepts from the museum collection . In the presented prototype , we show how constructing a user profile and applying recommender strategies in this way enable dynamical generation personalized museum tours for different users ."
"502","An incremental XSLT transformation processor for XML document manipulation In this paper , we present an incremental transformation framework called incXSLT . This framework has been experimented for the XSLT language defined at the World Wide Web Consortium . For the currently available tools , designing the XML content and the transformation sheets is an inefficient , a tedious and an error prone experience . Incremental transformation processors such as incXSLT represent a better alternative to help in the design of both the content and the transformation sheets . We believe that such frameworks are a first step toward fully interactive transformation-based authoring environments ."
"503","Parsing owl dl : trees or triples ? The Web Ontology Language ( OWL ) defines three classes of documents : Lite , DL , and Full . All RDF\/XML documents are OWL Full documents , some OWL Full documents are also OWL DL documents , and some OWL DL documents are also OWL Lite documents . This paper discusses parsing and species recognition -- that is the process of determining whether a given document falls into the OWL Lite , DL or Full class . Wedescribe two alternative approaches to this task , one based on abstract syntax trees , the other on RDF triples , and compare their key characteristics ."
"504","An enhanced model for searching in semantic portals Semantic Portal is the next generation of web portals that are powered by Semantic Web technologies for improved information sharing and exchange for a community of users . Current methods of searching in Semantic Portals are limited to keyword-based search using information retrieval ( IR ) techniques , ontology-based formal query and reasoning , or a simple combination of the two . In this paper , we propose an enhanced model that tightly integrates IR with formal query and reasoning to fully utilize both textual and semantic information for searching in Semantic Portals . The model extends the search capabilities of existing methods and can answer more complex search requests . The ideas in a fuzzy description logic ( DL ) IR model and a formal DL query method are employed and combined in our model . Based on the model , a semantic search service is implemented and evaluated . The evaluation shows very large improvements over existing methods ."
"505","A service creation environment based on end to end composition of Web services The demand for quickly delivering new applications is increasingly becoming a business imperative today . Application development is often done in an ad hoc manner , without standard frameworks or libraries , thus resulting in poor reuse of software assets . Web services have received much interest in industry due to their potential in facilitating seamless business-to-business or enterprise application integration . A web services composition tool can help automate the process , from creating business process functionality , to developing executable workflows , to deploying them on an execution environment . However , we find that the main approaches taken thus far to standardize and compose web services are piecemeal and insufficient . The business world has adopted a ( distributed ) programming approach in which web service instances are described using WSDL , composed into flows with a language like BPEL and invoked with the SOAP protocol . Academia has propounded the AI approach of formally representing web service capabilities in ontologies , and reasoning about their composition using goal-oriented inferencing techniques from planning . We present the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the above approaches . We describe a prototype service creation environment along with a use-case scenario , and demonstrate how it can significantly speed up the time-to-market for new services ."
"506","Authoring and annotation of web pages in CREAM Richly interlinked , machine-understandable data constitute the basis for the Semantic Web . We provide a framework , CREAM , that allows for creation of metadata . While the annotation mode of CREAM allows to create metadata for existing web pages , the authoring mode lets authors create metadata -- almost for free -- while putting together the content of a page . As a particularity of our framework , CREAM allows to create relational metadata , i.e. metadata that instantiate interrelated definitions of classes in a domain ontology rather than a comparatively rigid template-like schema asm Dublin Core . We discuss some of the requirements one has to meet when developing such an ontology-based framework , e.g. the integration of a metadata crawler , inference services , document management and a meta-ontology , and describe its implementation , viz . Ont-O-Mat , a component-based , ontology-driven Web page authoring and annotation tool ."
"507","Template detection via data mining and its applications We formulate and propose the template detection problem , and suggest a practical solution for it based on counting frequent item sets . We show that the use of templates is pervasive on the web . We describe three principles , which characterize the assumptions made by hypertext information retrieval ( IR ) and data mining ( DM ) systems , and show that templates are a major source of violation of these principles . As a consequence , basic `` pure '' implementations of simple search algorithms coupled with template detection and elimination show surprising increases in precision at all levels of recall ."
"508","Towards a multimedia formatting vocabulary Time-based , media-centric Web presentations can be described declaratively in the XML world through the development of languages such as SMIL . It is difficult , however , to fully integrate them in a complete document transformation processing chain . In order to achieve the desired processing of data-driven , time-based , media-centric presentations , the text-flow based formatting vocabularies used by style languages such as XSL , CSS and DSSSL need to be extended . The paper presents a selection of use cases which are used to derive a list of requirements for a multimedia style and transformation formatting vocabulary . The boundaries of applicability of existing text-based formatting models for media-centric transformations are analyzed . The paper then discusses the advantages and disadvantages of a fully-fledged time-based multimedia formatting model . Finally , the discussion is illustrated by describing the key properties of the example multimedia formatting vocabulary currently implemented in the back-end of our Cuypers multimedia transformation engine ."
"509","Semantic search Activities such as Web Services and the Semantic Web are working to create a web of distributed machine understandable data . In this paper we present an application called ` Semantic Search ' which is built on these supporting technologies and is designed to improve traditional web searching . We provide an overview of TAP , the application framework upon which the Semantic Search is built . We describe two implemented Semantic Search systems which , based on the denotation of the search query , augment traditional search results with relevant data aggregated from distributed sources . We also discuss some general issues related to searching and the Semantic Web and outline how an understanding of the semantics of the search terms can be used to provide better results ."
"510","Mining newsgroups using networks arising from social behavior Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text . We investigate the feasibility of applying link-based methods in new applications domains . The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups . A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author . This social behavior gives rise to a network in which the vertices are individuals and the links represent `` responded-to '' relationships . An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree . This behavior is in sharp contrast to the WWW link graph , where linkage is an indicator of agreement or common interest . By analyzing the graph structure of the responses , we are able to effectively classify people into opposite camps . In contrast , methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical , and many newsgroup postings consist of relatively few words of text ."
"511","A graphical user interface toolkit approach to thin-client computing Network and server-centric computing paradigms are quickly returning to being the dominant methods by which we use computers . Web applications are so prevalent that the role of a PC today has been largely reduced to a terminal for running a client or viewer such as a Web browser . Implementers of network-centric applications typically rely on the limited capabilities of HTML , employing proprietary `` plug ins '' or transmitting the binary image of an entire application that will be executed on the client . Alternatively , implementers can develop without regard for remote use , requiring users who wish to run such applications on a remote server to rely on a system that creates a virtual frame buffer on the server , and transmits a copy of its raster image to the local client . We review some of the problems that these current approaches pose , and show how they can be solved by developing a distributed user interface toolkit . A distributed user interface toolkit applies techniques to the high level components of a toolkit that are similar to those used at a low level in the X Window System . As an example of this approach , we present RemoteJFC , a working distributed user interface toolkit that makes it possible to develop thin-client applications using a distributed version of the Java Foundation Classes ."
"512","Static approximation of dynamically generated Web pages Server-side programming is one of the key technologies that support today 's WWW environment . It makes it possible to generate Web pages dynamically according to a user 's request and to customize pages for each user . However , the flexibility obtained by server-side programming makes it much harder to guarantee validity and security of dynamically generated pages . To check statically the properties of Web pages generated dynamically by a server-side program , we develop a static program analysis that approximates the string output of a program with a context-free grammar . The approximation obtained by the analyzer can be used to check various properties of a server-side program and the pages it generates . To demonstrate the effectiveness of the analysis , we have implemented a string analyzer for the server-side scripting language PHP . The analyzer is successfully applied to publicly available PHP programs to detect cross-site scripting vulnerabilities and to validate pages they generate dynamically ."
"513","Analysis of topic dynamics in web search We report on a study of topic dynamics for pages visited by a sample of people using MSN Search . We examine the predictive accuracies of probabilistic models of topic transitions for individuals and groups of users . We explore temporal dynamics by comparing the accuracy of the models for predicting topic transitions at increasingly distant times in the future . Finally , we discuss directions for applying models of search topic dynamics ."
"514","Globetp : template-based database replication for scalable web applications Generic database replication algorithms do not scale linearly in throughput as all update , deletion and insertion ( UDI ) queries must be applied to every database replica . The throughput is therefore limited to the point where the number of UDI queries alone is sufficient to overload one server . In such scenarios , partial replication of a database can help , as UDI queries are executed only by a subset of all servers . In this paper we propose GlobeTP , a system that employs partial replication to improve database throughput . GlobeTP exploits the fact that a Web application 's query workload is composed of a small set of read and write templates . Using knowledge of these templates and their respective execution costs , GlobeTP provides database table placements that produce significant improvements in database throughput . We demonstrate the efficiency of this technique using two different industry standard benchmarks . In our experiments , GlobeTP increases the throughput by 57 % to 150 % compared to full replication , while using identical hardware configuration . Furthermore , adding a single query cache improves the throughput by another 30 % to 60 % ."
"515","Unsupervised learning of soft patterns for generating definitions from online news Breaking news often contains timely definitions and descriptions of current terms , organizations and personalities . We utilize such web sources to construct definitions for such terms . Previous work has identified definitions using hand-crafted rules or supervised learning that constructs rigid , hard text patterns . In contrast , we demonstrate a new approach that uses flexible , soft matching patterns to characterize definition sentences . Our soft patterns are able to effectively accommodate the diversity of definition sentence structure exhibited in news . We use pseudo-relevance feedback to automatically label sentences for use in soft pattern generation . The application of our unsupervised method significantly improves baseline systems on both the standardized TREC corpus as well as crawled online news articles by 27 % and 30 % , respectively , in terms of F measure . When applied to a state-of-art definition generation system recently fielded in the TREC 2003 definitional question answering task , it improves the performance by 14 % ."
"516","Position paper : ontology construction from online ontologies One of the main hurdles towards a wide endorsement of ontologies is the high cost of constructing them . Reuse of existing ontologies offers a much cheaper alternative than building new ones from scratch , yet tools to support such reuse are still in their infancy . However , more ontologies are becoming available on the web , and online libraries for storing and indexing ontologies are increasing in number and demand . Search engines have also started to appear , to facilitate search and retrieval of online ontologies . This paper presents a fresh view on constructing ontologies automatically , by identifying , ranking , and merging fragments of online ontologies ."
"517","Automatic web news extraction using tree edit distance The Web poses itself as the largest data repository ever available in the history of humankind . Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data . Although several techniques have been developed to the problem of Web data extraction , their use is still not spread , mostly because of the need for high human intervention and the low quality of the extraction results . In this paper , we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites . Our approach is based on a highly efficient tree structure analysis that produces very effective results . We have tested our approach with several important Brazilian on-line news sites and achieved very precise results , correctly extracting 87.71 % of the news in a set of 4088 pages distributed among 35 different sites ."
"518","Anti-aliasing on the web It is increasingly common for users to interact with the web using a number of different aliases . This trend is a double-edged sword . On one hand , it is a fundamental building block in approaches to online privacy . On the other hand , there are economic and social consequences to allowing each user an arbitrary number of free aliases . Thus , there is great interest in understanding the fundamental issues in obscuring the identities behind aliases . However , most work in the area has focused on linking aliases through analysis of lower-level properties of interactions such as network routes . We show that aliases that actively post text on the web can be linked together through analysis of that text . We study a large number of users posting on bulletin boards , and develop algorithms to anti-alias those users : we can with a high degree of success identify when two aliases belong to the same individual . Our results show that such techniques are surprisingly effective , leading us to conclude that guaranteeing privacy among aliases that post actively requires mechanisms that do not yet exist ."
"519","Web service interfaces We present a language for specifying web service interfaces . A web service interface puts three kinds of constraints on the users of the service . First , the interface specifies the methods that can be called by a client , together with types of input and output parameters ; these are called signature constraints . Second , the interface may specify propositional constraints on method calls and output values that may occur in a web service conversation ; these are called consistency constraints . Third , the interface may specify temporal constraints on the ordering of method calls ; these are called protocol constraints . The interfaces can be used to check , first , if two or more web services are compatible , and second , if a web service A can be safely substituted for a web service B. The algorithm for compatibility checking verifies that two or more interfaces fulfill each others ' constraints . The algorithm for substitutivity checking verifies that service A demands fewer and fulfills more constraints than service B."
"520","Incremental maintenance for materialized XPath\/XSLT views This paper proposes an incremental maintenance algorithm that efficiently updates the materialized XPath\/XSLT views defined using XPath expressions in XP ( ( ) , \* , \/ \/ , vars ) . The algorithm consists of two processes . 1 ) The dynamic execution flow of an XSLT program is stored as an XT ( XML Transformation ) tree during the full transformation . 2 ) In response to a source XML data update , the impacted portions of the XT-tree are identified and maintained by partially re-evaluating the XSLT program . This paper discusses the XPath\/XSLT features of incremental view maintenance for subtree insertion\/deletion and applies them to the maintenance algorithm . Experiments show that the incremental maintenance algorithm outperforms full XML transformation algorithms by factors of up to 500 ."
"521","Web object retrieval The primary function of current Web search engines is essentially relevance ranking at the document level . However , myriad structured information about real-world objects is embedded in static Web pages and online Web databases . Document-level information retrieval can unfortunately lead to highly inaccurate relevance ranking in answering object-oriented queries . In this paper , we propose a paradigm shift to enable searching at the object level . In traditional information retrieval models , documents are taken as the retrieval units and the content of a document is considered reliable . However , this reliability assumption is no longer valid in the object retrieval context when multiple copies of information about the same object typically exist . These copies may be inconsistent because of diversity of Web site qualities and the limited performance of current information extraction techniques . If we simply combine the noisy and inaccurate attribute information extracted from different sources , we may not be able to achieve satisfactory retrieval performance . In this paper , we propose several language models for Web object retrieval , namely an unstructured object retrieval model , a structured object retrieval model , and a hybrid model with both structured and unstructured retrieval features . We test these models on a paper search engine and compare their performances . We conclude that the hybrid model is the superior by taking into account the extraction errors at varying levels ."
"522","Semantic web applications to e-science in silico experiments This paper explains our research and implementations of manual , automatic and deep annotations of provenance logs for e-Science insilico experiments . Compared to annotating general Web documents , annotations for scientific data require more sophisticated professional knowledge to recognize concepts from documents , and more complex text extraction and mapping mechanisms . A simple automatic annotation approach based on `` lexicons '' and a deep annotation implemented by semantically populating , translating and annotating provenance logs are introduced in this paper . We used COHSE ( Conceptual Open Hypermedia Services Environment ) to annotate and browse provenance logs from my Grid project , which are conceptually linked together as a hypertext Web of provenance logs and experiment resources , based on the associated conceptual metadata and reasoning over these metadata ."
"523","Design for verification for asynchronously communicating Web services We present a design for verification approach to developing reliable web services . We focus on composite web services which consist of asynchronously communicating peers . Our goal is to automatically verify properties of interactions among such peers . We propose a design pattern that eases the development of such web services and enables a modular , assume-guarantee style verification strategy . In the proposed design pattern , each peer is associated with a behavioral interface description which specifies how that peer will interact with other peers . Using these peer interfaces we automatically generate BPEL specifications to publish for interoperability . Assuming that the participating peers behave according to their interfaces , we verify safety and liveness properties about the global behavior of the composite web service during behavior verification . During interface verification , we check that each peer implementation conforms to its interface . Using the modularity in the proposed design pattern , we are able to perform the interface verification of each peer and the behavior verification as separate steps . Our experiments show that , using this modular approach , one can automatically and efficiently verify web service implementations ."
"524","On the lack of typical behavior in the global Web traffic network We offer the first large-scale analysis of Web traffic based on network flow data . Using data collected on the Internet2 network , we constructed a weighted bipartite client-server host graph containing more than 18 x 106 vertices and 68 x 106 edges valued by relative traffic flows . When considered as a traffic map of the World-Wide Web , the generated graph provides valuable information on the statistical patterns that characterize the global information flow on the Web . Statistical analysis shows that client-server connections and traffic flows exhibit heavy-tailed probability distributions lacking any typical scale . In particular , the absence of an intrinsic average in some of the distributions implies the absence of a prototypical scale appropriate for server design , Web-centric network design , or traffic modeling . The inspection of the amount of traffic handled by clients and servers and their number of connections highlights non-trivial correlations between information flow and patterns of connectivity as well as the presence of anomalous statistical patterns related to the behavior of users on the Web . The results presented here may impact considerably the modeling , scalability analysis , and behavioral study of Web applications ."
"525","AwareDAV : a generic WebDAV notification framework and implementation WebDAV needs awareness support in order to be a full-fledged collaboration system , This paper introduces AwareDAV , a new WebDAV extension framework enabling shared awareness through event notification . By extending the WebDAV protocol with seven new request-methods and an extensible XML based event subscription scheme , AwareDAV supports fine grained event subscriptions over a range of transport mechanisms and enables a wide range of collaboration scenarios . This paper describes the design of AwareDAV , its API , experiences with its initial implementation , as well as a comparison with Microsoft Exchange and WebDAV-notify ."
"526","A community-aware search engine Current search technologies work in a `` one size fits all '' fashion . Therefore , the answer to a query is independent of specific user information need . In this paper we describe a novel ranking technique for personalized search servicesthat combines content-based and community-based evidences . The community-based information is used in order to provide context for queries andis influenced by the current interaction of the user with the service . Ouralgorithm is evaluated using data derived from an actual service available on the Web an online bookstore . We show that the quality of content-based ranking strategies can be improved by the use of communityinformation as another evidential source of relevance . In our experiments the improvements reach up to 48 % in terms of average precision ."
"527","The WT10G dataset and the evolution of the web The purpose of this paper is threefold . First , we study the evolution of the web based on data available from an earlier snapshot of the web and compare the results with those predicted in ( 2 ) . Secondly , we establish whether the WT10G dataset , a popular benchmark for the development and evaluation of internet based applications is appropriate for the tasks . Finally , is there a need for a collection of a new dataset for such purposes . The findings are that the appropriateness of using the popular WT10G dataset in recent Internet-based experiments is questionable and that there is a need for a new collection of dataset for development and evaluation purposes of algorithms related to Internet search engine developments ."
"528","Towards autonomic web-sites based on learning automata Autonomics or self-reorganization becomes pertinent for web-sites serving a large number of users with highly varying workloads . An important component of self-adaptation is to model the behavior of users and adapt accordingly . This paper proposes a learning-automata based technique for model discovery . User access patterns are used to construct an FSM model of user behavior that in turn is used for prediction and prefetching . The proposed technique uses a generalization algorithm to classify behavior patterns into a small number of generalized classes . It has been tested on both synthetic and live data-sets and has shown a prediction hit-rate of up to 89 % on a real web-site ."
"529","Predicting outcomes of web navigation Two exploratory studies examined the relationships among web navigation metrics , measures of lostness , and success on web navigation tasks . The web metrics were based on counts of visits to web pages , properties of the web usage graph , and similarity to an optimal path . Metrics based on similarity to an optimal path were good predictors of lostness and task success ."
"530","Improving text collection selection with coverage and overlap statistics In an environment of distributed text collections , the first step in the information retrieval process is to identify which of all available collections are more relevant to a given query and which should thus be accessed to answer the query . We address the challenge of collection selection when there is full or partial overlap between the available text collections , a scenario which has not been examined previously despite its real-world applications . To that end , we present COSCO , a collection selection approach which uses collection-specific coverage and overlap statistics . We describe our experimental results which show that the presented approach displays the desired behavior of retrieving more new results early on in the collection order , and performs consistently and significantly better than CORI , previously considered to be one of the best collection selection systems ."
"531","Answering order-based queries over XML data Order-based queries over XML data include XPath navigation axes such as following-sibling and following . In this paper , we present holistic algorithms that evaluate such order-based queries . An experimental comparison with previous approaches shows the performance benefits of our algorithms ."
"532","TruRank : taking PageRank to the limit PageRank is defined as the stationary state of a Markov chain depending on a damping factor α that spreads uniformly part of the rank . The choice of α is eminently empirical , and in most cases the original suggestion α = 0.85 by Brin and Page is still used . It is common belief that values of α closer to 1 give a `` truer to the web '' PageRank , but a small α accelerates convergence . Recently , however , it has been shown that when α = 1 all pages in the core component are very likely to have rank 0 ( 1 ) . This behavior makes it difficult to understand PageRank when α ≈ 1 , as it converges to a meaningless value for most pages . We propose a simple and natural modification to the standard preprocessing performed on the adjacency matrix of the graph , resulting in a ranking scheme we call TruRank . TruRank ranks the web with principles almost identical to PageRank , but it gives meaningful values also when α ☰ 1 ."
"533","Three-level caching for efficient query processing in large Web search engines Large web search engines have to answer thousands of queries per second with interactive response times . Due to the sizes of the data sets involved , often in the range of multiple terabytes , a single query may require the processing of hundreds of megabytes or more of index data . To keep up with this immense workload , large search engines employ clusters of hundreds or thousands of machines , and a number of techniques such as caching , index compression , and index and query pruning are used to improve scalability . In particular , two-level caching techniques cache results of repeated identical queries at the frontend , while index data for frequently used query terms are cached in each node at a lower level . We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains . This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists . We propose and study several offline and online algorithms for the resulting weighted caching problem , which turns out to be surprisingly rich in structure . Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes , both in isolation and in combination with the other caching levels . We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance ."
"534","Topic segmentation of message hierarchies for indexing and navigation support Message hierarchies in web discussion boards grow with new postings . Threads of messages evolve as new postings focus within or diverge from the original themes of the threads . Thus , just by investigating the subject headings or contents of earlier postings in a message thread , one may not be able to guess the contents of the later postings . The resulting navigation problem is further compounded for blind users who need the help of a screen reader program that can provide only a linear representation of the content . We see that , in order to overcome the navigation obstacle for blind as well as sighted users , it is essential to develop techniques that help identify how the content of a discussion board grows through generalizations and specializations of topics . This knowledge can be used in segmenting the content in coherent units and guiding the users through segments relevant to their navigational goals . Our experimental results showed that the segmentation algorithm described in this paper provides up to 80-85 % success rate in labeling messages . The algorithm is being deployed in a software system to reduce the navigational load of blind students in accessing web-based electronic course materials ; however , we note that the techniques are equally applicable for developing web indexing and summarization tools for users with sight ."
"535","Using OWL for querying an XML\/RDF syntax Some recent initiatives try to take profit from RDF to make XML documents interoperate at the semantic level . Ontologies are used to establish semantic connections among XML languages , and some mechanisms have been defined to query them with natural XML query languages like XPath and XML Query . Generally structure-mapping approaches define a simple translation between trivial XPath expressions and some RDF query language like RDQL ; however some XPath constructs can not be covered in a structure-mapping strategy . In contrast , our work takes the model-mapping approach , respectful with node order , that allows mapping all XPath axis . The obtained XPath implementation has the properties of schema-awareness and IDREF-awareness , so it can be used to exploit inheritance hierarchies defined in one or more XML schemas ."
"536","OWL DL vs. OWL flight : conceptual modeling and reasoning for the semantic Web The Semantic Web languages RDFS and OWL have been around for some time now . However , the presence of these languages has not brought the breakthrough of the Semantic Web the creators of the languages had hoped for . OWL has a number of problems in the area of interoperability and usability in the context of many practical application scenarios which impede the connection to the Software Engineering and Database communities . In this paper we present OWL Flight , which is loosely based on OWL , but the semantics is grounded in Logic Programming rather than Description Logics , and it borrows the constraint-based modeling style common in databases . This results in different types of modeling primitives and enforces a different style of ontology modeling . We analyze the modeling paradigms of OWL DL and OWL Flight , as well as reasoning tasks supported by both languages . We argue that different applications on the Semantic Web require different styles of modeling and thus both types of languages are required for the Semantic Web ."
"537","Exploiting the deep web with DynaBot : matching , probing , and ranking We present the design of Dynabot , a guided Deep Web discovery system . Dynabot 's modular architecture supports focused crawling of the Deep Web with an emphasis on matching , probing , and ranking discovered sources using two key components : service class descriptions and source-biased analysis . We describe the overall architecture of Dynabot and discuss how these components support effective exploitation of the massive Deep Web data available ."
"538","Disambiguating Web appearances of people in a social network Say you are looking for information about a particular person . A search engine returns many pages for that person 's name but which pages are about the person you care about , and which are about other people who happen to have the same name ? Furthermore , if we are looking for multiple people who are related in some way , how can we best leverage this social network ? This paper presents two unsupervised frameworks for solving this problem : one based on link structure of the Web pages , another using Agglomerative\/Conglomerative Double Clustering ( A\/CDC ) -- an application of a recently introduced multi-way distributional clustering method . To evaluate our methods , we collected and hand-labeled a dataset of over 1000 Web pages retrieved from Google queries on 12 personal names appearing together in someones in an email folder . On this dataset our methods outperform traditional agglomerative clustering by more than 20 % , achieving over 80 % F-measure ."
"539","An experimental study on large-scale web categorization Taxonomies of the Web typically have hundreds of thousands of categories and skewed category distribution over documents . It is not clear whether existing text classification technologies can perform well on and scale up to such large-scale applications . To understand this , we conducted the evaluation of several representative methods ( Support Vector Machines , k-Nearest Neighbor and Naive Bayes ) with Yahoo ! taxonomies . In particular , we evaluated the effectiveness\/efficiency tradeoff in classifiers with hierarchical setting compared to conventional ( flat ) setting , and tested popular threshold tuning strategies for their scalability and accuracy in large-scale classification problems ."
"540","Need for non-visual feedback with long response times in mobile HCI When browsing Web pages with a mobile device , the system response times are variable and much longer than on a PC . Users must repeatedly glance at the display to see when the page finally arrives , although mobility demands a Minimal Attention User Interface . We conducted a user study with 27 participants to discover the point at which visual feedback stops reaching the user in mobile context . In the study , we examined the deployment of attention during page loading to the phone vs. the environment in several different everyday mobility contexts , and compared these to the laboratory context . The first part of the page appeared on the screen typically in 11 seconds , but we found that the user 's visual attention shifted away from the mobile browser usually between 4 and 8 seconds in the mobile context . In contrast , the continuous span of attention to the browser was more than 14 seconds in the laboratory condition . Based on our study results , we recommend mobile applications provide multimodal feedback for delays of more than four seconds ."
"541","Modeling the author bias between two on-line computer science citation databases We examine the difference and similarities between two on-line computer science citation databases DBLP and CiteSeer . The database entries in DBLP are inserted manually while the CiteSeer entries are obtained autonomously . We show that the CiteSeer database contains considerably fewer single author papers . This bias can be modeled by an exponential process with intuitive explanation . The model permits us to predict that the DBLP database covers approximately 30 % of the entire literature of Computer Science ."
"542","Duplicate detection in click streams We consider the problem of finding duplicates in data streams . Duplicate detection in data streams is utilized in various applications including fraud detection . We develop a solution based on Bloom Filters ( 9 ) , and discuss the space and time requirements for running the proposed algorithm in both the contexts of sliding , and landmark stream windows . We run a comprehensive set of experiments , using both real and synthetic click streams , to evaluate the performance of the proposed solution . The results demonstrate that the proposed solution yields extremely low error rates ."
"543","The infocious web search engine : improving web searching through linguistic analysis In this paper we present the Infocious Web search engine ( 23 ) . Our goal in creating Infocious is to improve the way people find information on the Web by resolving ambiguities present in natural language text . This is achieved by performing linguistic analysis on the content of the Web pages we index , which is a departure from existing Web search engines that return results mainly based on keyword matching . This additional step of linguistic processing gives Infocious two main advantages . First , Infocious gains a deeper understanding of the content of Web pages so it can better match users ' queries with indexed documents and therefore can improve relevancy of the returned results . Second , based on its linguistic processing , Infocious can organize and present the results to the user in more intuitive ways . In this paper we present the linguistic processing technologies that we incorporated in Infocious and how they are applied in helping users find information on the Web more efficiently . We discuss the various components in the architecture of Infocious and how each of them benefits from the added linguistic processing . Finally , we experimentally evaluate the performance of a component which leverages linguistic information in order to categorize Web pages ."
"544","Identifying link farm spam pages With the increasing importance of search in guiding today 's web traffic , more and more effort has been spent to create search engine spam . Since link analysis is one of the most important factors in current commercial search engines ' ranking systems , new kinds of spam aiming at links have appeared . Building link farms is one technique that can deteriorate link-based ranking algorithms . In this paper , we present algorithms for detecting these link farms automatically by first generating a seed set based on the common link set between incoming and outgoing links of Web pages and then expanding it . Links between identified pages are re-weighted , providing a modified web graph to use in ranking page importance . Experimental results show that we can identify most link farm spam pages and the final ranking results are improved for almost all tested queries ."
"545","A multi-threaded PIPELINED Web server architecture for SMP\/SoC machines Design of high performance Web servers has become a recent research thrust to meet the increasing demand of network-based services . In this paper , we propose a new Web server architecture , called multi-threaded PIPELINED Web server , suitable for Symmetric Multi-Processor ( SMP ) or System-on-Chip ( SoC ) architectures . The proposed PIPELINED model consists of multiple thread pools , where each thread pool consists of five basic threads and two helper threads . The main advantages of the proposed model are global information sharing by the threads , minimal synchronization overhead due to less number of threads , and non-blocking I\/O operations , possible with the helper threads . We have conducted an in-depth performance analysis of the proposed server model along with four prior Web server models ( Multi-Process ( MP ) , Multi-Thread ( MT ) , Single-Process Event-Driven ( SPED ) and Asynchronous Multi-Process Event-Driven ( AMPED ) ) via simulation using six Web server workloads . The experiments are conducted to investigate the impact of various factors such as the memory size , disk speed and numbers of clients . The simulation results indicate that the proposed PIPELINED Web server architecture shows the best performance across all system and workload parameters compared to the MP , MT , SPED and AMPED models . Although the MT and AMPED models show competitive performance with less number of processors , the advantage of the PIPELINED model becomes obvious as the number of processors or clients in an SMP\/SoC machine increases . The MP model shows the worst performance in most of the cases . The results indicate that the proposed server architecture can be used in future large-scale SMP\/SoC machines to boost system performance ."
"546","Extracting context to improve accuracy for HTML content extraction Previous work on content extraction utilized various heuristics such as link to text ratio , prominence of tables , and identification of advertising . Many of these heuristics were associated with `` settings '' , whereby some heuristics could be turned on or off and others parameterized by minimum or maximum threshold values . A given collection of settings - such as removing table cells with high linked to non-linked text ratios and removing all apparent advertising -- might work very well for a news website , but leave little or no content left for the reader of a shopping site or a web portal We present a new technique , based on incrementally clustering websites using search engine snippets , to associate a newly requested website with a particular `` genre '' , and then employ settings previously determined to be appropriate for that genre , with dramatically improved content extraction results overall ."
"547","Extracting semantic structure of web documents using content and visual information This work aims to provide a page segmentation algorithm which uses both visual and content information to extract the semantic structure of a web page . The visual information is utilized using the VIPS algorithm and the content information using a pre-trained Naive Bayes classifier . The output of the algorithm is a semantic structure tree whose leaves represent segments having unique topic . However contents of the leaf segments may possibly be physically distributed in the web page . This structure can be useful in many web applications like information retrieval , information extraction and automatic web page adaptation . This algorithm is expected to outperform other existing page segmentation algorithms since it utilizes both content and visual information ."
"548","Executing incoherency bounded continuous queries at web data aggregators Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making . Typically a user desires to obtain the value of some function over distributed data items , for example , to determine when and whether ( a ) the traffic entering a highway from multiple feed roads will result in congestion in a thoroughfare or ( b ) the value of a stock portfolio exceeds a threshold . Using the standard Web infrastructure for these applications will increase the reach of the underlying information . But , since these queries involve data from multiple sources , with sources supporting standard HTTP ( pull-based ) interfaces , special query processing techniques are needed . Also , these applications often have the flexibility to tolerate some incoherency , i.e. , some differences between the results reported to the user and that produced from the virtual database made up of the distributed data sources . In this paper , we develop and evaluate client-pull-based techniques for refreshing data so that the results of the queries over distributed data can be correctly reported , conforming to the limited incoherency acceptable to the users . We model as well as estimate the dynamics of the data items using a probabilistic approach based on Markov Chains . Depending on the dynamics of data we adapt the data refresh times to deliver query results with the desired coherency . The commonality of data needs of multiple queries is exploited to further reduce refresh overheads . Effectiveness of our approach is demonstrated using live sources of dynamic data : the number of refreshes it requires is ( a ) an order of magnitude less than what we would need if every potential update is pulled from the sources , and ( b ) comparable to the number of messages needed by an ideal algorithm , one that knows how to optimally refresh the data from distributed data sources . Our evaluations also bring out a very practical and attractive tradeoff property of pull based approaches , e.g. , a small increase in tolerable incoherency leads to a large decrease in message overheads ."
"549","Browsing fatigue in handhelds : semantic bookmarking spells relief Focused Web browsing activities such as periodically looking up headline news , weather reports , etc. , which require only selective fragments of particular Web pages , can be made more efficient for users of limited-display-size handheld mobile devices by delivering only the target fragments . Semantic bookmarks provide a robust conceptual framework for recording and retrieving such targeted content not only from the specific pages used in creating the bookmarks but also from any user-specified page with similar content semantics . This paper describes a technique for realizing semantic bookmarks by coupling machine learning with Web page segmentation to create a statistical model of the bookmarked content . These models are used to identify and retrieve the bookmarked content from Web pages that share a common content domain . In contrast to ontology-based approaches where semantic bookmarks are limited to available concepts in the ontology , the learning-based approach allows users to bookmark ad-hoc personalized semantic concepts to effectively target content that fits the limited display of handhelds . User evaluation measuring the effectiveness of a prototype implementation of learning-based semantic bookmarking at reducing browsing fatigue in handhelds is provided ."
"550","Building an open source meta-search engine In this short paper we introduce Helios , a flexible and efficient open source meta-search engine . Helios currently runs on the top of 18 search engines ( in Web , Books , News , and Academic publication domains ) , but additional search engines can be easily plugged in . We also report some performance mesured during its development ."
"551","WAND : a meta-data maintenance system over the internet WAND is a meta-data management system that provides a file-system tree for users of an internet based P2P network . The tree is robust and retains its structure even when nodes ( peers ) enter and leave the network . The robustness is based on a concept of virtual folders that are automatically created to retain paths to lower level folders whenever a node hosting a higher-level folder moves away . Other contributions of the WAND system include its novel approach towards managing root directory information and handling network partitions ."
"552","Predictive ranking : a novel page ranking approach by estimating the web structure PageRank ( PR ) is one of the most popular ways to rank web pages . However , as the Web continues to grow in volume , it is becoming more and more difficult to crawl all the available pages . As a result , the page ranks computed by PR are only based on a subset of the whole Web . This produces inaccurate outcome because of the inherent incomplete information ( dangling pages ) that exist in the calculation . To overcome this incompleteness , we propose a new variant of the PageRank algorithm called , Predictive Ranking ( PreR ) , in which different classes of dangling pages are analyzed individually so that the link structure can be predicted more accurately . We detail our proposed steps . Furthermore , experimental results show that this algorithm achieves encouraging results when compared with previous methods ."
"553","Building adaptable and reusable XML applications with model transformations We present an approach in which the semantics of an XML language is defined by means of a transformation from an XML document model ( an XML schema ) to an application specific model . The application specific model implements the intended behavior of documents written in the language . A transformation is specified in a model transformation language used in the Model Driven Architecture ( MDA ) approach for software development . Our approach provides a better separation of three concerns found in XML applications : syntax , syntax processing logic and intended meaning of the syntax . It frees the developer of low-level syntactical details and improves the adaptability and reusability of XML applications . Declarative transformation rules and the explicit application model provide a finer control over the application parts affected by adaptations . Transformation rules and the application model for an XML language may be composed with the corresponding rules and application models defined for other XML languages . In that way we achieve reuse and composition of XML applications ."
"554","LSH forest : self-tuning indexes for similarity search We consider the problem of indexing high-dimensional data for answering ( approximate ) similarity-search queries . Similarity indexes prove to be important in a wide variety of settings : Web search engines desire fast , parallel , main-memory-based indexes for similarity search on text data ; database systems desire disk-based similarity indexes for high-dimensional data , including text and images ; peer-to-peer systems desire distributed similarity indexes with low communication cost . We propose an indexing scheme called LSH Forest which is applicable in all the above contexts . Our index uses the well-known technique of locality-sensitive hashing ( LSH ) , but improves upon previous designs by ( a ) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned , and ( b ) improving on LSH 's performance guarantees for skewed data distributions while retaining the same storage and query overhead . We show how to construct this index in main memory , on disk , in parallel systems , and in peer-to-peer systems . We evaluate the design with experiments on multiple text corpora and demonstrate both the self-tuning nature and the superior performance of LSH Forest ."
"555","Web resource geographic location classification and detection Rapid pervasion of the web into users ' daily lives has put much importance on capturing location-specific information on the web , due to the fact that most human activities occur locally around where a user is located . This is especially true in the increasingly popular mobile and local search environments . Thus , how to correctly and effectively detect locations from web resources has become a key challenge to location-based web applications . In this paper , we first explicitly distinguish the locations of web resources into three types to cater to different application needs : 1 ) provider location ; 2 ) content location ; and 3 ) serving location . Then we describe a unified system that computes each of the three locations , employing a set of algorithms and different geographic sources ."
"556","GalaTex : a conformant implementation of the XQuery full-text language We describe GalaTex , the first complete implementation of XQuery Full-Text , a W3C specification that extends XPath 2.0 and XQuery 1.0 with full-text search . XQuery Full-Text provides composable full-text search primitives such as keyword search , Boolean queries , and keyword-distance predicates . GalaTex is intended to serve as a reference implementation for XQuery Full-Text and as a platform for addressing new research problems such as scoring full-text query results , optimizing XML queries over both structure and text , and evaluating top-k queries on scored results . GalaTex is an all-XQuery implementation initially focused on completeness and conformance rather than on efficiency . We describe its implementation on top of Galax , a complete XQuery implementation ."
"557","Representing personal web information using a topic-oriented interface Nowadays , Web activities have become daily practice for people . It is therefore essential to organize and present this continuously increasing Web information in a more usable manner . In this paper , we developed a novel approach to reorganize personal Web information as a topic-oriented interface . In our approach , we proposed to utilize anchor , title and URL information to represent content information for the browsed Web pages rather than the content body . Furthermore , we explored three methods to organize personal Web information : 1 ) top-down statistical clustering ; 2 ) salience phrase based clustering ; and 3 ) support vector machine ( SVM ) based classification . Finally , we conducted a usability study to verify the effectiveness of our proposed solution . The experimental results demonstrated that users could visit the pages that have been browsed previously more easily with our approach than existing solutions ."
"558","A comprehensive comparative study on term weighting schemes for text categorization with support vector machines Term weighting scheme , which has been used to convert the documents as vectors in the term space , is a vital step in automatic text categorization . In this paper , we conducted comprehensive experiments to compare various term weighting schemes with SVM on two widely-used benchmark data sets . We also presented a new term weighting scheme tf-rf to improve the term 's discriminating power . The controlled experimental results showed that this newly proposed tf-rf scheme is significantly better than other widely-used term weighting schemes . Compared with schemes related with tf factor alone , the idf factor does not improve or even decrease the term 's discriminating power for text categorization ."
"559","Preferential walk : towards efficient and scalable search in unstructured peer-to-peer networks To improve search efficiency and reduce unnecessary traffic in Peer-to-Peer ( P2P ) networks , this paper proposes a trust-based probabilistic search algorithm , called preferential walk ( P-Walk ) . Every peer ranks its neighbors according to searching experience . The highly ranked neighbors have higher probabilities to be queried . Simulation results show that P-Walk is not only efficient , but also robust against malicious behaviors . Furthermore , we measure peers ' rank distribution and draw implications ."
"560","Schemapath , a minimal extension to xml schema for conditional constraints In the past few years , a number of constraint languages for XML documents has been proposed . They are cumulatively called schema languages or validation languages and they comprise , among others , DTD , XML Schema , RELAX NG , Schematron , DSD , xlinkit . One major point of discrimination among schema languages is the support of co-constraints , or co-occurrence constraints , e.g. , requiring that attribute A is present if and only if attribute B is ( or is not ) presentin the same element . Although there is no way in XML Schema to express these requirements , they are in fact frequently used in many XML document types , usually only expressed in plain human-readable text , and validated by means of special code modules by the relevant applications . In this paper we propose SchemaPath , a light extension of XML Schema to handle conditional constraints on XML documents . Two new constructs have been added to XML Schema : conditions -- based on XPath patterns -- on type assignments for elements and attributes ; and a new simple type , xsd : error , for the direct expression of negative constraints ( e.g. it is prohibited for attribute A to be present if attribute B is also present ) . A proof-of-concept implementation is provided . A Web interface is publicly accessible for experiments and assessments of the real expressiveness of the proposed extension ."
"561","TotalRank : ranking without damping PageRank is defined as the stationary state of a Markov chain obtained by perturbing the transition matrix of a web graph with a damping factor α that spreads part of the rank . The choice of α is eminently empirical , but most applications use α = 0.85 ; nonetheless , the selection of α is critical , and some believe that link farms may use this choice adversarially . Recent results ( 1 ) prove that the PageRank of a page is a rational function of α , and that this function can be approximated quite efficiently : this fact can be used to define a new form of ranking , TotalRank , that averages PageRanks over all possible α 's . We show how this rank can be computed efficiently , and provide some preliminary experimental results on its quality and comparisons with PageRank ."
"562","A comparison of implicit and explicit links for web page classification It is well known that Web-page classification can be enhanced by using hyperlinks that provide linkages between Web pages . However , in the Web space , hyperlinks are usually sparse , noisy and thus in many situations can only provide limited help in classification . In this paper , we extend the concept of linkages from explicit hyperlinks to implicit links built between Web pages . By observing that people who search the Web with the same queries often click on different , but related documents together , we draw implicit links between Web pages that are clicked after the same queries . Those pages are implicitly linked . We provide an approach for automatically building the implicit links between Web pages using Web query logs , together with a thorough comparison between the uses of implicit and explicit links in Web page classification . Our experimental results on a large dataset confirm that the use of the implicit links is better than using explicit links in classification performance , with an increase of more than 10.5 % in terms of the Macro-F1 measurement ."
"563","Site level noise removal for search engines The currently booming search engine industry has determined many online organizations to attempt to artificially increase their ranking in order to attract more visitors to their web sites . At the same time , the growth of the web has also inherently generated several navigational hyperlink structures that have a negative impact on the importance measures employed by current search engines . In this paper we propose and evaluate algorithms for identifying all these noisy links on the web graph , may them be spam or simple relationships between real world entities represented by sites , replication of content , etc. . Unlike prior work , we target a different type of noisy link structures , residing at the site level , instead of the page level . We thus investigate and annihilate site level mutual reinforcement relationships , abnormal support coming from one site towards another , as well as complex link alliances between web sites . Our experiments with the link database of the TodoBR search engine show a very strong increase in the quality of the output rankings after having applied our techniques ."
"564","Mapping XML instances For XML-based applications in general and B2B applications in particular , mapping between differently structured XML documents , to enable exchange of data , is a basic problem . A generic solution to the problem is of interest and desirable both in an academic and practical sense . We present a case study of the problem that arises in an XML based project , which involves mapping of different XML schemas to each other . We describe our approach to solving the problem , its advantages and limitations . We also compare and contrast our approach with previously known approaches and commercially available software solutions ."
"565","Surfing the web by site We provide a system for surfing the web at a high level of abstraction , which is an analogy of the web browser , but which displays entire sites at a time . It allows a principled investigation of what is present , based on an overview of all available information . We show a site 's relation to other sites , the broad nature of the information contained and how it is structured , and how it has changed over time . Our current system maintains a continuously updated archive of 40 million sites representing 1.9 billion web pages , and enables real-time navigation through the sea of web sites ."
"566","Decentralized orchestration of composite web services Web services make information and software available programmatically via the Internet and may be used as building blocks for applications . A composite web service is one that is built using multiple component web services and is typically specified using a language such as BPEL4WS or WSIPL . Once its specification has been developed , the composite service may be orchestrated either in a centralized or in a decentralized fashion . Decentralized orchestration offers performance improvements in terms of increased throughput and scalability and lower response time . However , decentralized orchestration also brings additional complexity to the system in terms of error recovery and fault handling . Further , incorrect design of a decentralized system can lead to potential deadlock or non-optimal usage of system resources . This paper investigates build time and runtime issues related to decentralized orchestration of composite web services . We support our design decisions with performance results obtained on a decentralized setup using BPEL4WS to describe the composite web services and BPWS4J as the underlying runtime environment to orchestrate them ."
"567","Web customization using behavior-based remote executing agents ReAgents are remotely executing agents that customize Web browsing for non-standard clients . A reAgent is essentially a one-shot '' mobile agent that acts as an extension of a client dynamically launched by the client to run on its behalf at a remote more advantageous location . ReAgents simplify the use of mobile agent technology by transparently handling data migration and run-time network communications and provide a general interface for programmers to more easily implement their application-specific customizing logic . This is made possible by the identification of useful remote behaviors i.e. common patterns of actions that exploit the ability to process and communicate remotely . Examples of such behaviors are transformers monitors cachers and collators . In this paper we identify a set ofuseful reAgent behaviors for interacting with Web services via astandard browser describe how to program and use reAgents and show that the overhead of using reAgents is low and outweighed by its benefits ."
"568","Navigationaided retrieval Users searching for information in hypermedia environments often perform querying followed by manual navigation . Yet , the conventional text\/hypertext retrieval paradigm does not explicity take post-query navigation into account . This paper proposes a new retrieval paradigm , called navigation-aided retrieval ( NAR ) , which treats both querying and navigation as first-class activities . In the NAR paradigm , querying is seen as a means to identify starting points for navigation , and navigation is guided based on information supplied in the query . NAR is a generalization of the conventional probabilistic information retrieval paradigm , which implicitly assumes no navigation takes place . This paper presents a formal model for navigation-aided retrieval , and reports empirical results that point to the real-world applicability of the model . The experiments were performed over a large Web corpus provided by TREC , using human judgments on a new rating scale developed for navigation-aided retrieval . In the case of ambiguous queries , the new retrieval model identifies good starting points for post-query navigation . For less ambiguous queries that need not be paired with navigation , the output closely matches that of a conventional retrieval system ."
"569","Improving Web search efficiency via a locality based static pruning method The unarguably fast , and continuous , growth of the volume of indexed ( and indexable ) documents on the Web poses a great challenge for search engines . This is true regarding not only search effectiveness but also time and space efficiency . In this paper we present an index pruning technique targeted for search engines that addresses the latter issue without disconsidering the former . To this effect , we adopt a new pruning strategy capable of greatly reducing the size of search engine indices . Experiments using a real search engine show that our technique can reduce the indices ' storage costs by up to 60 % over traditional lossless compression methods , while keeping the loss in retrieval precision to a minimum . When compared to the indices size with no compression at all , the compression rate is higher than 88 % , i.e. , less than one eighth of the original size . More importantly , our results indicate that , due to the reduction in storage overhead , query processing time can be reduced to nearly 65 % of the original time , with no loss in average precision . The new method yields significative improvements when compared against the best known static pruning method for search engine indices . In addition , since our technique is orthogonal to the underlying search algorithms , it can be adopted by virtually any search engine ."
"570","Mining search engine query logs for query recommendation This paper presents a simple and intuitive method for mining search engine query logs to get fast query recommendations on a large scale industrial strength search engine . In order to get a more comprehensive solution , we combine two methods together . On the one hand , we study and model search engine users ' sequential search behavior , and interpret this consecutive search behavior as client-side query refinement , that should form the basis for the search engine 's own query refinement process . On the other hand , we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data , and more specifically , the shortness of most query sessions . To evaluate our method , we use one hundred day worth query logs from SINA ' search engine to do off-line mining . Then we analyze three independent editors evaluations on a query test set . Based on their judgement , our method was found to be effective for finding related queries , despite its simplicity . In addition to the subjective editors ' rating , we also perform tests based on actual anonymous user search sessions ."
"571","Ranking definitions with supervised learning methods This paper is concerned with the problem of definition search . Specifically , given a term , we are to retrieve definitional excerpts of the term and rank the extracted excerpts according to their likelihood of being good definitions . This is in contrast to the traditional approaches of either generating a single combined definition or simply outputting all retrieved definitions . Definition ranking is essential for the task . Methods for performing definition ranking are proposed in this paper , which formalize the problem as either classification or ordinal regression . A specification for judging the goodness of a definition is given . We employ SVM as the classification model and Ranking SVM as the ordinal regression model respectively , such that they rank definition candidates according to their likelihood of being good definitions . Features for constructing the SVM and Ranking SVM models are defined . An enterprise search system based on this method has been developed and has been put into practical use . Experimental results indicate that the use of SVM and Ranking SVM can significantly outperform the baseline methods of using heuristic rules or employing the conventional information retrieval method of Okapi . This is true both when the answers are paragraphs and when they are sentences . Experimental results also show that SVM or Ranking SVM models trained in one domain can be adapted to another domain , indicating that generic models for definition ranking can be constructed ."
"572","Model-directed web transactions under constrained modalities Online transactions ( e.g. , buying a book on the Web ) typically involve a number of steps spanning several pages . Conducting such transactions under constrained interaction modalities as exemplified by small screen handhelds or interactive speech interfaces - the primary mode of communication for visually impaired individuals - is a strenuous , fatigue-inducing activity . But usually one needs to browse only a small fragment of a Web page to perform a transactional step such as a form fillout , selecting an item from a search results list , etc. . We exploit this observation to develop an automata-based process model that delivers only the `` relevant '' page fragments at each transactional step , thereby reducing information overload on such narrow interaction bandwidths . We realize this model by coupling techniques from content analysis of Web documents , automata learning and statistical classification . The process model and associated techniques have been incorporated into Guide-O , a prototype system that facilitates online transactions using speech\/keyboard interface ( Guide-O-Speech ) , or with limited-display size handhelds ( Guide-O-Mobile ) . Performance of Guide-O and its user experience are reported ."
"573","WS-replication : a framework for highly available web services Due to the rapid acceptance of web services and its fast spreading , a number of mission-critical systems will be deployed as web services in next years . The availability of those systems must be guaranteed in case of failures and network disconnections . An example of web services for which availability will be a crucial issue are those belonging to coordination web service infrastructure , such as web services for transactional coordination ( e.g. , WS-CAF and WS-Transaction ) . These services should remain available despite site and connectivity failures to enable business interactions on a 24x7 basis . Some of the common techniques for attaining availability consist in the use of a clustering approach . However , in an Internet setting a domain can get partitioned from the network due to a link overload or some other connectivity problems . The unavailability of a coordination service impacts the availability of all the partners in the business process . That is , coordination services are an example of critical components that need higher provisions for availability . In this paper , we address this problem by providing an infrastructure , WS-Replication , for WAN replication of web services . The infrastructure is based on a group communication web service , WS-Multicast , that respects the web service autonomy . The transport of WS-Multicast is based on SOAP and relies exclusively on web service technology for interaction across organizations . We have replicated WS-CAF using our WS-Replication framework and evaluated its performance ."
"574","Time-dependent semantic similarity measure of queries using historical click-through data It has become a promising direction to measure similarity of Web search queries by mining the increasing amount of click-through data logged by Web search engines , which record the interactions between users and the search engines . Most existing approaches employ the click-through data for similarity measure of queries with little consideration of the temporal factor , while the click-through data is often dynamic and contains rich temporal information . In this paper we present a new framework of time-dependent query semantic similarity model on exploiting the temporal characteristics of historical click-through data . The intuition is that more accurate semantic similarity values between queries can be obtained by taking into account the timestamps of the log data . With a set of user-defined calendar schema and calendar patterns , our time-dependent query similarity model is constructed using the marginalized kernel technique , which can exploit both explicit similarity and implicit semantics from the click-through data effectively . Experimental results on a large set of click-through data acquired from a commercial search engine show that our time-dependent query similarity model is more accurate than the existing approaches . Moreover , we observe that our time-dependent query similarity model can , to some extent , reflect real-world semantics such as real-world events that are happening over time ."
"575","Abstracting application-level web security Application-level web security refers to vulnerabilities inherent in the code of a web-application itself ( irrespective of the technologies in which it is implemented or the security of the web-server\/back-end database on which it is built ) . In the last few months application-level vulnerabilities have been exploited with serious consequences : hackers have tricked e-commerce sites into shipping goods for no charge , user-names and passwords have been harvested and condential information ( such as addresses and credit-card numbers ) has been leaked . In this paper we investigate new tools and techniques which address the problem of application-level web security . We ( i ) describe a scalable structuring mechanism facilitating the abstraction of security policies from large web-applications developed in heterogenous multi-platform environments ; ( ii ) present a tool which assists programmers develop secure applications which are resilient to a wide range of common attacks ; and ( iii ) report results and experience arising from our implementation of these techniques ."
"576","Csurf : a context-driven non-visual web-browser Web sites are designed for graphical mode of interaction . Sighted users can `` cut to the chase '' and quickly identify relevant information in Web pages . On the contrary , individuals with visual disabilities have to use screen-readers tobrowse the Web . As screen-readers process pages sequentially and read through everything , Web browsing can become strenuous and time-consuming . Although , the use ofshortcuts and searching offers some improvements , the problem still remains . In this paper , we address the problemof information overload in non-visual Web access using thenotion of context . Our prototype system , CSurf , embodyingor approach , provides the usual features of a screen-reader . However , when a user follows a link , CSurf captures thecontext of the link using a simple topic-boundary detectiontechnique , and uses it to identify relevant information onthe next page with the help of a Support Vector Machine , astatistical machine-learning model . Then , CSurf reads the Web page starting from the most relevant section , identifiedby the model . We conducted a series experiments to evaluate the performance of CSurf against the state-of-the-artscreen-reader , JAWS . Our results show that the use of context can potentially save browsing time and substantiallyimprove browsing experience of visually disabled people ."
"577","Answering bounded continuous search queries in the world wide web Search queries applied to extract relevant information from the World Wide Web over a period of time may be denoted as continuous search queries . The improvement of continuous search queries may concern not only the quality of retrieved results but also the freshness of results , i.e. the time between the availability of a respective data object on the Web and the notification of a user by the search engine . In some cases a user should be notified immediately since the value of the respective information decreases quickly , as e.g. news about companies that affect the value of respective stocks , or sales offers for products that may no longer be available after a short period of time . In the document filtering literature , the optimization of such queries is usually based on threshold classification . Documents above a quality threshold are returned to a user . The threshold is tuned in order to optimize the quality of retrieved results . The disadvantage of such approaches is that the amount of information returned to a user may hardly be controlled without further user-interaction . In this paper , we consider the optimization of bounded continuous search queries where only the estimated best k elements are returned to a user . We present a new optimization method for bounded continuous search queries based on the optimal stopping theory and compare the new method to methods currently applied by Web search systems . The new method provides results of significantly higher quality for the cases where very fresh results have to be delivered ."
"578","P-TAG : large scale automatic generation of personalized annotation tags for the web The success of the Semantic Web depends on the availability of Web pages annotated with metadata . Free form metadata or tags , as used in social bookmarking and folksonomies , have become more and more popular and successful . Such tags are relevant keywords associated with or assigned to a piece of information ( e.g. , a Web page ) , describing the item and enabling keyword-based classification . In this paper we propose P-TAG , a method which automatically generates personalized tags for Web pages . Upon browsing a Web page , P-TAG produces keywords relevant both to its textual content , but also to the data residing on the surfer 's Desktop , thus expressing a personalized viewpoint . Empirical evaluations with several algorithms pursuing this approach showed very promising results . We are therefore very confident that such a user oriented automatic tagging approach can provide large scale personalized metadata annotations as an important step towards realizing the Semantic Web ."
"579","Opinion observer : analyzing and comparing opinions on the Web The Web has become an excellent source for gathering consumer opinions . There are now numerous Web sites containing such opinions , e.g. , customer reviews of products , forums , discussion groups , and blogs . This paper focuses on online customer reviews of products . It makes two contributions . First , it proposes a novel framework for analyzing and comparing consumer opinions of competing products . A prototype system called Opinion Observer is also implemented . The system is such that with a single glance of its visualization , the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features . This comparison is useful to both potential customers and product manufacturers . For a potential customer , he\/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products , which helps him\/her to decide which product to buy . For a product manufacturer , the comparison enables it to easily gather marketing intelligence and product benchmarking information . Second , a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews . Such features form the basis for the above comparison . Experimental results show that the technique is highly effective and outperform existing methods significantly ."
"580","Expressiveness of XSDs : from practice to theory , there and back again On an abstract level , XML Schema increases the limited expressive power of Document Type Definitions ( DTDs ) by extending them with a recursive typing mechanism . However , an investigation of the XML Schema Definitions ( XSDs ) occurring in practice reveals that the vast majority of them are structurally equivalent to DTDs . This might be due to the complexity of the XML Schema specification and the difficulty to understand the effect of constraints on typing and validation of schemas . To shed some light on the actual expressive power of XSDs this paper studies the impact of the Element Declarations Consistent ( EDC ) and the Unique Particle Attribution ( UPA ) rule . An equivalent formalism based on contextual patterns rather than on recursive types is proposed which might serve as a light-weight front end for XML Schema . Finally , the effect of EDC and UPA on the way XML documents can be typed is discussed . It is argued that a cleaner , more robust , stronger but equally efficient class is obtained by replacing EDC and UPA with the notion of 1-pass preorder typing : schemas that allow to determine the type of an element of a streaming document when its opening tag is met . This notion can be defined in terms of restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns ."
"581","XML screamer : an integrated approach to high performance XML parsing , validation and deserialization This paper describes an experimental system in which customized high performance XML parsers are prepared using parser generation and compilation techniques . Parsing is integrated with Schema-based validation and deserialization , and the resulting validating processors are shown to be as fast as or in many cases significantly faster than traditional nonvalidating parsers . High performance is achieved by integration across layers of software that are traditionally separate , by avoiding unnecessary data copying and transformation , and by careful attention to detail in the generated code . The effect of API design on XML performance is also briefly discussed . ."
"582","Learning how to learn with web contents Learning Web contents requires learners not only to navigate the Web pages to construct their own knowledge from the contents learned at and between the pages , but also to control their own navigation and knowledge construction processes . However , it is not so easy to control the learning processes . The main issue addressed is how to help learners learn how to learn with Web contents . This paper discusses how to design a meta-learning tool ."
"583","Query-free news search Many daily activities present information in the form of a stream of text , and often people can benefit from additional information on the topic discussed . TV broadcast news can be treated as one such stream of text ; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast . We evaluated a variety of algorithms for this problem , looking at the impact of inverse document frequency , stemming , compounds , history , and query length on the relevance and coverage of news articles returned in real time during a broadcast . We also evaluated several postprocessing techniques for improving the precision , including reranking using additional terms , reranking by document similarity , and filtering on document similarity . For the best algorithm , 84 % -91 % of the articles found were relevant , with at least 64 % of the articles being on the exact topic of the broadcast . In addition , a relevant article was found for at least 70 % of the topics ."
"584","Efficient query subscription processing for prospective search engines Current web search engines are retrospective in that they limit users to searches against already existing pages . Prospective search engines , on the other hand , allow users to upload queries that will be applied to newly discovered pages in the future . We study and compare algorithms for efficiently matching large numbers of simple keyword queries against a stream of newly discovered pages ."
"585","Visual web mining Analysis of web site usage data involves two significant challenges : firstly the volume of data , arising from the growth of the web , and secondly , the structural complexity of web sites . In this paper we apply Data Mining and Information Visualization techniques to the web domain in order to benefit from the power of both human visual perception and computing we term this Visual Web Mining . In response to the two challenges , we propose a generic framework , where we apply Data Mining techniques to large web data sets and use Information Visualization methods on the results . The goal is to correlate the outcomes of mining Web Usage Logs and the extracted Web Structure by visually superimposing the results . We design several new information visualization diagrams ."
"586","Position paper : a comparison of two modelling paradigms in the Semantic Web Classical logics and Datalog-related logics have both been proposed as underlying formalisms for the Semantic Web . Although these two different formalism groups have some commonalities , and look similar in the context of expressively-impoverished languages like RDF , their differences become apparent at more expressive language levels . After considering some of these differences , we argue that , although some of the characteristics of Datalog have their utility , the open environment of the Semantic Web is better served by standard logics ."
"587","Practical semantic analysis of web sites and documents As Web sites are now ordinary products , it is necessary to explicit the notion of quality of a Web site . The quality of a site may belinked to the easiness of accessibility and also to other criteria such as the fact that the site is up to date and coherent . This last quality is difficult to insure because sites may be updated very frequently , may have many authors , may be partially generated and inthis context proof-reading is very difficult . The same piece of information may be found in different occurrences , but also in data ormeta-data , leading to the need for consistency checking . In this paper we make a parallel between programs and Web sites . We present some examples of semantic constraints that one would like to specify ( constraints between the meaning of categories and sub-categories in a thematic directory , consistency between the organization chart and the rest of the site in an academic site ) . We present quickly the Natural Semantics a way to specify the semantics of programming languages that inspires ourworks . Natural Semantics itself comes from both an operational semantics and from logic programming and its implementation uses Prolog . Then we propose a specification language for semantic constraints in Web sites that , in conjunction with the well known `` make '' program , permits to generate some site verification tools by compiling the specification into Prolog code . We apply our method to alarge XML document which is the scientific part of our instituteactivity report , tracking errors or inconsistencies and alsoconstructing some indicators that can be used by the management of theinstitute ."
"588","An abuse-free fair contract signing protocol based on the RSA signature A fair contract signing protocol allows two potentially mistrusted parities to exchange their commitments ( i.e. , digital signatures ) to an agreed contract over the Internet in a fair way , so that either each of them obtains the other 's signature , or neither party does . Based on the RSA signature scheme , a new digital contract signing protocol is proposed in this paper . Like the existing RSA-based solutions for the same problem , our protocol is not only fair , but also optimistic , since the third trusted party is involved only in the situations where one party is cheating or the communication channel is interrupted . Furthermore , the proposed protocol satisfies a new property , i.e. , it is abuse-free . That is , if the protocol is executed unsuccessfully , none of the two parties can show the validity of intermediate results to others . Technical details are provided to analyze the security and performance of the proposed protocol . In summary , we present the first abuse-free fair contract signing protocol based on the RSA signature , and show that it is both secure and efficient ."
"589","Optimized query planning of continuous aggregation queries in dynamic data dissemination networks Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making . Typically a user desires to obtain the value of some aggregation function over distributed data items , for example , to know ( a ) the average of temperatures sensed by a set of sensors ( b ) the value of index of mid-cap stocks . In these queries a client specifies a coherency requirement as part of the query . In this paper we present a low-cost , scalable technique to answer continuous aggregation queries using a content distribution network of dynamic data items . In such a network of data aggregators , each data aggregator serves a set of data items at specific coherencies . Just as various fragments of a dynamic web-page are served by one or more nodes of a content distribution network , our technique involves decomposing a client query into sub-queries and executing sub-queries on judiciously chosen data aggregators with their individual sub-query incoherency bounds . We provide a technique of getting the optimal query plan ( i.e. , set of sub-queries and their chosen data aggregators ) which satisfies client query . s coherency requirement with least cost , measured in terms of the number of refresh messages sent from aggregators to the client . For estimating query execution cost , we build a continuous query cost model which can be used to estimate the number of messages required to satisfy the client specified incoherency bound . Performance results using real-world traces show that our cost based query planning leads to queries being executed using less than one third the number of messages required by existing schemes ."
"590","A search engine for natural language applications Many modern natural language-processing applications utilize search engines to locate large numbers of Web documents or to compute statistics over the Web corpus . Yet Web search engines are designed and optimized for simple human queries -- they are not well suited to support such applications . As a result , these applications are forced to issue millions of successive queries resulting in unnecessary search engine load and in slow applications with limited scalability . In response , this paper introduces the Bindings Engine ( BE ) , which supports queries containing typed variables and string-processing functions . For example , in response to the query `` powerful ` noun ' '' BE will return all the nouns in its index that immediately follow the word `` powerful '' , sorted by frequency . In response to the query `` Cities such as ProperNoun ( Head ( ` NounPhrase ' ) ) '' , BE will return a list of proper nouns likely to be city names . BE 's novel neighborhood index enables it to do so with O ( k ) random disk seeks and O ( k ) serial disk reads , where k is the number of non-variable terms in its query . As a result , BE can yield several orders of magnitude speedup for large-scale language-processing applications . The main cost is a modest increase in space to store the index . We report on experiments validating these claims , and analyze how BE 's space-time tradeoff scales with the size of its index and the number of variable types . Finally , we describe how a BE-based application extracts thousands of facts from the Web at interactive speeds in response to simple user queries ."
"591","Spam double-funnel : connecting web spammers with advertisers Spammers use questionable search engine optimization ( SEO ) techniques to promote their spam links into top search results . In this paper , we focus on one prevalent type of spam - redirection spam - where one can identify spam pages by the third-party domains that these pages redirect traffic to . We propose a five-layer , double-funnel model for describing end-to-end redirection spam , present a methodology for analyzing the layers , and identify prominent domains on each layer using two sets of commercial keywords . one targeting spammers and the other targeting advertisers . The methodology and findings are useful for search engines to strengthen their ranking algorithms against spam , for legitimate website owners to locate and remove spam doorway pages , and for legitimate advertisers to identify unscrupulous syndicators who serve ads on spam pages ."
"592","Dynamic placement for clustered web applications We introduce and evaluate a middleware clustering technology capable of allocating resources to web applications through dynamic application instance placement . We define application instance placement as the problem of placing application instances on a given set of server machines to adjust the amount of resources available to applications in response to varying resource demands of application clusters . The objective is to maximize the amount of demand that may be satisfied using a configured placement . To limit the disturbance to the system caused by starting and stopping application instances , the placement algorithm attempts to minimize the number of placement changes . It also strives to keep resource utilization balanced across all server machines . Two types of resources are managed , one load-dependent and one load-independent . When putting the chosen placement in effect our controller schedules placement changes in a manner that limits the disruption to the system ."
"593","Do not crawl in the DUST : different URLs with similar text We consider the problem of dust : Different URLs with Similar Text . Such duplicate URLs are prevalent in web sites , as web server software often uses aliases and redirections , translates URLs to some canonical form , and dynamically generates the same page from various different URL requests . We present a novel algorithm , DustBuster , for uncovering dust ; that is , for discovering rules for transforming a given URL to others that are likely to have similar content . DustBuster is able to detect dust effectively from previous crawl logs or web server logs , without examining page contents . Verifying these rules via sampling requires fetching few actual web pages . Search engines can benefit from this information to increase the effectiveness of crawling , reduce indexing overhead as well as improve the quality of popularity statistics such as PageRank ."
"594","Information retrieval in P2P networks using genetic algorithm Hybrid Peer-to-Peer ( P2P ) networks based on the direct connection model have two shortcomings which are high bandwidth consumption and poor semi-parallel search . However , they can further be improved by the query propagation model . In this paper , we propose a novel query routing strategy called GAroute based on the query propagation model . By giving the current P2P network topology and relevance level of each peer , GAroute returns a list of query routing paths that cover as many relevant peers as possible . We model this as the Longest Path Problem in a directed graph which is NP-complete and we obtain high quality ( 0.95 in 100 peers ) approximate solutions in polynomial time by using Genetic Algorithm ( GA ) . We describe the problem modeling and proposed GA for finding long paths . Finally , we summarize the experimental results which measure the scalability and quality of different searching algorithms . According to these results , GAroute works well in some large scaled P2P networks ."
"595","Just the right amount : extracting modules from ontologies The ability to extract meaningful fragments from an ontology is key for ontology re-use . We propose a definition of a module that guarantees to completely capture the meaning of a given set of terms , i.e. , to include all axioms relevant to the meaning of these terms , and study the problem of extracting minimal modules . We show that the problem of determining whether a subset of an ontology is a module for a given vocabulary is undecidable even for rather restricted sub-languages of OWL DL . Hence we propose two `` approximations '' , i.e. , alternative definitions of modules for a vocabulary that still provide the above guarantee , but that are possibly too strict , and that may thus result in larger modules : the first approximation is semantic and can be computed using existing DL reasoners ; the second is syntactic , and can be computed in polynomial time . Finally , we report on an empirical evaluation of our syntactic approximation which demonstrates that the modules we extract are surprisingly small ."
"596","Scalable techniques for memory-efficient CDN simulations Since CDN simulations are known to be highly memory-intensive , in this paper , we argue the need for reducing the memory requirements of such simulations . We propose a novel memory-efficient data structure that stores cache state for a small subset of popular objects accurately and uses approximations for storing the state for the remaining objects . Since popular objects receive a large fraction of the requests while less frequently accessed objects consume much of the memory space , this approach yields large memory savings and reduces errors . We use bloom filters to store approximate state and show that careful choice of parameters can substantially reduce the probability of errors due to approximations . We implement our techniques into a user library for constructing proxy caches in CDN simulators . Our experimental results show up to an order of magnitude reduction in memory requirements of CDN simulations , while incurring a 5-10 % error ."
"597","MedSearch : a specialized search engine for medical information People are thirsty for medical information . Existing Web search engines can not handle medical search well because they do not consider its special requirements . Often a medical information searcher is uncertain about his exact questions and unfamiliar with medical terminology . Therefore , he prefers to pose long queries , describing his symptoms and situation in plain English , and receive comprehensive , relevant information from search results . This paper presents MedSearch , a specialized medical Web search engine , to address these challenges . MedSearch can assist ordinary Internet users to search for medical information , by accepting queries of extended length , providing diversified search results , and suggesting related medical phrases ."
"598","Affinity rank : a new scheme for efficient web search Maximizing only the relevance between queries and documents will not satisfy users if they want the top search results to present a wide coverage of topics by a few representative documents . In this paper , we propose two new metrics to evaluate the performance of information retrieval : diversity , which measures the topic coverage of a group of documents , and information richness , which measures the amount of information contained in a document . Then we present a novel ranking scheme , Affinity Rank , which utilizes these two metrics to improve search results . We demonstrate how Affinity Rank works by a toy data set , and verify our method by experiments on real-world data sets ."
"599","Web data extraction based on partial tree alignment This paper studies the problem of extracting data from a Web page that contains several structured data records . The objective is to segment these data records , extract data items\/fields from them and put the data in a database table . This problem has been studied by several researchers . However , existing methods still have some serious limitations . The first class of methods is based on machine learning , which requires human labeling of many examples from each Web site that one is interested in extracting data from . The process is time consuming due to the large number of sites and pages on the Web . The second class of algorithms is based on automatic pattern discovery . These methods are either inaccurate or make many assumptions . This paper proposes a new method to perform the task automatically . It consists of two steps , ( 1 ) identifying individual data records in a page , and ( 2 ) aligning and extracting data items from the identified data records . For step 1 , we propose a method based on visual information to segment data records , which is more accurate than existing methods . For step 2 , we propose a novel partial alignment technique based on tree matching . Partial alignment means that we align only those data fields in a pair of data records that can be aligned ( or matched ) with certainty , and make no commitment on the rest of the data fields . This approach enables very accurate alignment of multiple data records . Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records , align and extract data from them very accurately ."
"600","KnowledgeTree : a distributed architecture for adaptive e-learning This paper presents KnowledgeTree , an architecture for adaptive E-Learning based on distributed reusable intelligent learning activities . The goal of KnowledgeTree is to bridge the gap between the currently popular approach to Web-based education , which is centered on learning management systems vs. the powerful but underused technologies in intelligent tutoring and adaptive hypermedia . This integrative architecture attempts to address both the component-based assembly of adaptive systems and teacher-level reusability ."
"601","Liveclassifier : creating hierarchical text classifiers through web corpora Many Web information services utilize techniques of information extraction ( IE ) to collect important facts from the Web . To create more advanced services , one possible method is to discover thematic information from the collected facts through text classification . However , most conventional text classification techniques rely on manual-labelled corpora and are thus ill-suited to cooperate with Web information services with open domains . In this work , we present a system named LiveClassifier that can automatically train classifiersthrough Web corpora based on user-defined topic hierarchies . Due to its flexibility and convenience , LiveClassifier can be easily adapted for various purposes . New Web information services can be created to fully exploit it ; human users can use it to create classifiers for their personal applications . The effectiveness of classifiers created by LiveClassifier is well supportedby empirical evidence ."
"602","Web page classification with heterogeneous data fusion Web pages are more than text and they contain much contextual and structural information , e.g. , the title , the meta data , the anchor text , etc. , each of which can be seen as a data source or are presentation . Due to the different dimensionality and different representing forms of these heterogeneous data sources , simply putting them together would not greatly enhance the classification performance . We observe that via a kernel function , different dimensions and types of data sources can be represented into acommon format of kernel matrix , which can be seen as a generalized similarity measure between a pair of web pages . In this sense , a kernel learning approach is employed to fuse these heterogeneous data sources . The experimental results on a collection of the ODP database validate the advantages of the proposed method over traditional methods based on any single data source and the uniformly weighted combination of them ."
"603","SweetDeal : representing agent contracts with exceptions using XML rules , ontologies , and process descriptions SweetDeal is a rule-based approach to representation of business contracts that enables software agents to create , evaluate , negotiate , and execute contracts with substantial automation and modularity . It builds upon the situated courteous logic programs knowledge representation in RuleML , the emerging standard for Semantic Web XML rules . Here , we newly extend the SweetDeal approach by also incorporating process knowledge descriptions whose ontologies are represented in DAML+OIL ( emerging standard for Semantic Web ontologies ) thereby enabling more complex contracts with behavioral provisions , especially for handling exception conditions ( e.g. , late delivery or non-payment ) that might arise during the execution of the contract . This provides a foundation for representing and automating deals about services -- in particular , about Web Services , so as to help search , select , and compose them . Our system is also the first to combine emerging Semantic Web standards for knowledge representation of rules ( RuleML ) with ontologies ( DAML+OIL ) for a practical e-business application domain , and further to do so with process knowledge . This also newly fleshes out the evolving concept of Semantic Web Services . A prototype ( soon public ) is running ."
"604","TJFast : effective processing of XML twig pattern matching Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries . A number of algorithms have been proposed to process a twig query based on region encoding . In this paper , based on a novel labeling scheme : extended Dewey , we propose a novel and efficient holistic twig join algorithm , namely TJFast . Compared to previous work , our algorithm only needs to access the labels of leaf query nodes . We report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned and query performance ."
"605","Improving web performance by client characterization driven server adaptation We categorize the set of clients communicating with a server on the Web based on information that can be determined by the server . The Web server uses the information to direct tailored actions . Users with poor connectivity may choose not to stay at a Web site if it takes a long time to receive a page , even if the Web server at the site is not the bottleneck . Retaining such clients may be of interest to a Web site . Better connected clients can receive enhanced representations of Web pages , such as with higher quality images . We explore a variety of considerations that could be used by a Web server in characterizing a client . Once a client is characterized as poor or rich , the server can deliver altered content , alter how content is delivered , alter policy and caching decisions , or decide when to redirect the client to a mirror site . We also use network-aware client clustering techniques to provide a coarser level of client categorization and use it to categorize subsequent clients from that cluster for which a client-specific categorization is not available . Our results for client characterization and applicable server actions are derived from real , recent , and diverse set of Web server logs . Our experiments demonstrate that a relatively simple characterization policy can classify poor clients such that these clients subsequently make the majority of badly performing requests to a Web server . This policy is also stable in terms of clients staying in the same class for a large portion of the analysis period . Client clustering can significantly help in initially classifying clients for which no previous information about the client is known . We also show that different server actions can be applied to a significant number of request sequences with poor performance ."
"606","A web middleware architecture for dynamic customization of content for wireless clients We present a new Web middleware architecture that allows users to customize their view of the Web for optimal interaction and system operation when using non-traditional resource-limited client machines such as wireless PDAs ( personal digital assistants ) . Web Stream Customizers ( WSC ) are dynamically deployable software modules and can be strategically located between client and server to achieve improvements in performance , reliability , or security . An important design feature is that Customizers provide two points of control in the communication path between client and server , supporting adaptive system-based and content-based customization . Our architecture exploits HTTP 's proxy capabilities , allowing Customizers to be seamlessly integrated with the basic Web transaction model . We describe the WSC architecture and implementation , and illustrate its use with three non-trivial , adaptive Customizer applications that we have built . We show that the overhead in our implementation is small and tolerable , and is outweighed by the benefits that Customizers provide ."
"607","Semantic analytics on social networks : experiences in addressing the problem of conflict of interest detection In this paper , we describe a Semantic Web application that detects Conflict of Interest ( COI ) relationships among potential reviewers and authors of scientific papers . This application discovers various ` semantic associations ' between the reviewers and authors in a populated ontology to determine a degree of Conflict of Interest . This ontology was created by integrating entities and relationships from two social networks , namely `` knows , '' from a FOAF ( Friend-of-a-Friend ) social network and `` co-author , '' from the underlying co-authorship network of the DBLP bibliography . We describe our experiences developing this application in the context of a class of Semantic Web applications , which have important research and engineering challenges in common . In addition , we present an evaluation of our approach for real-life COI detection ."
"608","Is question answering an acquired skill ? We present a question answering ( QA ) system which learns how to detect and rank answer passages by analyzing questions and their answers ( QA pairs ) provided as training data . We built our system in only a few person-months using off-the-shelf components : a part-of-speech tagger , a shallow parser , a lexical network , and a few well-known supervised learning algorithms . In contrast , many of the top TREC QA systems are large group efforts , using customized ontologies , question classifiers , and highly tuned ranking functions . Our ease of deployment arises from using generic , trainable algorithms that exploit simple feature extractors on QA pairs . With TREC QA data , our system achieves mean reciprocal rank ( MRR ) that compares favorably with the best scores in recent years , and generalizes from one corpus to another . Our key technique is to recover , from the question , fragments of what might have been posed as a structured query , had a suitable schema been available . comprises selectors : tokens that are likely to appear ( almost ) unchanged in an answer passage . The other fragment contains question tokens which give clues about the answer type , and are expected to be replaced in the answer passage by tokens which specialize or instantiate the desired answer type . Selectors are like constants in where-clauses in relational queries , and answer types are like column names . We present new algorithms for locating selectors and answer type clues and using them in scoring passages with respect to a question ."
"609","PRIVE : anonymous location-based queries in distributed mobile systems Nowadays , mobile users with global positioning devices canaccess Location Based Services ( LBS ) and query about pointsof interest in their proximity . For such applications to succeed , privacy and confidentiality are essential . Encryptionalone is not adequate ; although it safeguards the systemagainst eavesdroppers , the queries themselves may disclosethe location and identity of the user . Recently , there havebeen proposed centralized architectures based on K-anonymity , which utilize an intermediate anonymizer between themobile users and the LBS . However , the anonymizer mustbe updated continuously with the current locations of allusers . Moreover , the complete knowledge of the entire systemposes a security threat , if the anonymizer is compromised . In this paper we address two issues : ( i ) We show thatexisting approaches may fail to provide spatial anonymityfor some distributions of user locations and describe a noveltechnique which solves this problem . ( ii ) We propose Prive , a decentralized architecture for preserving the anonymityof users issuing spatial queries to LBS . Mobile users self-organizeinto an overlay network with good fault toleranceand load balancing properties . Prive avoids the bottleneckcaused by centralized techniques both in terms of anonymizationand location updates . Moreover , the system state isdistributed in numerous users , rendering Prive resilient toattacks . Extensive experimental studies suggest that Priveis applicable to real-life scenarios with large populations ofmobile users ."
"610","The Yin\/Yang web : XML syntax and RDF semantics XML is the W3C standard document format for writing and exchanging information on the Web . RDF is the W3C standard model for describing the semantics and reasoning about information on the Web . Unfortunately , RDF and XML -- although very close to each other -- are based on two different paradigms . We argue that in order to lead the Semantic Web to its full potential , the syntax and the semantics of information needs to work together . To this end , we develop a model-theoretic semantics for the XML XQuery 1.0 and XPath 2.0 Data Model , which provides a unified model for both XML and RDF . This unified model can serve as the basis for Web applications that deal with both data and semantics . We illustrate the use of this model on a concrete information integration scenario . Our approach enables each side of the fence to benefit from the other , notably , we show how the RDF world can take advantage of XML query languages , and how the XML world can take advantage of the reasoning capabilities available for RDF ."
"611","Web projections : learning from contextual subgraphs of the web Graphical relationships among Web pages have been exploited inmethods for ranking search results . To date , specific graphicalproperties have been used in these analyses . We introduce a WebProjection methodology that generalizes prior efforts of graphicalrelationships of the web in several ways . With the approach , wecreate subgraphs by projecting sets of pages and domains onto thelarger web graph , and then use machine learning to constructpredictive models that consider graphical properties as evidence . Wedescribe the method and then present experiments that illustrate theconstruction of predictive models of search result quality and userquery reformulation ."
"612","Using urls and table layout for web classification tasks We propose new features and algorithms for automating Web-page classification tasks such as content recommendation and ad blocking . We show that the automated classification of Web pages can be much improved if , instead of looking at their textual content , we consider each links 's URL and the visual placement of those links on a referring page . These features are unusual : rather than being scalar measurements like word counts they are tree structured -- describing the position of the item in a tree . We develop a model and algorithm for machine learning using such tree-structured features . We apply our methods in automated tools for recognizing and blocking Web advertisements and for recommending `` interesting '' news stories to a reader . Experiments show that our algorithms are both faster and more accurate than those based on the text content of Web documents ."
"613","The discoverability of the web Previous studies have highlighted the high arrival rate of new contenton the web . We study the extent to which this new content can beefficiently discovered by a crawler . Our study has two parts . First , we study the inherent difficulty of the discovery problem using amaximum cover formulation , under an assumption of perfect estimates oflikely sources of links to new content . Second , we relax thisassumption and study a more realistic setting in which algorithms mustuse historical statistics to estimate which pages are most likely toyield links to new content . We recommend a simple algorithm thatperforms comparably to all approaches we consider . We measure the emphoverhead of discovering new content , defined asthe average number of fetches required to discover one new page . Weshow first that with perfect foreknowledge of where to explore forlinks to new content , it is possible to discover 90 % of all newcontent with under 3 % overhead , and 100 % of new content with 9 % overhead . But actual algorithms , which do not have access to perfectforeknowledge , face a more difficult task : one quarter of new contentis simply not amenable to efficient discovery . Of the remaining threequarters , 80 % of new content during a given week may be discoveredwith 160 % overhead if content is recrawled fully on a monthly basis ."
"614","Scaling personalized web search Recent web search techniques augment traditional text matching with a global notion of `` importance '' based on the linkage structure of the web , such as in Google 's PageRank algorithm . For more refined searches , this global notion of importance can be specialized to create personalized views of importance -- for example , importance scores can be biased according to a user-specified set of initially-interesting pages . Computing and storing all possible personalized views in advance is impractical , as is computing personalized views at query time , since the computation of each view requires an iterative computation over the web graph . We present new graph-theoretical results , and a new technique based on these results , that encode personalized views as partial vectors . Partial vectors are shared across multiple personalized views , and their computation and storage costs scale well with the number of views . Our approach enables incremental computation , so that the construction of personalized views from partial vectors is practical at query time . We present efficient dynamic programming algorithms for computing partial vectors , an algorithm for constructing personalized views from partial vectors , and experimental results demonstrating the effectiveness and scalability of our techniques ."
"615","Extrapolation methods for accelerating PageRank computations We present a novel algorithm for the fast computation of PageRank , a hyperlink-based estimate of the '' importance '' of Web pages . The original PageRank algorithm uses the Power Method to compute successive iterates that converge to the principal eigenvector of the Markov matrix representing the Web link graph . The algorithm presented here , called Quadratic Extrapolation , accelerates the convergence of the Power Method by periodically subtracting off estimates of the nonprincipal eigenvectors from the current iterate of the Power Method . In Quadratic Extrapolation , we take advantage of the fact that the first eigenvalue of a Markov matrix is known to be 1 to compute the nonprincipal eigenvectors using successive iterates of the Power Method . Empirically , we show that using Quadratic Extrapolation speeds up PageRank computation by 25-300 % on a Web graph of 80 million nodes , with minimal overhead . Our contribution is useful to the PageRank community and the numerical linear algebra community in general , as it is a fast method for determining the dominant eigenvector of a matrix that is too large for standard fast methods to be practical ."
"616","Web ontology segmentation : analysis , classification and use Ontologies are at the heart of the semantic web . They define the concepts and relationships that make global interoperability possible . However , as these ontologies grow in size they become more and more difficult to create , use , understand , maintain , transform and classify . We present and evaluate several algorithms for extracting relevant segments out of large description logic ontologies for the purposes of increasing tractability for both humans and computers . The segments are not mere fragments , but stand alone as ontologies in their own right . This technique takes advantage of the detailed semantics captured within an OWL ontology to produce highly relevant segments . The research was evaluated using the GALEN ontology of medical terms and procedures ."
"617","Estimating required recall for successful knowledge acquisition from the web Information on the Web is not only abundant but also redundant . This redundancy of information has an important consequence on the relation between the recall of an information gathering system and its capacity to harvest the core information of a certain domain of knowledge . This paper provides a new idea for estimating the necessary Web coverage of a knowledge acquisition system in order to achieve a certain desired coverage of the contained core information ."
"618","Combining link and content analysis to estimate semantic similarity Search engines use content and link information to crawl , index , retrieve , and rank Web pages . The correlations between similarity measures based on these cues and on semantic associations between pages therefore crucially affects the performance of any search tool . Here I begin to quantitatively analyze the relationship between content , link , and semantic similarity measures across a massive number of Web page pairs . Maps of semantic similarity across textual and link similarity highlight the potential and limitations of lexical and link analysis for relevance approximation , and provide us with a way to study whether and how text and link based measures should be combined ."
"619","Automatically learning document taxonomies for hierarchical classification While several hierarchical classification methods have been applied to web content , such techniques invariably rely on a pre-defined taxonomy of documents . We propose a new technique that extracts a suitable hierarchical structure automatically from a corpus of labeled documents . We show that our technique groups similar classes closer together in the tree and discovers relationships among documents that are not encoded in the class labels . The learned taxonomy is then used along with binary SVMs for multi-class classification . We demonstrate the efficacy of our approach by testing it on the 20-Newsgroup dataset ."
"620","Learning block importance models for web pages Previous work shows that a web page can be partitioned into multiple segments or blocks , and often the importance of those blocks in a page is not equivalent . Also , it has been proven that differentiating noisy or unimportant blocks from pages can facilitate web mining , search and accessibility . However , no uniform approach and model has been presented to measure the importance of different segments in web pages . Through a user study , we found that people do have a consistent view about the importance of blocks in web pages . In this paper , we investigate how to find a model to automatically assign importance values to blocks in a web page . We define the block importance estimation as a learning problem . First , we use a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure . Then spatial features ( such as position and size ) and content features ( such as the number of images and links ) are extracted to construct a feature vector for each block . Based on these features , learning algorithms are used to train a model to assign importance to different segments in the web page . In our experiments , the best model can achieve the performance with Micro-F1 79 % and Micro-Accuracy 85.9 % , which is quite close to a person 's view ."
"621","An ontology for internal and external business processes In this paper we introduce our multi metamodel process ontology ( m3po ) , which is based on various existing reference models and languages from the workflow and choreography domain . This ontology allows the extraction of arbitrary choreography interface descriptions from arbitrary internal workflow models . We also report on an initial validation : we translate an IBM Websphere MQ Workflow model into the m3po ontology and then extract an Abstract BPEL model from the ontology ."
"622","A personalized search engine based on web-snippet hierarchical clustering In this paper we propose a hierarchical clustering engine , called snaket , that is able to organize on-the-fly the search results drawn from 16 commodity search engines into a hierarchy of labeled folders . The hierarchy offers a complementary view to the flat-ranked list of results returned by current search engines . Users can navigate through the hierarchy driven by their search needs . This is especially useful for informative , polysemous and poor queries . SnakeT is the first complete and open-source system in the literature that offers both hierarchical clustering and folder labeling with variable-length sentences . We extensively test SnakeT against all available web-snippet clustering engines , and show that it achieves efficiency and efficacy performance close to the best known engine Vivisimo.com . Recently , personalized search engines have been introduced with the aim of improving search results by focusing on the users , rather than on their submitted queries . We show how to plug SnakeT on top of any ( un-personalized ) search engine in order to obtain a form of personalization that is fully adaptive , privacy preserving , scalable , and non intrusive for underlying search engines ."
"623","Spam attacks : p2p to the rescue We propose a decentralized privacy-preserving approach to spam filtering . Our solution exploits robust digests to identify messages that are a slight variation of one another and a peer-to-peer architecture between mail servers to collaboratively share knowledge about spam ."
"624","Capturing RIA concepts in a web modeling language This work addresses conceptual modeling and automatic code generation for Rich Internet Applications , a variant of Web-based systems bridging the gap between desktop and Web interfaces . The approach we propose is a first step towards a full integration of RIA paradigms into the Web development process , enabling the specification of complex Web solutions mixing HTTP+HTML and Rich Internet Applications , using a single modeling language and tool ."
"625","Choosing the best knowledge base system for large semantic web applications We present an evaluation of four knowledge base systems with respect to use in large Semantic Web applications . We discuss the performance of each system . In particular , we show that existing systems need to place a greater emphasis on scalability ."
"626","Information flow modeling based on diffusion rate for prediction and ranking Information flows in a network where individuals influence each other . The diffusion rate captures how efficiently the information can diffuse among the users in the network . We propose an information flow model that leverages diffusion rates for : ( 1 ) prediction . identify where information should flow to , and ( 2 ) ranking . identify who will most quickly receive the information . For prediction , we measure how likely information will propagate from a specific sender to a specific receiver during a certain time period . Accordingly a rate-based recommendation algorithm is proposed that predicts who will most likely receive the information during a limited time period . For ranking , we estimate the expected time for information diffusion to reach a specific user in a network . Subsequently , a DiffusionRank algorithm is proposed that ranks users based on how quickly information will flow to them . Experiments on two datasets demonstrate the effectiveness of the proposed algorithms to both improve the recommendation performance and rank users by the efficiency of information flow ."
"627","Probabilistic query expansion using query logs Query expansion has long been suggested as an effective way to resolve the short query and word mismatching problems . A number of query expansion methods have been proposed in traditional information retrieval . However , these previous methods do not take into account the specific characteristics of web searching ; in particular , of the availability of large amount of user interaction information recorded in the web query logs . In this study , we propose a new method for query expansion based on query logs . The central idea is to extract probabilistic correlations between query terms and document terms by analyzing query logs . These correlations are then used to select high-quality expansion terms for new queries . The experimental results show that our log-based probabilistic query expansion method can greatly improve the search performance and has several advantages over other existing methods ."
"628","The case for multi-user design for computer aided learning in developing regions Computer-aided learning is fast gaining traction in developing regions as a means to augment classroom instruction . Reasons for using computer-aided learning range from supplementing teacher shortages to starting underprivileged children off in technology , and funding for such initiatives range from state education funds to international agencies and private groups interested in child development . The interaction of children with computers is seen at various levels , from unsupervised self-guided learning at public booths without specific curriculum to highly regulated in-class computer applications with modules designed to go with school curriculum . Such learning is used at various levels from children as young as 5 year-old to high-schoolers . This paper uses field observations of primary school children in India using computer-aided learning modules , and finds patterns by which children who perform better in classroom activities seat themselves in front of computer monitors , and control the mouse , in cases where children are required to share computer resources . We find that in such circumstances , there emerges a pattern of learning , unique to multi-user environments - wherein certain children tend to learn better because of their control of the mouse . This research also shows that while computer aided learning software for children is primarily designed for single-users , the implementation realities of resource-strapped learning environments in developing regions presents a strong case for multi-user design ."
"629","Beyond XML and RDF : the versatile web query language xcerpt Applications and services that access Web data are becoming increasingly more useful and wide-spread . Current main-stream Web query languages such as XQuery , XSLT , or SPARQL , however , focus only on one of the different data formats available on the Web . In contrast , Xcerpt is a emphversatile semi-structured query language , i.e. , a query language able to access all kinds of Web data such as XML and RDF in the same language reusing common concepts and language constructs . To integrate heterogeneous data and as a foundation for Semantic Web reasoning , Xcerpt also provides rules . Xcerpt has a visual companion language , visXcerpt , that is conceived as a mere rendering of the ( textual ) query language Xcerpt using a slightly extended CSS . Both languages are demonstrated along a realistic use case integrating XML and RDF data highlighting interesting and unique features . Novel language constructs and optimization techniques are currently under investigation in the Xcerpt project ( cf. @ urlhttp : \/ \/ xcerpt.org \/ ) ."
"630","Similarity spreading : a unified framework for similarity calculation of interrelated objects In many Web search applications , similarities between objects of one type ( say , queries ) can be affected by the similarities between their interrelated objects of another type ( say , Web pages ) , and vice versa . We propose a novel framework called similarity spreading to take account of the interrelationship and improve the similarity calculation . Experiment results show that the proposed framework can significantly improve the accuracy of the similarity measurement of the objects in a search engine ."
"631","Protecting browser state from web privacy attacks Through a variety of means , including a range of browser cache methods and inspecting the color of a visited hyperlink , client-side browser state can be exploited to track users against their wishes . This tracking is possible because persistent , client-side browser state is not properly partitioned on per-site basis in current browsers . We address this problem by refining the general notion of a `` same-origin '' policy and implementing two browser extensions that enforce this policy on the browser cache and visited links . We also analyze various degrees of cooperation between sites to track users , and show that even if long-term browser state is properly partitioned , it is still possible for sites to use modern web features to bounce users between sites and invisibly engage in cross-domain tracking of their visitors . Cooperative privacy attacks are an unavoidable consequence of all persistent browser state that affects the behavior of the browser , and disabling or frequently expiring this state is the only way to achieve true privacy against colluding parties ."
"632","Finding advertising keywords on web pages A large and growing number of web pages display contextual advertising based on keywords automatically extracted from the text of the page , and this is a substantial source of revenue supporting the web today . Despite the importance of this area , little formal , published research exists . We describe a system that learns how to extract keywords from web pages for advertisement targeting . The system uses a number of features , such as term frequency of each potential keyword , inverse document frequency , presence in meta-data , and how often the term occurs in search query logs . The system is trained with a set of example pages that have been hand-labeled with `` relevant '' keywords . Based on this training , it can then extract new keywords from previously unseen pages . Accuracy is substantially better than several baseline systems ."
"633","Enforcing strict model-view separation in template engines The mantra of every experienced web application developer is the same : thou shalt separate business logic from display . Ironically , almost all template engines allow violation of this separation principle , which is the very impetus for HTML template engine development . This situation is due mostly to a lack of formal definition of separation and fear that enforcing separation emasculates a template 's power . I show that not only is strict separation a worthy design principle , but that we can enforce separation while providing a potent template engine . I demonstrate my StringTemplate engine , used to build jGuru.com and other commercial sites , at work solving some nontrivial generational tasks . My goal is to formalize the study of template engines , thus , providing a common nomenclature , a means of classifying template generational power , and a way to leverage interesting results from formal language theory . I classify three types of restricted templates analogous to Chomsky 's type 1 . .3 grammar classes and formally define separation including the rules that embody separation . Because this paper provides a clear definition of model-view separation , template engine designers may no longer blindly claim enforcement of separation . Moreover , given theoretical arguments and empirical evidence , programmers no longer have an excuse to entangle model and view ."
"634","Consistency-preserving caching of dynamic database content With the growing use of dynamic web content generated from relational databases , traditional caching solutions for through put and latency improvements are ineffective . We describe a middleware layer called Ganesh that reduces the volume of data transmitted without semantic interpretation of queries or results . It achieves this reduction through the use of cryptographic hashing to detect similarities with previous results . These benefits do not require any compromise of the strict consistency semantics provided by the back-end database . Further , Ganesh does not require modifications to applications , web servers , or database servers , and works with closed-source applications and databases . Using two bench marks representative of dynamic web sites , measurements of our prototype show that it can increase end-to-end throughput by as much as two fold for non-data intensive applications and by as much as ten fold for dataintensive ones ."
"635","Quality driven web services composition The process-driven composition of Web services is emerging as a promising approach to integrate business applications within and across organizational boundaries . In this approach , individual Web services are federated into composite Web services whose business logic is expressed as a process model . The tasks of this process model are essentially invocations to functionalities offered by the underlying component services . Usually , several component services are able to execute a given task , although with different levels of pricing and quality . In this paper , we advocate that the selection of component services should be carried out during the execution of a composite service , rather than at design-time . In addition , this selection should consider multiple criteria ( e.g. , price , duration , reliability ) , and it should take into account global constraints and preferences set by the user ( e.g. , budget constraints ) . Accordingly , the paper proposes a global planning approach to optimally select component services during the execution of a composite service . Service selection is formulated as an optimization problem which can be solved using efficient linear programming methods . Experimental results show that this global planning approach outperforms approaches in which the component services are selected individually for each task in a composite service ."
"636","Online curriculum on the semantic Web : the CSD-UoC portal for peer-to-peer e-learning Online Curriculum Portals aim to support networks of instructors and learners by providing a space of convergence for enhancing peer-to-peer learning interactions among individuals of an educational institution . To this end , effective , open and scalable e-learning systems are required to acquire , store , and share knowledge under the form of learning objects ( LO ) . In this paper , we are interested in exploiting the semantic relationships that characterize these LOs ( e.g. , prerequisite , part-of or see-also ) in order to capture and access individual and group knowledge in conjunction with the learning processes supported by educational institutions . To achieve this functionality , Semantic Web ( e.g. , RDF\/s ) and declarative query languages ( e.g. , RQL ) are employed to represent LOs and their relationships ( e.g. , LOM ) , as well as , to support navigation at the conceptual e-learning Portal space . In this way , different LOs could be presented to the same learners , according to the traversed schema navigation paths ( i.e. , learning paths ) . Using the Apache Jetspeed framework we are able to generate and assemble at run-time portlets ( i.e. , pluggable web components ) for visualizing personalized views as dynamic web pages . Last but not least , both learners and instructors can employ the same Portal GUI for updating semantically described LOs and thus support an open-ended continuum of learning . To the best of our knowledge , the work presented in this paper is the first Online Curriculum Portal platform supporting the aforementioned functionality ."
"637","Web montage : a dynamic personalized start page Despite the connotation of the words `` browsing '' and `` surfing , '' web usage often follows routine patterns of access . However , few mechanisms exist to assist users with these routine tasks ; bookmarks or portal sites must be maintained manually and are insensitive to the user 's browsing context . To fill this void , we designed and implemented the montage system . A web montage is an ensemble of links and content fused into a single view . Such a coalesced view can be presented to the user whenever he or she opens the browser or returns to the start page . We pose a number of hypotheses about how users would interact with such a system , and test these hypotheses with a fielded user study . Our findings support some design decisions , such as using browsing context to tailor the montage , raise questions about others , and point the way toward future work ."
"638","Mining clickthrough data for collaborative web search This paper is to investigate the group behavior patterns of search activities based on Web search history data , i.e. , clickthrough data , to boost search performance . We propose a Collaborative Web Search ( CWS ) framework based on the probabilistic modeling of the co-occurrence relationship among the heterogeneous web objects : users , queries , and Web pages . The CWS framework consists of two steps : ( 1 ) a cube-clustering approach is put forward to estimate the semantic cluster structures of the Web objects ; ( 2 ) Web search activities are conducted by leveraging the probabilistic relations among the estimated cluster structures . Experiments on a real-world clickthrough data set validate the effectiveness of our CWS approach ."
"639","Detecting semantic cloaking on the web By supplying different versions of a web page to search engines and to browsers , a content provider attempts to cloak the real content from the view of the search engine . Semantic cloaking refers to differences in meaning between pages which have the effect of deceiving search engine ranking algorithms . In this paper , we propose an automated two-step method to detect semantic cloaking pages based on different copies of the same page downloaded by a web crawler and a web browser . The first step is a filtering step , which generates a candidate list of semantic cloaking pages . In the second step , a classifier is used to detect semantic cloaking pages from the candidates generated by the filtering step . Experiments on manually labeled data sets show that we can generate a classifier with a precision of 93 % and a recall of 85 % . We apply our approach to links from the dmoz Open Directory Project and estimate that more than 50,000 of these pages employ semantic cloaking ."
"640","Efficient search engine measurements We address the problem of measuring global quality met-rics of search engines , like corpus size , index freshness , anddensity of duplicates in the corpus . The recently proposedestimators for such metrics ( 2 , 6 ) suffer from significant biasand\/or poor performance , due to inaccurate approximationof the so called . document degrees . . We present two new estimators that are able to overcomethe bias introduced by approximate degrees . Our estimatorsare based on a careful implementation of an approximateimportance sampling procedure . Comprehensive theoreti-cal and empirical analysis of the estimators demonstratesthat they have essentially no bias even in situations wheredocument degrees are poorly approximated . Building on an idea from ( 6 ) , we discuss Rao Blackwelliza-tion as a generic method for reducing variance in searchengine estimators . We show that Rao-Blackwellizing ourestimators results in significant performance improvements , while not compromising accuracy ."
"641","Efficient search for peer-to-peer information retrieval using semantic small world This paper proposes a semantic overlay based on the small world phenomenon that facilitates efficient search for information retrieval in unstructured P2P systems . In the semantic overlay , each node maintains a number of short-range links which are semantically similar to each other , together with a small collection of long-range links that help increasing recall rate of information retrieval and reduce network traffic as well . Experimental results show that our model can improve performance by 150 % compared to Gnutella and by up to 60 % compared to the Interest-based model - a similar shortcut-based search technique ."
"642","Browsing on small screens : recasting web-page segmentation into an efficient machine learning framework Fitting enough information from webpages to make browsing on small screens compelling is a challenging task . One approach is to present the user with a thumbnail image of the full web page and allow the user to simply press a single key to zoom into a region ( which may then be transcoded into wml\/xhtml , summarized , etc ) . However , if regions for zooming are presented naively , this yields a frustrating experience because of the number of coherent regions , sentences , images , and words that may be inadvertently separated . Here , we cast the web page segmentation problem into a machine learning framework , where we re-examine this task through the lens of entropy reduction and decision tree learning . This yields an efficient and effective page segmentation algorithm . We demonstrate how simple techniques from computer vision can be used to fine-tune the results . The resulting segmentation keeps coherent regions together when tested on a broad set of complex webpages ."
"643","pTHINC : a thin-client architecture for mobile wireless web Although web applications are gaining popularity on mobile wireless PDAs , web browsers on these systems can be quite slow and often lack adequate functionality to access many web sites . We have developed pTHINC , a PDA thin-client solution that leverages more powerful servers to run full-function web browsers and other application logic , then sends simple screen updates to the PDA for display . pTHINC uses server-side screen scaling to provide high-fidelity display and seamless mobility across a broad range of different clients and screen sizes , including both portrait and landscape viewing modes . pTHINC also leverages existing PDA control buttons to improve system usability and maximize available screen resolution for application display . We have implemented pTHINC on Windows Mobile and evaluated its performance on mobile wireless devices . Our results compared to local PDA web browsers and other thin-client approaches demonstrate that pTHINC provides superior web browsing performance and is the only PDA thin client that effectively supports crucial browser helper applications such as video playback ."
"644","Large-scale text categorization by batch mode active learning Large-scale text categorization is an important research topic for Web data mining . One of the challenges in large-scale text categorization is how to reduce the human efforts in labeling text documents for building reliable classification models . In the past , there have been many studies on applying active learning methods to automatic text categorization , which try to select the most informative documents for labeling manually . Most of these studies focused on selecting a single unlabeled document in each iteration . As a result , the text categorization model has to be retrained after each labeled document is solicited . In this paper , we present a novel active learning algorithm that selects a batch of text documents for labeling manually in each iteration . The key of the batch mode active learning is how to reduce the redundancy among the selected examples such that each example provides unique information for model updating . To this end , we use the Fisher information matrix as the measurement of model uncertainty and choose the set of documents to effectively maximize the Fisher information of a classification model . Extensive experiments with three different datasets have shown that our algorithm is more effective than the state-of-the-art active learning techniques for text categorization and can be a promising tool toward large-scale text categorization for World Wide Web documents ."
"645","Preference-based selection of highly configurable web services A key challenge for dynamic Web service selection is that Web services are typically highly configurable and service requesters often have dynamic preferences on service configurations . Current approaches , such as WS-Agreement , describe Web services by enumerating the various possible service configurations , an inefficient approach when dealing with numerous service attributes with large value spaces . We model Web service configurations and associated prices and preferences more compactly using utility function policies , which also allows us to draw from multi-attribute decision theory methods to develop an algorithm for optimal service selection . In this paper , we present an OWL ontology for the specification of configurable Web service offers and requests , and a flexible and extensible framework for optimal service selection that combines declarative logic-based matching rules with optimization methods , such as linear programming . Assuming additive price\/preference functions , experimental results indicate that our algorithm introduces an overhead of only around 2 sec . ~ compared to random service selection , while giving optimal results . The overhead , as percentage of total time , decreases as the number of offers and configurations increase ."
"646","Semantic WS-agreement partner selection In a dynamic service oriented environment it is desirable for service consumers and providers to offer and obtain guarantees regarding their capabilities and requirements . WS-Agreement defines a language and protocol for establishing agreements between two parties . The agreements are complex and expressive to the extent that the manual matching of these agreements would be expensive both in time and resources . It is essential to develop a method for matching agreements automatically . This work presents the framework and implementation of an innovative tool for the matching providers and consumers based on WS-Agreements . The approach utilizes Semantic Web technologies to achieve rich and accurate matches . A key feature is the novel and flexible approach for achieving user personalized matches ."
"647","XVM : a bridge between xml data and its behavior XML has become one of the core technologies for contemporary business applications , especially web-based applications . To facilitate processing of diverse XML data , we propose an extensible , integrated XML processing architecture , the XML Virtual Machine ( XVM ) , which connects XML data with their behaviors . At the same time , the XVM is also a framework for developing and deploying XML-based applications . Using component-based techniques , the XVM supports arbitrary granularity and provides a high degree of modularity and reusability . XVM components are dynamically loaded and composed during XML data processing . Using the XVM , both client-side and server-side XML applications can be developed and deployed in an integrated way . We also present an XML application container built on top of the XVM along with several sample applications to demonstrate the applicability of the XVM framework ."
"648","Efficient and robust streaming provisioning in VPNs Today , most large companies maintain virtual private networks ( VPNs ) to connect their remote locations into a single secure network . VPNs can be quite large covering more than 1000 locations and in most cases use standard Internet protocols and services . Such VPNs are implemented using a diverse set of technologies such as Frame Relay , MPLS , or IPSEC to achieve the goal of privacy and performance isolation from the public Internet . Using VPNs to distribute live content has recently received tremendous interest . For example , a VPN could be used to broadcast a CEO-employee town hall meeting . To distribute this type of content economically without overloading the network , the deployment of streaming caches or splitters is most likely required . In this paper , we address the problem of optimally placing such streaming splitters or caches to broadcast to a given set of VPN endpoints under the constraints typically found within a VPN . In particular , we introduce an efficient algorithm with complexity O ( V ) , V being the number of routers in the VPN . This guarantees the optimal cache placement if interception is used for redirection . We prove that the general problem is NP-hard and introduce multiple heuristics for efficient and robust cache placement suitable under different constraints . At the expense of increased implementation complexity , each heuristic solution provides additional saving in the number of caches required . We evaluate proposed solutions using extensive simulations . In particular , we show our flow-based solution is very close to the optimal ."
"649","Robust methodologies for modeling web click distributions Metrics such as click counts are vital to online businesses but their measurement has been problematic due to inclusion of high variance robot traffic . We posit that by applying statistical methods more rigorous than have been employed to date that we can build a robust model of thedistribution of clicks following which we can set probabilistically sound thresholds to address outliers and robots . Prior research in this domain has used inappropriate statistical methodology to model distributions and current industrial practice eschews this research for conservative ad-hoc click-level thresholds . Prevailing belief is that such distributions are scale-free power law distributions but using more rigorous statistical methods we find the best description of the data is instead provided by a scale-sensitive Zipf-Mandelbrot mixture distribution . Our results are based on ten data sets from various verticals in the Yahoo domain . Since mixture models can overfit the data we take care to use the BIC log-likelihood method which penalizes overly complex models . Using a mixture model in the web activity domain makes sense because there are likely multiple classes of users . In particular , we have noticed that there is a significantly large set of `` users '' that visit the Yahoo portal exactly once a day . We surmise these may be robots testing internet connectivity by pinging the Yahoo main website . Backing up our quantitative analysis is graphical analysis in which empirical distributions are plotted against heoretical distributions in log-log space using robust cumulative distribution plots . This methodology has two advantages : plotting in log-log space allows one to visually differentiate the various exponential distributions and secondly , cumulative plots are much more robust to outliers . We plan to use the results of this work for applications for robot removal from web metrics business intelligence systems ."
"650","A content-driven reputation system for the wikipedia We present a content-driven reputation system for Wikipedia authors . In our system , authors gain reputation when the edits they perform to Wikipedia articles are preserved by subsequent authors , and they lose reputation when their edits are rolled back or undone in short order . Thus , author reputation is computed solely on the basis of content evolution ; user-to-user comments or ratings are not used . The author reputation we compute could be used to flag new contributions from low-reputation authors , or it could be used to allow only authors with high reputation to contribute to controversialor critical pages . A reputation system for the Wikipedia could also provide an incentive for high-quality contributions . We have implemented the proposed system , and we have used it to analyze the entire Italian and French Wikipedias , consisting of a total of 691 , 551 pages and 5 , 587 , 523 revisions . Our results show that our notion of reputation has good predictive value : changes performed by low-reputation authors have a significantly larger than average probability of having poor quality , as judged by human observers , and of being later undone , as measured by our algorithms ."
"651","On anonymizing query logs via token-based hashing In this paper we study the privacy preservation properties of aspecific technique for query log anonymization : token-based hashing . In this approach , each query is tokenized , and then a secure hash function is applied to each token . We show that statistical techniques may be applied to partially compromise the anonymization . We then analyze the specific risks that arise from these partial compromises , focused on revelation of identity from unambiguous names , addresses , and so forth , and the revelation of facts associated with an identity that are deemed to be highly sensitive . Our goal in this work is two fold : to show that token-based hashing is unsuitable for anonymization , and to present a concrete analysis of specific techniques that may be effective in breaching privacy , against which other anonymization schemes should be measured ."
"652","Page-level template detection via isotonic smoothing We develop a novel framework for the page-level template detection problem . Our framework is built on two main ideas . The first is theautomatic generation of training data for a classifier that , given apage , assigns a templateness score to every DOM node of the page . The second is the global smoothing of these per-node classifier scores bysolving a regularized isotonic regression problem ; the latter follows from a simple yet powerful abstraction of templateness on a page . Our extensive experiments on human-labeled test data show that our approachdetects templates effectively ."
"653","Living the TV revolution : unite MHP to the web or face IDTV irrelevance ! The union of Interactive Digital TV ( IDTV ) and Web promotes the development of new interactive multimedia services , enjoyable while watching TV even on the new handheld digital TV receivers . Yet , several design constraints complicate the deployment of this new pattern of services . Indeed , for a suitable presentation on a TV set , Web contents must be structured in such a way that they can be effectively displayed on TV screens via low-end Set Top Boxes ( STBs ) . Moreover , usable interfaces for IDTV platforms are needed which ensure a smooth access to contents . Our claim is that the distribution of Web contents over the IDTV broadcast channels may bring IDTV to a new life . A failure of this attempt may put IDTV on a progressive track towards irrelevance . We propose a system for the distribution of Web contents towards IDTV under the Digital Video Broadcasting - Multimedia Home Platform ( DVB-MHP ) standard . Our system is able to automatically transcode Web contents and ensure a proper visualization on IDTV . The system is endowed with a client application which permits to easily browse contents on the TV via a remote control . Real assessments have confirmed the effectiveness for such an automatic online service able to reconfigure Web contents for an appropriate distribution and presentation on IDTV ."
"654","Anchor-based proximity measures We present a family of measures of proximity of an arbitrary node in a directed graph to a pre-specified subset of nodes , called the anchor . Our measures are based on three different propagation schemesand two different uses of the connectivity structure of the graph . We consider a web-specific application of the above measures with two disjoint anchors - good and bad web pages - and study the accuracy of these measures in this context ."
"655","Propagation of trust and distrust A ( directed ) network of people connected by ratings or trust scores , and a model for propagating those trust scores , is a fundamental building block in many of today 's most successful e-commerce and recommendation systems . We develop a framework of trust propagation schemes , each of which may be appropriate in certain circumstances , and evaluate the schemes on a large trust network consisting of 800K trust scores expressed among 130K people . We show that a small number of expressed trusts\/distrust per individual allows us to predict trust between any two people in the system with high accuracy . Our work appears to be the first to incorporate distrust in a computational trust propagation setting ."
"656","Proximity within paragraph : a measure to enhance document retrieval performance We created a proximity measure , called Proximity Within Paragraph ( PWP ) , which is based on the concept of value assignment to queried words , grouped by associated ideas within paragraphs . Based on the WT10G dataset , a test system comprising three test sets and fifty queries were constructed to evaluate the effectiveness of PWP by comparing it with the existing method : Minimum Distance Between Queried Pairs . A further experiment combines the scores obtained from both methods and the results suggest that the combination can significantly improve the effectiveness ."
"657","Characterization of a large web site population with implications for content delivery This paper presents a systematic study of the properties of a large number of Web sites hosted by a major ISP . To our knowledge , ours is the first comprehensive study of a large server farm that contains thousands of commercial Web sites . We also perform a simulation analysis to estimate potential performance benefits of content delivery networks ( CDNs ) for these Web sites . We make several interesting observations about the current usage of Web technologies and Web site performance characteristics . First , compared with previous client workload studies , the Web server farm workload contains a much higher degree of uncacheable responses and responses that require mandatory cache validations . A significant reason for this is that cookie use is prevalent among our population , especially among more popular sites . However , we found an indication of wide-spread indiscriminate usage of cookies , which unnecessarily impedes the use of many content delivery optimizations . We also found that most Web sites do not utilize the cache-control features ofthe HTTP 1.1 protocol , resulting in suboptimal performance . Moreover , the implicit expiration time in client caches for responses is constrained by the maximum values allowed in the Squid proxy . Finally , our simulation results indicate that most Web sites benefit from the use of a CDN . The amount of the benefit depends on site popularity , and , somewhat surprisingly , a CDN may increase the peak to average request ratio at the origin server because the CDN can decrease the average request rate more than the peak request rate ."
"658","Clarifying the fundamentals of HTTP The simplicity of HTTP was a major factor in the success of the Web . However , as both the protocol and its uses have evolved , HTTP has grown complex . This complexity results in numerous problems , including confused implementors , interoperability failures , difficulty in extending the protocol , and a long specification without much documented rationale . Many of the problems with HTTP can be traced to unfortunate choices about fundamental definitions and models . This paper analyzes the current ( HTTP\/1 .1 ) protocol design , showing how it fails in certain cases , and how to improve these fundamentals . Some problems with HTTP can be fixed simply by adopting new models and terminology , allowing us to think more clearly about implementations and extensions . Other problems require explicit ( but compatible ) protocol changes ."
"659","Optimizing web search using social annotations This paper explores the use of social annotations to improve websearch . Nowadays , many services , e.g. del. icio . us , have been developed for web users to organize and share their favorite webpages on line by using social annotations . We observe that the social annotations can benefit web search in two aspects : 1 ) the annotations are usually good summaries of corresponding webpages ; 2 ) the count of annotations indicates the popularity of webpages . Two novel algorithms are proposed to incorporate the above information into page ranking : 1 ) SocialSimRank ( SSR ) calculates the similarity between social annotations and webqueries ; 2 ) SocialPageRank ( SPR ) captures the popularity of webpages . Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations , while SPR successfully measures the quality ( popularity ) of a webpage from the web users ' perspective . We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious . Experiments show that both SSR and SPRbenefit web search significantly ."
"660","POLYPHONET : an advanced social network extraction system from the web Social networks play important roles in the Semantic Web : knowledge management , information retrieval , ubiquitous computing , and so on . We propose a social network extraction system called POLYPHONET , which employs several advanced techniques to extract relations of persons , detect groups of persons , and obtain keywords for a person . Search engines , especially Google , are used to measure co-occurrence of information and obtain Web documents . Several studies have used search engines to extract social networks from the Web , but our research advances the following points : First , we reduce the related methods into simple pseudocodes using Google so that we can build up integrated systems . Second , we develop several new algorithms for social networking mining such as those to classify relations into categories , to make extraction scalable , and to obtain and utilize person-to-word relations . Third , every module is implemented in POLYPHONET , which has been used at four academic conferences , each with more than 500 participants . We overview that system . Finally , a novel architecture called Super Social Network Mining is proposed ; it utilizes simple modules using Google and is characterized by scalability and Relate-Identify processes : Identification of each entity and extraction of relations are repeated to obtain a more precise social network ."
"661","Web log mining with adaptive support thresholds With the fast increase in Web activities , Web data mining has recently become an important research topic . However , most previous studies of mining path traversal patterns are based on the model of a uniform support threshold without taking into consideration such important factors as the length of a pattern , the positions of Web pages , and the importance of a particular pattern , etc. . In view of this , we study and apply the Markov chain model to provide the determination of support threshold of Web documents . Furthermore , by properly employing some techniques devised for joining reference sequences , a new mining procedure of Web traversal patterns is proposed in this paper ."
"662","Efficient search in large textual collections with redundancy Current web search engines focus on searching only themost recentsnapshot of the web . In some cases , however , it would be desirableto search over collections that include many different crawls andversions of each page . One important example of such a collectionis the Internet Archive , though there are many others . Sincethe data size of such an archive is multiple times that of a singlesnapshot , this presents us with significant performance challenges . Current engines use various techniques for index compression andoptimized query execution , but these techniques do not exploit thesignificant similarities between different versions of a page , or betweendifferent pages . In this paper , we propose a general framework for indexing andquery processing of archival collections and , more generally , anycollections with a sufficient amount of redundancy . Our approachresults in significant reductions in index size and query processingcosts on such collections , and it is orthogonal to and can be combinedwith the existing techniques . It also supports highly efficientupdates , both locally and over a network . Within this framework , we describe and evaluate different implementations that trade offindex size versus CPU cost and other factors , and discuss applicationsranging from archival web search to local search of web sites , email archives , or file systems . We present experimental resultsbased on search engine query log and a large collection consistingof multiple crawls ."
"663","The credibility of the posted information in a recommendation system based on a map We propose a method for estimating the credibility of the posted information from users . The system displays these information on the map . Since posted information can include subjective information from various perspectives , we ca n't trust all of the postings as they are . We propose and integrate factors of the user 's geographic posting tendency and votes by other users ."
"664","Detecting nepotistic links by language model disagreement In this short note we demonstrate the applicability of hyperlink downweighting by means of language model disagreement . The method filters out hyperlinks with no relevance to the target page without the need of white and blacklists or human interaction . We fight various forms of nepotism such as common maintainers , ads , link exchanges or misused affiliate programs . Our method is tested on a 31 M page crawl of the . de domain with a manually classified 1000-page random sample ."
"665","Hierarchical , perceptron-like learning for ontology-based information extraction Recent work on ontology-based Information Extraction ( IE ) has tried to make use of knowledge from the target ontology in order to improve semantic annotation results . However , very few approaches exploit the ontology structure itself , and those that do so , have some limitations . This paper introduces a hierarchical learning approach for IE , which uses the target ontology as an essential part of the extraction process , by taking into account the relations between concepts . The approach is evaluated on the largest available semantically annotated corpus . The results demonstrate clearly the benefits of using knowledge from the ontology as input to the information extraction process . We also demonstrate the advantages of our approach over other state-of-the-art learning systems on a commonly used benchmark dataset ."
"666","The SOWES approach to P2P web search using semantic overlays Peer-to-peer ( P2P ) Web search has gained a lot of interest lately , due to the salient characteristics of P2P systems , namely scalability , fault-tolerance and load-balancing . However , the lack of global knowledge in a vast and dynamically evolving environment like the Web presents a grand challenge for organizing content and providing efficient searching . Semantic overlay networks ( SONs ) have been proposed as an approach to reduce cost and increase quality of results , and in this paper we present an unsupervised approach for distributed and decentralized SON construction , aiming to support efficient search mechanisms in unstructured P2P systems ."
"667","Topic sentiment mixture : modeling facets and opinions in weblogs In this paper , we define the problem of topic-sentiment analysis on Weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneously . The proposed Topic-Sentiment Mixture ( TSM ) model can reveal the latent topical facets in a Weblog collection , the subtopics in the results of an ad hoc query , and their associated sentiments . It could also provide general sentiment models that are applicable to any ad hoc topics . With a specifically designed HMM structure , the sentiment models and topic models estimated with TSM can be utilized to extract topic life cycles and sentiment dynamics . Empirical experiments on different Weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from Weblog collections . The TSM model is quite general ; it can be applied to any text collections with a mixture of topics and sentiments , thus has many potential applications , such as search result summarization , opinion tracking , and user behavior prediction ."
"668","Detectives : detecting coalition hit inflation attacks in advertising networks streams Click fraud is jeopardizing the industry of Internet advertising . Internet advertising is crucial for the thriving of the entire Internet , since it allows producers to advertise their products , and hence contributes to the well being of e-commerce . Moreover , advertising supports the intellectual value of the Internet by covering the running expenses of publishing content . Some content publishers are dishonest , and use automation to generate traffic to defraud the advertisers . Similarly , some advertisers automate clicks on the advertisements of their competitors to deplete their competitors ' advertising budgets . This paper describes the advertising network model , and focuses on the most sophisticated type of fraud , which involves coalitions among fraudsters . We build on several published theoretical results to devise the Similarity-Seeker algorithm that discovers coalitions made by pairs of fraudsters . We then generalize the solution to coalitions of arbitrary sizes . Before deploying our system on a real network , we conducted comprehensive experiments on data samples for proof of concept . The results were very accurate . We detected several coalitions , formed using various techniques , and spanning numerous sites . This reveals the generality of our model and approach ."
"669","Explorations in the use of semantic web technologies for product information management Master data refers to core business entities a company uses repeatedly across many business processes and systems ( such as lists or hierarchies of customers , suppliers , accounts , products , or organizational units ) . Product information is the most important kind of master data and product information management ( PIM ) is becoming critical for modern enterprises because it provides a rich business context for various applications . Existing PIM systems are less flexible and scalable for on-demand business , as well as too weak to completely capture and use the semantics of master data . This paper explores how to use semantic web technologies to enhance a collaborative PIM system by simplifying modeling and representation while preserving enough dynamic flexibility . Furthermore , we build a semantic PIM system using one of the state-of-art ontology repositories and summarize the challenges we encountered based on our experimental results , especially on performance and scalability . We believe that our study and experiences are valuable for both semantic web community and master data management community ."
"670","Extensible schema documentation with XSLT 2.0 XML Schema documents are defined using an XML syntax , which means that the idea of generating schema documentation through standard XML technologies is intriguing . We present X2Doc , a framework for generating schema-documentation solely through XSLT . The framework uses SCX , an XML syntax for XML Schema components , as intermediate format and produces XML-based output formats . Using a modular set of XSLT stylesheets , X2Doc is highly configurable and carefully crafted towards extensibility . This proves especially useful for composite schemas , where additional schema information like Schematron rules are embedded into XML Schemas ."
"671","Search engine retrieval of changing information In this paper we analyze the Web coverage of three search engines , Google , Yahoo and MSN . We conducted a 15 month study collecting 15,770 Web content or information pages linked from 260 Australian federal and local government Web pages . The key feature of this domain is that new information pages are constantly added but the 260 web pages tend to provide links only to the more recently added information pages . Search engines list only some of the information pages and their coverage varies from month to month . Meta-search engines do little to improve coverage of information pages , because the problem is not the size of web coverage , but the frequency with which information is updated . We conclude that organizations such as governments which post important information on the Web can not rely on all relevant pages being found with conventional search engines , and need to consider other strategies to ensure important information can be found ."
"672","Finding specification pages according to attributes This paper presents a method for finding a specification page on the web for a given object ( e.g. `` Titanic ö ) and its class label ( e.g. `` film ö ) . A specification page for an object is a web page which gives concise attribute-value information about the object ( e.g. `` director ö - `` James Cameron öfor `` Titanic ö ) . A simple unsupervised method using layout and symbolic decoration cues was applied to a large number of web pages to acquire the class attributes . We used these acquired attributes to select a representative specification page for a given object from the web pages retrieved by a normal search engine . Experimental results revealed that our method greatly outperformed the normal search engine in terms of specification retrieval ."
"673","Answering relationship queries on the web Finding relationships between entities on the Web , e.g. , the connections between different places or the commonalities of people , is a novel and challenging problem . Existing Web search engines excel in keyword matching and document ranking , but they can not well handle many relationship queries . This paper proposes a new method for answering relationship queries on two entities . Our method first respectively retrieves the top Web pages for either entity from a Web search engine . It then matches these Web pages and generates an ordered list of Web page pairs . Each Web page pair consists of one Web page for either entity . The top ranked Web page pairs are likely to contain the relationships between the two entities . One main challenge in the ranking process is to effectively filter out the large amount of noise in the Web pages without losing much useful information . To achieve this , our method assigns appropriate weights to terms in Web pages and intelligently identifies the potential connecting terms that capture the relationships between the two entities . Only those top potential connecting terms with large weights are used to rank Web page pairs . Finally , the top ranked Web page pairs are presented to the searcher . For each such pair , the query terms and the top potential connecting terms are properly highlighted so that the relationships between the two entities can be easily identified . We implemented a prototype on top of the Google search engine and evaluated it under a wide variety of query scenarios . The experimental results show that our method is effective at finding important relationships with low overhead ."
"674","Dynamics of bid optimization in online advertisement auctions We consider the problem of online keyword advertising auctions among multiple bidders with limited budgets , and study a natural bidding heuristic in which advertisers attempt to optimize their utility by equalizing their return-on-investment across all keywords . We show that existing auction mechanisms combined with this heuristic can experience cycling ( as has been observed in many current systems ) , and therefore propose a modified class of mechanisms with small random perturbations . This perturbation is reminiscent of the small time-dependent perturbations employed in the dynamical systems literature to convert many types of chaos into attracting motions . We show that the perturbed mechanism provably converges in the case of first-price auctions and experimentally converges in the case of second-price auctions . Moreover , the point of convergence has a natural economic interpretation as the unique market equilibrium in the case of first-price mechanisms . In the case of second-price auctions , we conjecture that it converges to the `` supply-aware '' market equilibrium . Thus , our results can be alternatively described as a tâtonnement process for convergence to market equilibriumin which prices are adjusted on the side of the buyers rather than the sellers . We also observe that perturbation in mechanism design is useful in a broader context : In general , it can allow bidders to `` share '' a particular item , leading to stable allocations and pricing for the bidders , and improved revenue for the auctioneer ."
"675","Semantic personalization of web portal contents Enriching Web applications with personalized data is of major interest for facilitating the user access to the published contents , and therefore , for guaranteeing successful user navigation . We propose a conceptual model for extracting personalized recommendations based on user profiling , ontological domain models , and semantic reasoning . The approach offers a high-level representation of the designed application based on a domain-specific metamodel for Web applications called WebML ."
"676","Brand awareness and the evaluation of search results We investigate the effect of search engine brand ( i.e. , the identifying name or logo that distinguishes a product from its competitors ) on evaluation of system performance . This research is motivated by the large amount of search traffic directed to a handful of Web search engines , even though most are of equal technical quality with similar interfaces . We conducted a laboratory study with 32 participants to measure the effect of four search engine brands while controlling for the quality of search engine results . There was a 25 % difference between the most highly rated search engine and the lowest using average relevance ratings , even though search engine results were identical in both content and presentation . Qualitative analysis suggests branding affects user views of popularity , trust and specialization . We discuss implications for search engine marketing and the design of search engine quality studies ."
"677","A new suffix tree similarity measure for document clustering In this paper , we propose a new similarity measure to compute the pairwise similarity of text-based documents based on suffix tree document model . By applying the new suffix tree similarity measure in Group-average Agglomerative Hierarchical Clustering ( GAHC ) algorithm , we developed a new suffix tree document clustering algorithm ( NSTC ) . Experimental results on two standard document clustering benchmark corpus OHSUMED and RCV1 indicate that the new clustering algorithm is a very effective document clustering algorithm . Comparing with the results of traditional word term weight tf-idf similarity measure in the same GAHC algorithm , NSTC achieved an improvement of 51 % on the average of F-measure score . Furthermore , we apply the new clustering algorithm in analyzing the Web documents in online forum communities . A topic oriented clustering algorithm is developed to help people in assessing , classifying and searching the the Web documents in a large forum community ."
"678","A decentralized CF approach based on cooperative agents In this paper , we propose a decentralized collaborative filtering ( CF ) approach based on P2P overlay network for the autonomous agents ' environment . Experiments show that our approach is more scalable than traditional centralized CF filtering systems and alleviates the sparsity problem in distributed CF."
"679","Adaptive record extraction from web pages We describe an adaptive method for extracting records from web pages . Our algorithm combines a weighted tree matching metric with clustering for obtaining data extraction patterns . We compare our method experimentally to the state-of-the-art , and show that our approach is very competitive for rigidly-structured records ( such as product descriptions ) and far superior for loosely-structured records ( such as entrieson blogs ) ."
"680","A browser for browsing the past web We describe a browser for the past web . It can retrieve data from multiple past web resources and features a passive browsing style based on change detection and presentation . The browser shows past pages one by one along a time line . The parts that were changed between consecutive page versions are animated to reflect their deletion or insertion , thereby drawing the user 's attention to them . The browser enables automatic skipping of changeless periods and filtered browsing based on user specified query ."
"681","Navigating the intranet with high precision Despite the success of web search engines , search over large enterprise intranets still suffers from poor result quality . Earlier work ( 6 ) that compared intranets and the Internet from the view point of keyword search has pointed to several reasons why the search problem is quite different in these two domains . In this paper , we address the problem of providing high quality answers to navigational queries in the intranet ( e.g. , queries intended to find product or personal home pages , service pages , etc. ) . Our approach is based on offline identification of navigational pages , intelligent generation of term-variants to associate with each page , and the construction of separate indices exclusively devoted to answering navigational queries . Using a testbed of 5.5 M pages from the IBM intranet , we present evaluation results that demonstrate that for navigational queries , our approach of using custom indices produces results of significantly higher precision than those produced by a general purpose search algorithm ."
"682","ASDL : a wide spectrum language for designing web services A Service oriented system emerges from composition of services . Dynamically composed reactive Web services form a special class of service oriented system , where the delays associated with communication , unreliability and unavailability of services , and competition for resources from multiple service requesters are dominant concerns . As complexity of services increase , an abstract design language for the specification of services and interaction between them is desired . In this paper , we present ASDL ( Abstract Service Design Language ) , a wide spectrum language for modelling Web services . We initially provide an informal description of our computational model for service oriented systems . We then present ASDL along with its specification oriented semantics defined in Interval Temporal Logic ( ITL ) : a sound formalism for specifying and reasoning about temporal properties of systems . The objective of ASDL is to provide a notation for the design of service composition and interaction protocols at an abstract level ."
"683","Discovering the best web service Major research challenges in discovering Web services include , provisioning of services across multiple or heterogeneous registries , differentiating between services that share similar functionalities , improving end-to-end Quality of Service ( QoS ) , and enabling clients to customize the discovery process . Proliferation and interoperability of this multitude of Web services have lead to the emergence of new standards on how services can be published , discovered , or used ( i.e. UDDI , WSDL , SOAP ) . Such standards can potentially provide many of these features and much more , however , there are technical challenges associated with existing standards . One of these challenges is the client . s ability to control the discovery process across accessible service registries for finding services of interest . This work proposes a solution to this problem and introduces the Web Service Relevancy Function ( WsRF ) used for measuring the relevancy ranking of a particular Web service based on QoS metrics and client preferences . We present experimental validation , results , and analysis of the presented ideas ."
"684","Estimating the cardinality of RDF graph patterns Most RDF query languages allow for graph structure search through a conjunction of triples which is typically processed using join operations . A key factor in optimizing joins is determining the join order which depends on the expected cardinality of intermediate results . This work proposes a pattern-based summarization framework for estimating the cardinality of RDF graph patterns . We present experiments on real world and synthetic datasets which confirm the feasibility of our approach ."
"685","Comparing apples and oranges : normalized pagerank for evolving graphs PageRank is the best known technique for link-based importance ranking . The computed importance scores , however , are not directly comparable across different snapshots of an evolving graph . We present an efficiently computable normalization for PageRank scores that makes them comparable across graphs . Furthermore , we show that the normalized PageRank scores are robust to non-local changes in the graph , unlike the standard PageRank measure ."
"686","A query algebra for xml p2p databases This paper describes a query algebra for queries over XML p2p databases that provides explicit mechanisms for modeling data dissemination , replication constraints , and for capturing the transient nature of data and replicas ."
"687","Expertise networks in online communities : structure and algorithms Web-based communities have become important places for people to seek and share expertise . We find that networks in these communities typically differ in their topology from other online networks such as the World Wide Web . Systems targeted to augment web-based communities by automatically identifying users with expertise , for example , need to adapt to the underlying interaction dynamics . In this study , we analyze the Java Forum , a large online help-seeking community , using social network analysis methods . We test a set of network-based ranking algorithms , including PageRank and HITS , on this large size social network in order to identify users with high expertise . We then use simulations to identify a small number of simple simulation rules governing the question-answer dynamic in the network . These simple rules not only replicate the structural characteristics and algorithm performance on the empirically observed Java Forum , but also allow us to evaluate how other algorithms may perform in communities with different characteristics . We believe this approach will be fruitful for practical algorithm design and implementation for online expertise-sharing communities ."
"688","Finding visual concepts by web image mining We propose measuring `` visualness '' of concepts with images on the Web , that is , what extent concepts have visual characteristics . This is a new application of `` Web image mining '' . To know which concept has visually discriminative power is important for image recognition , since not all concepts are related to visual contents . Mining image data on the Web with our method enables it . Our method performs probabilistic region selection for images and computes an entropy measure which represents `` visualness '' of concepts . In the experiments , we collected about forty thousand images from the Web for 150 concepts . We examined which concepts are suitable for annotation of image contents ."
"689","Long distance wireless mesh network planning : problem formulation and solution Several research efforts as well as deployments have chosen IEEE802 .11 as a low-cost , long-distance access technology to bridge the digital divide . In this paper , we consider the important issue of planning such networks to the minimize system cost . This is a non-trivial task since it involves several sets of variables : the network topology , tower heights , antenna types to be used and the irorientations , and radio transmit powers . The task is further complicated due to the presence of network performance constraints , and the inter-dependence among the variables . Our first contribution in this paper is the formulation of this problem in terms of the variables , constraints and the optimization criterion . Our second contribution is in identifying the dependencies among the variables and breaking-down the problem into four tractable sub-parts . In this process , we extensively use domain knowledge to strike a balance between tractability and practicality . We have evaluated the proposed algorithms using random input sets as well as real-life instances with success . We have been able to show detailed planning of network topology , required tower heights , antenna types , and transmit powers for the Ashwini project , a long distance WiFi network under deployment in Andhra Pradesh , India , In this case , we are able to achieve within 2 % additional cost of a lower bound estimate ."
"690","Migrating web application sessions in mobile computing The capability to change user agent while working is starting to appear in state of the art mobile computing due to the proliferation of different kinds of devices , ranging from personal wireless devices to desktop computers , and to the consequent necessity of migrating working sessions from a device to a more apt one . Research results related to the hand-off at low level are not sufficient to solve the problem at application level . The paper presents a scheme for session hand-off in Web applications which , by exploiting a proxy-based architecture , is able to work without interventions on existing code ."
"691","A novel clustering-based RSS aggregator In recent years , different commercial Weblog subscribing systems have been proposed to return stories from users . subscribed feeds . In this paper , we propose a novel clustering-based RSS aggregator called as RSS Clusgator System ( RCS ) for Weblog reading . Note that an RSS feed may have several different topics . A user may only be interested in a subset of these topics . In addition there could be many different stories from multiple RSS feeds , which discuss similar topic from different perspectives . A user may be interested in this topic but do not know how to collect all feeds related to this topic . In contrast to many previous works , we cluster all stories in RSS feeds into hierarchical structure to better serve the readers . Through this way , users can easily find all their interested stories . To make the system current , we propose a flexible time window for incremental clustering . RCS utilizes both link information and content information for efficient clustering . Experiments show the effectiveness of RCS ."
"692","Clustering for probabilistic model estimation for CF Based on the type of collaborative objects , a collaborative filtering ( CF ) system falls into one of two categories : item-based CF and user-based CF. Clustering is the basic idea in both cases , where users or items are classified into user groups where users share similar preference or item groups where items have similar attributes or characteristics . Observing the fact that in user-based CF each user community is characterized by a Gaussian distribution on the ratings for each item and the fact that in item-based CF the ratings of each user in item community satisfy a Gaussian distribution , we propose a method of probabilistic model estimation for CF , where objects ( user or items ) are classified into groups based on the content information and ratings at the same time and predictions are made considering the Gaussian distribution of ratings . Experiments on a real-world data set illustrate that our approach is favorable ."
"693","Mapping-driven XML transformation Clio is an existing schema-mapping tool that provides user-friendly means to manage and facilitate the complex task of transformation and integration of heterogeneous data such as XML over the Web or in XML databases . By means of mappings from source to target schemas , Clio can help users conveniently establish the precise semantics of data transformation and integration . In this paper we study the problem of how to efficiently implement such data transformation ( i.e. , generating target data from the source data based on schema mappings ) . We present a three-phase framework for high-performance XML-to-XML transformation based on schema mappings , and discuss methodologies and algorithms for implementing these phases . In particular , we elaborate on novel techniques such as streamed extraction of mapped source values and scalable disk-based merging of overlapping data ( including duplicate elimination ) . We compare our transformation framework with alternative methods such as using XQuery or SQL\/XML provided by current commercial databases . The results demonstrate that the three-phase framework ( although as simple as it is ) is highly scalable and outperforms the alternative methods by orders of magnitude ."
"694","WAP5 : black-box performance debugging for wide-area systems Wide-area distributed applications are challenging to debug , optimize , and maintain . We present Wide-Area Project 5 ( WAP5 ) , which aims to make these tasks easier by exposing the causal structure of communication within an application and by exposing delays that imply bottlenecks . These bottlenecks might not otherwise be obvious , with or without the application 's source code . Previous research projects have presented algorithms to reconstruct application structure and the corresponding timing information from black-box message traces of local-area systems . In this paper we present ( 1 ) a new algorithm for reconstructing application structure in both local - and wide-area distributed systems , ( 2 ) an infrastructure for gathering application traces in PlanetLab , and ( 3 ) our experiences tracing and analyzing three systems : CoDeeN and Coral , two content-distribution networks in PlanetLab ; and Slurpee , an enterprise-scale incident-monitoring system ."
"695","Identifying and discriminating between web and peer-to-peer traffic in the network core Traffic classification is the ability to identify and categorize network traffic by application type . In this paper , we consider the problem of traffic classification in the network core . Classification at the core is challenging because only partial information about the flows and their contributors is available . We address this problem by developing a framework that can classify a flow using only unidirectional flow information . We evaluated this approach using recent packet traces that we collected and pre-classified to establish a `` base truth '' . From our evaluation , we find that flow statistics for the server-to-client direction of a TCP connection provide greater classification accuracy than the flow statistics for the client-to-server direction . Because collection of the server-to-client flow statistics may not always be feasible , we developed and validated an algorithm that can estimate the missing statistics froma unidirectional packet trace ."
"696","Background knowledge for ontology construction In this paper we describe a solution for incorporating background knowledge into the OntoGen system for semi-automatic ontology construction . This makes it easier for different users to construct different and more personalized ontologies for the same domain . To achieve this we introduce a word weighting schema to be used in the document representation . The weighting schema is learned based on the background knowledge provided by user . It is than used by OntoGen 's machine learning and text mining algorithms ."
"697","A large-scale study of web password habits We report the results of a large scale study of password use andpassword re-use habits . The study involved half a million users over athree month period . A client component on users ' machines recorded a variety of password strength , usage and frequency metrics . This allows us to measure or estimate such quantities as the average number of passwords and average number of accounts each user has , how many passwords she types per day , how often passwords are shared among sites , and how often they are forgotten . We get extremely detailed data on password strength , the types and lengths of passwords chosen , and how they vary by site . The data is the first large scale study of its kind , and yields numerous other insights into the role the passwords play in users ' online experience ."
"698","Discovering event evolution graphs from newswires In this paper , we propose an approach to automatically mine event evolution graphs from newswires on the Web . Event evolution graph is a directed graph in which the vertices and edges denote news events and the evolutions between events respectively , in a news affair . Our model utilizes the content similarity between events and incorporates temporal proximity and document distributional proximity as decaying functions . Our approach is effective in presenting the inside developments of news affairs along the timeline , which can facilitate users ' information browsing tasks ."
"699","Focused crawling by exploiting anchor text using decision tree Focused crawlers are considered as a promising way to tackle the scalability problem of topic-oriented or personalized search engines . To design a focused crawler , the choice of strategy for prioritizing unvisited URLs is crucial . In this paper , we propose a method using a decision tree on anchor texts of hyperlinks . We conducted experiments on the real data sets of four Japanese universities and verified our approach ."
"700","Generating maps of web pages using cellular automata The aim of web pages visualization is to present in a very informative and interactive way a set of web documents to the user in order to let him or her navigate through these documents . In the web context , this may correspond to several user 's tasks : displaying the results of a search engine , or visualizing a graph of pages such as a hypertext or a surf map . In addition to web pages visualization , web pages clustering also greatly improves the amount of information presented to the user by highlighting the similarities between the documents ( 6 ) . In this paper we explore the use of a cellular automata ( CA ) to generate such maps of web pages ."
"701","A content and structure website mining model We present a novel model for validating and improving the content and structure organization of a website . This model studies the website as a graph and evaluates its interconnectivity in relation to the similarity of its documents . The aim of this model is to provide a simple way for improving the overall structure , contents and interconnectivity of a website . This model has been implemented as a prototype and applied to several websites , showing very interesting results . Our model is complementary to other methods of website personalization and improvement ."
"702","Effort estimation : how valuable is it for a web company to use a cross-company data set , compared to using its own single-company data set ? Previous studies comparing the prediction accuracy of effort models built using Web cross - and single-company data sets have been inconclusive , and as such replicated studies are necessary to determine under what circumstances a company can place reliance on a cross-company effort model . This paper therefore replicates a previous study by investigating how successful a cross-company effort model is : i ) to estimate effort for Web projects that belong to a single company and were not used to build the cross-company model ; ii ) compared to a single-company effort model . Our single-company data set had data on 15 Web projects from a single company and our cross-company data set had data on 68 Web projects from 25 different companies . The effort estimates used in our analysis were obtained by means of two effort estimation techniques , namely forward stepwise regression and case-based reasoning . Our results were similar to those from the replicated study , showing that predictions based on the single-company model were significantly more accurate than those based on the cross-company model ."
"703","XAR-miner : efficient association rules mining for XML data In this paper , we propose a framework , called XAR-Miner , for mining ARs from XML documents efficiently . In XAR-Miner , raw data in the XML document are first preprocessed to transform to either an Indexed Content Tree ( IX-tree ) or Multi-relational databases ( Multi-DB ) , depending on the size of XML document and memory constraint of the system , for efficient data selection and AR mining . Task-relevant concepts are generalized to produce generalized meta-patterns , based on which the large ARs that meet the support and confidence levels are generated ."
"704","Mining contiguous sequential patterns from web logs Finding Contiguous Sequential Patterns ( CSP ) is an important problem in Web usage mining . In this paper we propose a new data structure , UpDown Tree , for CSP mining . An UpDown Tree combines suffix tree and prefix tree for efficient storage of all the sequences that contain a given item . The special structure of UpDown Tree ensures efficient detection of CSPs . Experiments show that UpDown Tree improves CSP mining in terms of both time and memory usage comparing to previous approaches ."
"705","Selective early request termination for busy internet services Internet traffic is bursty and network servers are often overloaded with surprising events or abnormal client request patterns . This paper studies a load shedding mechanism called selective early request termination ( SERT ) for network services that use threads to handle multiple incoming requests continuously and concurrently . Our investigation with applications from Ask.com shows that during overloaded situations , a relatively small percentage of long requests that require excessive computing resource can dramatically affect other short requests and reduce the overall system throughput . By actively detecting and aborting overdue long requests , services can perform significantly better to achieve QoS objectives compared to a purely admission based approach . We have proposed a termination scheme that monitors running time of requests , accounts for their resource usage , adaptively adjusts the selection threshold , and performs a safe termination for a class of requests . This paper presents the design and implementation of this scheme and describes experimental results to validate the proposed approach ."
"706","EDUTELLA : a P2P networking infrastructure based on RDF Metadata for the World Wide Web is important , but metadata for Peer-to-Peer ( P2P ) networks is absolutely crucial . In this paper we discuss the open source project Edutella which builds upon metadata standards defined for the WWW and aims to provide an RDF-based metadata infrastructure for P2P applications , building on the recently announced JXTA Framework . We describe the goals and main services this infrastructure will provide and the architecture to connect Edutella Peers based on exchange of RDF metadata . As the query service is one of the core services of Edutella , upon which other services are built , we specify in detail the Edutella Common Data Model ( ECDM ) as basis for the Edutella query exchange language ( RDF-QEL-i ) and format implementing distributed queries over the Edutella network . Finally , we shortly discuss registration and mediation services , and introduce the prototype and application scenario for our current Edutella aware peers ."
"707","Tag clouds for summarizing web search results In this paper , we describe an application , PubCloud that uses tagclouds for the summarization of results from queries over thePubMed database of biomedical literature . PubCloud responds toqueries of this database with tag clouds generated from wordsextracted from the abstracts returned by the query . The results ofa user study comparing the PubCloud tag-cloud summarization ofquery results with the standard result list provided by PubMedindicated that the tag cloud interface is advantageous in presenting descriptive information and in reducing user frustrationbut that it is less effective at the task of enabling users to discoverrelations between concepts ."
"708","Life is sharable : mechanisms to support and sustain blogging life experience Recent trend in the development of mobile devices , wireless communications , sensor technologies , weblogs , and peer-to-peer communications have prompted a new design opportunity for enhancing social interactions . This paper introduces our preliminary experiences in designing a prototype utilizing the aforementioned technologies to share life experience . Users equipped with camera phones coupled with short-range communication technology , such as RFID , can capture life experience and share it as weblogs to other people . However , in reality , this is easier said than done . The success of weblogs relies on the active participation and willingness of people to contribute . To encourage active participations , a ranking system , AgreeRank , is specifically developed to get them motivated ."
"709","Bringing communities to the semantic web and the semantic web to communities In this paper we consider the types of community networks that are most often codified within the Semantic Web . We propose the recognition of a new structure which fulfils the definition of community used outside the Semantic Web . We argue that the properties inherent in a community allow additional processing to be done with the described relationships existing between entities within the community network . Taking an existing online community as a case study we describe the ontologies and applications that we developed to support this community in the Semantic Web environment and discuss what lessons can be learned from this exercise and applied in more general settings ."
"710","A probabilistic semantic approach for discovering web services Service discovery is one of challenging issues in Service-Oriented computing . Currently , most of the existing service discovering and matching approaches are based on keywords-based strategy . However , this method is inefficient and time-consuming . In this paper , we present a novel approach for discovering web services . Based on the current dominating mechanisms of discovering and describing Web Services with UDDI and WSDL , the proposed approach utilizes Probabilistic Latent Semantic Analysis ( PLSA ) to capture semantic concepts hidden behind words in the query and advertisements in services so that services matching is expected to carry out at concept level . We also present related algorithms and preliminary experiments to evaluate the effectiveness of our approach ."
"711","Text joins in an RDBMS for web data integration The integration of data produced and collected across autonomous , heterogeneous web services is an increasingly important and challenging problem . Due to the lack of global identifiers , the same entity ( e.g. , a product ) might have different textual representations across databases . Textual data is also often noisy because of transcription errors , incomplete information , and lack of standard formats . A fundamental task during data integration is matching of strings that refer to the same entity . In this paper , we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources . We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes , where the similarity of matches exceeds a specified threshold . Computing an exact answer to the text join can be expensive . For query processing efficiency , we propose a sampling-based join approximation strategy for execution in a standard , unmodified relational database management system ( RDBMS ) , since more and more web sites are powered by RDBMSs with a web-based front end . We implement the join inside an RDBMS , using SQL queries , for scalability and robustness reasons . Finally , we present a detailed performance evaluation of an implementation of our algorithm within a commercial RDBMS , using real-life data sets . Our experimental results demonstrate the efficiency and accuracy of our techniques ."
"712","Towards automating regression test selection for web services This paper reports a safe regression test selection ( RTS ) approach that is designed for verifying Web services in an end-to-end manner . The Safe RTS technique has been integrated into a systematic method that monitors distributed code modifications and automates the RTS and RT processes ."
"713","Hubble : an advanced dynamic folder system for XML Organizing large document collections for finding information easily and quickly has always been an important user requirement . This paper describes a flexible and powerful dynamic folder technology , called Hubble , which exploits XML semantics to precisely categorize XML documents into categories or folders ."
"714","Reappraising cognitive styles in adaptive web applications The mechanisms for personalisation used in web applications are currently the subject of much debate amongst researchers from many diverse subject areas . One of the most contemporary ideas for user modelling in web applications is that of cognitive styles , where a user 's psychological preferences are assessed stored in a database and then used to provide personalised content and\/or links . We describe user trials of a case study that utilises visual-verbal preferences in an adaptive web-based educational system ( AWBES ) . Students in this trial were assessed by the Felder-Solomon Inventory of Learning Styles ( ILS ) instrument , and their preferences were used as a means of content personalisation . Contrary to previous findings by other researchers , we found no significant differences in performance between matched and mismatched students . Conclusions are drawn about the value and validity of using cognitive styles as a way of modelling user preferences in educational web applications ."
"715","Association search in semantic web : search + inference Association search is to search for certain instances in semantic web and then make inferences from and about the instances we have found . In this paper , we propose the problem of association search and our preliminary solution for it using Bayesian network . We first minutely define the association search and its categorization . We then define tasks in association search . In terms of Bayesian network , we take ontology taxonomy as network structure in Bayesian network . We use the query log of instances to estimate the network parameters . After the Bayesian network is constructed , we give the solution for association search in the network ."
"716","Verifying genre-based clustering approach to content extraction The content of a webpage is usually contained within a small body of text and images , or perhaps several articles on the same page ; however , the content may be lost in the clutter , particularly hurting users browsing on small cell phone and PDA screens and visually impaired users relying on speed rendering of web pages . Using the genre of a web page , we have created a solution , Crunch that automatically identifies clutter and removes it , thus leaving a clean content-full page . In order to evaluate the improvement in the applications for this technology , we identified a number of experiments . In this paper , we have those experiments , the associated results and their evaluation ."
"717","A probabilistic approach to spatiotemporal theme pattern mining on weblogs Mining subtopics from weblogs and analyzing their spatiotemporal patterns have applications in multiple domains . In this paper , we define the novel problem of mining spatiotemporal theme patterns from weblogs and propose a novel probabilistic approach to model the subtopic themes and spatiotemporal theme patterns simultaneously . The proposed model discovers spatiotemporal theme patterns by ( 1 ) extracting common themes from weblogs ; ( 2 ) generating theme life cycles for each given location ; and ( 3 ) generating theme snapshots for each given time period . Evolution of patterns can be discovered by comparative analysis of theme life cycles and theme snapshots . Experiments on three different data sets show that the proposed approach can discover interesting spatiotemporal theme patterns effectively . The proposed probabilistic model is general and can be used for spatiotemporal text mining on any domain with time and location information ."
"718","Information search and re-access strategies of experienced web users Experienced web users have strategies for information search and re-access that are not directly supported by web browsers or search engines . We studied how prevalent these strategies are and whether even experienced users have problems with searching and re-accessing information . With this aim , we conducted a survey with 236 experienced web users . The results showed that this group has frequently used key strategies ( e.g. , using several browser windows in parallel ) that they find important , whereas some of the strategies that have been suggested in previous studies are clearly less important for them ( e.g. , including URLs on a webpage ) . In some aspects , such as query formulation , this group resembles less experienced web users . For instance , we found that most of the respondents had misconceptions about how their search engine handles queries , as well as other problems with information search and re-access . In addition to presenting the prevalence of the strategies and rationales for their use , we present concrete designs solutions and ideas for making the key strategies also available to less experienced users ."
"719","Determining the user intent of web search engine queries Determining the user intent of Web searches is a difficult problem due to the sparse data available concerning the searcher . In this paper , we examine a method to determine the user intent underlying Web search engine queries . We qualitatively analyze samples of queries from seven transaction logs from three different Web search engines containing more than five million queries . From this analysis , we identified characteristics of user queries based on three broad classifications of user intent . The classifications of informational , navigational , and transactional represent the type of content destination the searcher desired as expressed by their query . We implemented our classification algorithm and automatically classified a separate Web search engine transaction log of over a million queries submitted by several hundred thousand users . Our findings show that more than 80 % of Web queries are informational in nature , with about 10 % each being navigational and transactional . In order to validate the accuracy of our algorithm , we manually coded 400 queries and compared the classification to the results from our algorithm . This comparison showed that our automatic classification has an accuracy of 74 % . Of the remaining 25 % of the queries , the user intent is generally vague or multi-faceted , pointing to the need to for probabilistic classification . We illustrate how knowledge of searcher intent might be used to enhance future Web search engines ."
"720","An audio\/video analysis mechanism for web indexing The high availability of video streams is making necessary mechanisms for indexing such contents in the Web world . In this paper we focus on news programs and we propose a mechanism that integrates low and high level video features to provide a high level semantic description . A color\/luminance analysis is coupled with audio analysis to provide a better identification of all the video segments that compose the video stream . Each video segment is subject to speech detection and is described through MPEG7 so that the resulting metadata description can be used to index the video stream . An experimental evaluation shows the benefits of integrating audio and video analysis ."
"721","A high-performance interpretive approach to schema-directed parsing XML delivers key advantages in interoperability due to its flexibility , expressiveness , and platform-neutrality . As XML has become a performance-critical aspect of the next generation of business computing infrastructure , however , it has become increasingly clear that XML parsing often carries a heavy performance penalty , and that current , widely-used parsing technologies are unable to meet the performance demands of an XML-based computing infrastructure . Several efforts have been made to address this performance gap through the use of grammar-based parser generation . While the performance of generated parsers has been significantly improved , adoption of the technology has been hindered by the complexity of compiling and deploying the generated parsers . Through careful analysis of the operations required for parsing and validation , we have devised a set of specialized byte codes , designed for the task of XML parsing and validation . These byte codes are designed to engender the benefits of fine-grained composition of parsing and validation that make existing compiled parsers fast , while being coarse-grained enough to minimize interpreter overhead . This technique of using an interpretive , validating parser balances the need for performance against the requirements of simple tooling and robust scalable infrastructure . Our approach is demonstrated with a specialized schema compiler , used to generate byte codes which in turn drive an interpretive parser . With almost as little tooling and deployment complexity as a traditional interpretive parser , the byte code-driven parser usually demonstrates performance within 20 % of the fastest fully compiled solutions ."
"722","A framework for rapid integration of presentation components The development of user interfaces ( UIs ) is one of the most time-consuming aspects in software development . In this context , the lack of proper reuse mechanisms for UIs is increasingly becoming manifest , especially as software development is more and more moving toward composite applications . In this paper we propose a framework for the integration of stand-alone modules or applications , where integration occurs at the presentation layer . Hence , the final goal is to reduce the effort required for UI development by maximizing reus . The design of the framework is inspired by lessons learned from application integration , appropriately modified to account for the specificity of the UI integration problem . We provide an abstract component model to specify characteristics and behaviors of presentation components and propose an event-based composition model to specify the composition logic . Components and composition are described by means of a simple XML-based language , which is interpreted by a runtime middleware for the execution of the resulting composite application . A proof-of-concept prototype allows us to show that the proposed component model can also easily be applied to existing presentation components , built with different languages and\/or component technologies ."
"723","Multi-factor clustering for a marketplace search interface Search engines provide a small window to the vast repository of data they index and against which they search . They try their best to return the documents that are of relevance to the user but often a large number of results may be returned . Users struggle to manage this vast result set looking for the items of interest . Clustering search results is one way of alleviating this navigational pain . In this paper we describe a clustering system that enables clustering search results in an online marketplace search system ."
"724","DemIL : an online interaction language between citizen and government Electronic democracy should provide information and service for the citizens on the Internet , allowing room for debate , participation and electronic voting . The languages being adopted by mass communication means , especially Reality Shows , are efficient and encourage public participation in decision-making . This paper discusses a citizen-government interaction language intended to facilitate citizen participation in the government 's decisions . An e-Democracy Model for people participation through web-based technologies is conceived . This model specifies the syntax of an Democracy Interaction Language , a DemIL . Such language incorporates characteristics of Reality Show Formats , and it is the back-end of a web-interface project in the domain researched . The study of case Participative Budget of Brazil represents the language proposed ."
"725","Causal relation of queries from temporal logs In this paper , we study a new problem of mining causal relation of queries in search engine query logs . Causal relation between two queries means event on one query is the causation of some event on the other . We first detect events in query logs by efficient statistical frequency threshold . Then the causal relation of queries is mined by the geometric features of the events . Finally the Granger Causality Test ( GCT ) is utilized to further re-rank the causal relation of queries according to their GCT coefficients . In addition , we develop a 2-dimensional visualization tool to display the detected relationship of events in a more intuitive way . The experimental results on the MSN search engine query logs demonstrate that our approach can accurately detect the events in temporal query logs and the causal relation of queries is detected effectively ."
"726","Bilingual web page and site readability assessment Readability assessment is a method to measure the difficulty of a piece of text material , and it is widely used in educational field to assist instructors to prepare appropriate materials for students . In this paper , we investigate the applications of readability assessment in Web development , such that users can retrieve information which is appropriate to their levels . We propose a bilingual ( English and Chinese ) assessment scheme for Web page and Web site readability based on textual features , and conduct a series of experiments with real Web data to evaluate our scheme . Experimental results show that , apart from just indicating the readability level , the estimated score acts as a good heuristic to figure out pages with low textual content . Furthermore , we can obtain the overall content distribution in a Web site by studying the variation of its readability ."
"727","DoNet : a semantic domotic framework In the very near future complete households will be entirely networked as a de facto standard . In this poster we briefly describe our work in the area of domotics , where personalization , semantics and agent technology come together . We illustrate a home system oriented ontology and an intelligent agent based framework for the rapid development of home control and automation . The ever changing nature of the home , places the user in a position were he needs to be involved and become , through DoNet , a part of an ongoing home system optimization process ."
"728","Integrating value-based requirement engineering models to webml using vip business modeling framework Requirement engineering ( RE ) is emerging as an increasingly important discipline for supporting Web application development , as these are designed to satisfy diverse stakeholder needs , additional functional , information , multimedia and usability requirements as compared to traditional software applications . Moreover , when considering innovative e-commerce applications , value-based RE is an extremely relevant methodology which exploits the concept of economic value during the RE activity . In contrast , most of the methodologies proposed for the development of Web applications , primarily focus on the system design , and paying less attention to the RE , and specifically to value-based RE . Focusing this aspect , the paper presents integration of value-based RE models to WebML models using our recently proposed VIP Business Modeling Framework ( 1 ) . We also analyze the framework 's potential in linking other modeling approaches , and argue about its significant integration potential with various E-R\/OO-based , process aware Web modeling approaches ."
"729","Adaptive web sites : user studies and simulation Adaptive web sites have been proposed to enhance ease of navigation and information retrieval . A variety of approaches are described in the literature , but consideration of interface presentation issues and realistic user studies are generally lacking . We report here a large-scale study of sites with dynamic information collections and user interests , where adaptation is based on an Ant Colony Optimization technique . We find that most users were able to locate information effectively without needing to perform explicit searches . The behavior of users who did search was similar to that on Internet search engines . Simulations based on site and user models give insight into the adaptive behavior and correspond to observations ."
"730","Toward expressive syndication on the web Syndication systems on the Web have attracted vast amounts of attention in recent years . As technologies have emerged and matured , there has been a transition to more expressive syndication approaches ; that is , subscribers and publishers are provided with more expressive means of describing their interests and published content , enabling more accurate information filtering . In this paper , we formalize a syndication architecture that utilizes expressive Web ontologies and logic-based reasoning for selective content dissemination . This provides finer grained control for filtering and automated reasoning for discovering implicit subscription matches , both of which are not achievable in less expressive approaches . We then address one of the main limitations with such a syndication approach , namely matching newly published information with subscription requests in an efficient and practical manner . To this end , we investigate continuous query answering for a large subset of the Web Ontology Language ( OWL ) ; specifically , we formally define continuous queries for OWL knowledge bases and present a novel algorithm for continuous query answering in a large subset of this language . Lastly , an evaluation of the query approach is shown , demonstrating its effectiveness for syndication purposes ."
"731","Web services security configuration in a service-oriented architecture Security is one of the major concerns when developing mission-critical business applications , and this concern motivated the Web Services Security specifications . However , the existing tools to configure the security properties of Web Services give a technology-oriented view ; only assisting in choosing data to encrypt and the encryption algorithms to use . A user must manually bridge the gap between the security requirements and the configuration , which could cause extra configuration costs and lead to potential misconfiguration hazards . To ease this situation , we came up with refining security requirements from business to technology , leveraging the concepts of Service-Oriented Architecture ( SOA ) and Model-Driven Architecture ( MDA ) . Security requirements are gradually transformed to more detailed ones or countermeasures by bridging the gap between them by using best practice patterns ."
"732","First-order focused crawling This paper reports a new general framework of focused web crawling based on `` relational subgroup discovery '' . Predicates are used explicitly to represent the relevance clues of those unvisited pages in the crawl frontier , and then first-order classification rules are induced using subgroup discovery technique . The learned relational rules with sufficient support and confidence will guide the crawling process afterwards . We present the many interesting features of our proposed first-order focused crawler , together with preliminary promising experimental results ."
"733","Visually guided bottom-up table detection and segmentation in web documents In the AllRight project , we are developing an algorithm for unsupervised table detection and segmentation that uses the visual rendition of a Web page rather than the HTML code . Our algorithm works bottom-up by grouping word bounding boxes into larger groups and uses a set of heuristics . It has already been implemented and a preliminary evaluation on about 6000 Web documents has been carried out ."
"734","Constructing extensible XQuery mappings Constructing and maintaining semantic mappings are necessary but troublesome in data sharing systems . While most current work focuses on seeking automated techniques to solve this problem , this paper proposes a combination model for constructing exten-sible mappings between XML schemas . In our model , complex global mappings are constructed by first defining simple atomic mappings for each target schema element , and then combining them using a few basic operators . At the same time , we provide automated support for constructing such combined mappings ."
"735","Modeling user behavior in recommender systems based on maximum entropy We propose a model for user purchase behavior in online stores that provide recommendation services . We model the purchase probability given recommendations for each user based on the maximum entropy principle using features that deal with recommendations and user interests . The proposed model enable us to measure the effect of recommendations on user purchase behavior , and the effect can be used to evaluate recommender systems . We show the validity of our model using the log data of an online cartoon distribution service , and measure the recommendation effects for evaluating the recommender system ."
"736","Using Google distance to weight approximate ontology matches Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web . The problem is even harder in domains where concepts are inherently vague and ill-defined , and can not be given a crisp definition . A notion of approximate concept mapping is required in such domains , but until now , no such notion is vailable . The first contribution of this paper is a definition for approximate mappings between concepts . Roughly , a mapping between two concepts is decomposed into a number of submappings , and a sloppiness value determines the fraction of these submappings that can be ignored when establishing the mapping . A potential problem of such a definition is that with an increasing sloppiness value , it will gradually allow mappings between any two arbitrary concepts . To improve on this trivial behavior , we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable matches , but at the same time maximises the sloppiness required to conclude undesirable matches . The second contribution of this paper is to show that a Google based similarity measure has exactly these desirable properties . We establish these results by experimental validation in the domain of musical genres . We show that this domain does suffer from ill-defined concepts . We take two real-life genre hierarchies from the Web , we compute approximate mappings between them at varying levels of sloppiness , and we validate our results against a handcrafted Gold Standard . Our method makes use of the huge amount of knowledge that is implicit in the current Web , and exploits this knowledge as a heuristic for establishing approximate mappings between ill-defined concepts ."
"737","Using proportional transportation similarity with learned element semantics for XML document clustering This paper proposes a novel approach to measuring XML document similarity by taking into account the semantics between XML elements . The motivation of the proposed approach is to overcome the problems of `` under-contributionö and `` over-contributionö existing in previous work . The element semantics are learned in an unsupervised way and the Proportional Transportation Similarity is proposed to evaluate XML document similarity by modeling the similarity calculation as a transportation problem . Experiments of clustering are performed on three ACM SIGMOD data sets and results show the favorable performance of the proposed approach ."
"738","Generation of multimedia TV news contents for WWW In this paper , we present a system we have developed for automatic TV News video indexing that successfully combines results from the fields of speaker verification , acoustic analysis , very large vocabulary video OCR , content based sampling of video , information retrieval , dialogue systems , and ASF media delivery over IP . The prototype of TV news content processing Web was completed in July 2003 . Since then , the system has been up running continuously . Up to the date when this message is written ( March 27 , 2006 ) , the system records and analyzes the prime time evening news program in Taiwan every day of these years , except a few power failure shutdown . The TV news web is at http:\/\/140.113.216.64\/NewsQuery\/main.as"
"739","A semantic-link-based infrastructure for web service discovery in P2P networks An important issue arising from P2P applications is how to accurately and efficiently retrieve the required Web services from large-scale repositories . This paper resolves this issue by organizing services in the overlay combining the Semantic Service Link Network and the Chord P2P network . A service request will first be routed in the Chord according to the given service operation names and keywords . Then , the same request will be routed in the Semantic Link Network according to the service link type and semantic matching . Compared with previous P2P service discovery approaches , the proposed approach has two advantages : ( 1 ) produce more accurate and meaning results when searching for particular services in a P2P network ; and ( 2 ) enable users and peers to discover services in a more flexible way ."
"740","Mirror site maintenance based on evolution associations of web directories Mirroring Web sites is a well-known technique commonly used in the Web community . A mirror site should be updated frequently to ensure that it reflects the content of the original site . Existing mirroring tools apply page-level strategies to check each page of a site , which is inefficient and expensive . In this paper , we propose a novel site-level mirror maintenance strategy . Our approach studies the evolution of Web directorystructures and mines association rules between ancestor-descendant Web directories . Discovered rules indicate the evolution correlations between Web directories . Thus , when maintaining the mirror of a Web site ( directory ) , we can optimally skipsubdirectories which are negatively correlated with it in undergoing significant changes . The preliminary experimental results show that our approach improves the efficiency of the mirror maintenance process significantly while sacrificing slightly in keeping the `` freshness '' of the mirrors ."
"741","Utility analysis for topically biased PageRank PageRank is known to be an efficient metric for computing general document importance in the Web . While commonly used as a one-size-fits-all measure , the ability to produce topically biased ranks has not yet been fully explored in detail . In particular , it was still unclear to what granularity of `` topic '' the computation of biased page ranks makes sense . In this paper we present the results of a thorough quantitative and qualitative analysis of biasing PageRank on Open Directory categories . We show that the MAP quality of Biased PageRank generally increases with the ODP level up to a certain point , thus sustaining the usage of more specialized categories to bias PageRank on , in order to improve topic specific search ."
"742","Temporal rules for mobile web personalization Many systems use past behavior , preferences and environmental factors to attempt to predict user navigation on the Internet . However we believe that many of these models have shortcomings , in that they do not take into account that users may have many different sets of preferences . Here we investigate an environmental factor , namely time , in making predictions about user navigation . We present methods for creating temporal rules that describe user navigation patterns . We also show the benefit of using these rules to predict user navigation and also show the benefits of these models over traditional methods . An analysis is carried out on a sample of usage logs for Wireless Application Protocol ( WAP ) browsing , and the results of this analysis verify our hypothesis ."
"743","A user profile-based approach for personal information access : shaping your information portfolio In the spread of internet , internet-based information service business has started to become profitable . One of the key technologies is personalization . Successful internet information services must realize personalized information delivery , by which the users can automatically receive highly tuned information according to their personal needs and preferences . In order to realize such personalized information services , we have developed an automatic user preference capture and an automatic information clipping function based on a Personalized Information Access technique . In this paper , those techniques will be demonstrated by showing a deployed personalized webpage service application ."
"744","XML-based XML schema access XML Schema 's abstract data model consists of components , which are the structures that eventually define a schema as a whole . XML Schema 's XML syntax , on the other hand , is not a direct representation of the schema components , and it proves to be surprisingly hard to derive a schema 's components from the XML syntax . The Schema Component XML Syntax ( SCX ) is a representation which attempts to map schema components as faithfully as possible to XML structures . SCX serves as the starting point for applications which need access to schema components and want to do so using standardized and widely available XML technologies ."
"745","Bootstrapping semantics on the web : meaning elicitation from schemas In most web sites , web-based applications ( such as web portals , e-marketplaces , search engines ) , and in the file systems of personal computers , a wide variety of schemas ( such as taxonomies , directory trees , thesauri , Entity-Relationship schemas , RDF Schemas ) are published which ( i ) convey a clear meaning to humans ( e.g. help in the navigation of large collections of documents ) , but ( ii ) convey only a small fraction ( if any ) of their meaning to machines , as their intended meaning is not formally\/explicitly represented . In this paper we present a general methodology for automatically eliciting and representing the intended meaning of these structures , and for making this meaning available in domains like information integration and interoperability , web service discovery and composition , peer-to-peer knowledge management , and semantic browsers . We also present an implementation ( called CtxMatch2 ) of how such a method can be used for semantic interoperability ."
"746","GoGetIt ! : a tool for generating structure-driven web crawlers We present GoGetIt ! , a tool for generating structure-driven crawlers that requires a minimum effort from the users . The tool takes as input a sample page and an entry point to a Web site and generates a structure-driven crawler based on navigation patterns , sequences of patterns for the links a crawler has to follow to reach the pages structurally similar to the sample page . In the experiments we have performed , structure-driven crawlers generated by GoGetIt ! were able to collect all pages that match the samples given , including those pages added after their generation ."
"747","Semantic link based top-K join queries in P2P networks An important issue arising from Peer-to-Peer applications is how to accurately and efficiently retrieve a set of K best matching data objects from different sources while minimizing the number of objects that have to be accessed . This paper resolves this issue by organizing peers in a Semantic Link Network Overlay , where semantic links are established to denote the semantic relationship between peers ' data schemas . A query request will be routed to appropriate peers according to the semantic link type and a lower bound of rank function . Optimization strategies are proposed to reduce the total amount of data transmitted ."
"748","A novel collaborative filtering-based framework for personalized services in m-commerce With the rapid growth of wireless technologies and handheld devices , m-commerce is becoming a promising research area . Personalization is especially important to the success of m-commerce . This paper proposes a novel collaborative filtering-based framework for personalized services in m-commerce . The framework extends our previous work by using Online Analytical Processing ( OLAP ) to represent the relations among user , content and context information , and adopting a multi-dimensional collaborative filtering model to perform inference . It provides a powerful and well-founded mechanism to personalization for m-commerce . We implemented it in an existing m-commerce platform , and experimental results demonstrate its feasibility and correctness ."
"749","Bayesian network based sentence retrieval model This paper makes an intensive investigation of the application of Bayesian network in sentence retrieval and introduces three Bayesian network based sentence retrieval models with or without consideration of term relationships . Term relationships in this paper are considered from two perspectives : relationships between pairs of terms and relationships between terms and term sets . Experiments have proven the efficiency of Bayesian network in the application of sentence retrieval . Particularly , retrieval result with consideration of the second kind of term relationship performs better in improving retrieval precision ."
"750","EPCI : extracting potentially copyright infringement texts from the web In this paper , we propose a new system extracting potentially copyright infringement texts from the Web , called EPCI . EPCI extracts them in the following way : ( 1 ) generating a set of queries based on a given copyright reserved seed-text , ( 2 ) putting every query to search engine API , ( 3 ) gathering the search result Web pages from high ranking until the similarity between the given seed-text and the search result pages becomes less than a given threshold value , and ( 4 ) merging all the gathered pages , then re-ranking them in the order of their similarity . Our experimental result using 40 seed-texts shows that EPCI is able to extract 132 potentially copyright infringement Web pages per a given copyright reserved seed-text with 94 % precision in average ."
"751","Dealing with different distributions in learning from In the problem of learning with positive and unlabeled examples , existing research all assumes that positive examples P and the hidden positive examples in the unlabeled set U are generated from the same distribution . This assumption may be violated in practice . In such cases , existing methods perform poorly . This paper proposes a novel technique A-EM to deal with the problem . Experimental results with product page classification demonstrate the effectiveness of the proposed technique ."
"752","CWS : a comparative web search system In this paper , we define and study a novel search problem : Comparative Web Search ( CWS ) . The task of CWS is to seek relevant and comparative information from the Web to help users conduct comparisons among a set of topics . A system called CWS is developed to effectively facilitate Web users ' comparison needs . Given a set of queries , which represent the topics that a user wants to compare , the system is characterized by : ( 1 ) automatic retrieval and ranking of Web pages by incorporating both their relevance to the queries and the comparative contents they contain ; ( 2 ) automatic clustering of the comparative contents into semantically meaningful themes ; ( 3 ) extraction of representative keyphrases to summarize the commonness and differences of the comparative contents in each theme . We developed a novel interface which supports two types of view modes : a pair-view which displays the result in the page level , and a cluster-view which organizes the comparative pages into the themes and displays the extracted phrases to facilitate users ' comparison . Experiment results show the CWS system is effective and efficient ."
"753","Logical structure based semantic relationship extraction from semi-structured documents Addressed in this paper is the issue of semantic relationship extraction from semi-structured documents . Many research efforts have been made so far on the semantic information extraction . However , much of the previous work focuses on detecting ` isolated ' semantic information by making use of linguistic analysis or linkage information in web pages and limited research has been done on extracting semantic relationship from the semi-structured documents . In this paper , we propose a method for semantic relationship extraction by using the logical information in the semi-structured document ( semi-structured document usually has various types of structure information , e.g. a semi-structured document may be hierarchical laid out ) . To the best of our knowledge , extracting semantic relationships by using logical information has not been investigated previously . A probabilistic approach has been proposed in the paper . Features used in the probabilistic model have been defined ."
"754","Using d-gap patterns for index compression Sequential patterns of d-gaps exist pervasively in inverted lists of Web document collection indices due to the cluster property . In this paper the information of d-gap sequential patterns is used as a new dimension for improving inverted index compression . We first detect d-gap sequential patterns using a novel data structure , UpDown Tree . Based on the detected patterns , we further substitute each pattern with its pattern Id in the inverted lists that contain it . The resulted inverted lists are then coded with an existing coding scheme . Experiments show that this approach can effectively improve the compression ratio of existing codes ."
"755","The complex dynamics of collaborative tagging The debate within the Web community over the optimal means by which to organize information often pits formalized classifications against distributed collaborative tagging systems . A number of questions remain unanswered , however , regarding the nature of collaborative tagging systems including whether coherent categorization schemes can emerge from unsupervised tagging by users . This paper uses data from the social bookmarking site delicio . us to examine the dynamics of collaborative tagging systems . In particular , we examine whether the distribution of the frequency of use of tags for `` popular '' sites with a long history ( many tags and many users ) can be described by a power law distribution , often characteristic of what are considered complex systems . We produce a generative model of collaborative tagging in order to understand the basic dynamics behind tagging , including how a power law distribution of tags could arise . We empirically examine the tagging history of sites in order to determine how this distribution arises over time and to determine the patterns prior to a stable distribution . Lastly , by focusing on the high-frequency tags of a site where the distribution of tags is a stabilized power law , we show how tag co-occurrence networks for a sample domain of tags can be used to analyze the meaning of particular tags given their relationship to other tags ."
"756","Using annotations in enterprise search A major difference between corporate intranets and the Internet is that in intranets the barrier for users to create web pages is much higher . This limits the amount and quality of anchor text , one of the major factors used by Internet search engines , making intranet search more difficult . The social phenomenon at play also means that spam is relatively rare . Both on the Internet and in intranets , users are often willing to cooperate with the search engine in improving the search experience . These characteristics naturally lead to considering using user feedback to improve search quality in intranets . In this paper we show how a particular form of feedback , namely user annotations , can be used to improve the quality of intranet search . An annotation is a short description of the contents of a web page , which can be considered a substitute for anchor text . We propose two ways to obtain user annotations , using explicit and implicit feedback , and show how they can be integrated into a search engine . Preliminary experiments on the IBM intranet demonstrate that using annotations improves the search quality ."
"757","Generating summaries for large collections of geo-referenced photographs We describe a framework for automatically selecting a summary set of photographs from a large collection of geo-referenced photos . The summary algorithm is based on spatial patterns in photo sets , but can be expanded to support social , temporal , as well as textual-topical factors of the photo set . The summary set can be biased by the user , the content of the user 's query , and the context in which the query is made . An initial evaluation on a set of geo-referenced photos shows that our algorithm performs well , producing results that are highly rated by users ."
"758","U-REST : an unsupervised record extraction system In this paper , we describe a system that can extract recordstructures from web pages with no direct human supervision . Records are commonly occurring HTML-embedded data tuples that describe people , offered courses , products , company profiles , etc. . We present a simplified frameworkfor studying the problem of unsupervised record extraction . one which separates the algorithms from the feature engineering . Our system , U-REST formalizes an approach tothe problem of unsupervised record extraction using a simple two-stage machine learning framework . The first stage involves clustering , where structurally similar regions are discovered , and the second stage involves classification , where discovered groupings ( clusters of regions ) are ranked by their likelihood of being records . In our work , we describe , and summarize the results of an extensive survey of features for both stages . We conclude by comparing U-REST to related systems . The results of our empirical evaluation show encouraging improvements in extraction accuracy ."
"759","Detecting near-duplicates for web crawling Near-duplicate web documents are abundant . Two such documents differ from each other in a very small portion that displays advertisements , for example . Such differences are irrelevant for web search . So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not . In the course of developing a near-duplicate detection system for a multi-billion page repository , we make two research contributions . First , we demonstrate that Charikar 's fingerprinting technique is appropriate for this goal . Second , we present an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in at most k bit-positions , for small k. Our technique is useful for both online queries ( single fingerprints ) and all batch queries ( multiple fingerprints ) . Experimental evaluation over real data confirms the practicality of our design ."
"760","Effective web-scale crawling through website analysis The web crawler space is often delimited into two general areas : full-web crawling and focused crawling . We present netSifter , a crawler system which integrates features from these two areas to provide an effective mechanism for web-scale crawling . netSifter utilizes a combination of page-level analytics and heuristics which are applied to a sample of web pages from a given website . These algorithms score individual web pages to determine the general utility of the overall website . In doing so , netSifter can formulate an in-depth opinion of a website ( and the entirety of its web pages ) with a relative minimum of work . netSifter is then able to bias the future efforts of its crawl towards higher quality websites , and away from the myriad of low quality websites and crawler traps that litter the World Wide Web ."
"761","Understanding the function of web elements for mobile content delivery using random walk models In this paper , we describe a method for understanding the function of web elements . It classifies web elements into five functional categories : Content ( C ) , Related Links ( R ) , Navigation and Support ( N ) , Advertisement ( A ) and Form ( F ) . We construct five graphs for a web page , and each graph is designed such that most of the probability mass of the stationary distribution is concentrated in nodes belong to its corresponding category . We perform random walks on these graphs until convergence and classify based on its rank value in different graphs . Our experiment shows that the new method performed very well comparing to basic machine learning methods ."
"762","Learning and inferencing in user ontology for personalized semantic web services Domain ontology has been used in many Semantic Web applications . However , few applications explore the use of ontology for personalized services . This paper proposes an ontology based user model consisting of both concepts and semantic relations to represent users ' interests . Specifically , we adopt a statistical approach to learning a semantic-based user ontology model from domain ontology and a spreading activation procedure for inferencing in the user ontology model . We apply the methods of learning and exploiting user ontology to a semantic search engine for finding academic publications . Our experimental results support the efficacy of user ontology and spreading activation theory ( SAT ) for providing personalized semantic services ."
"763","Adding semantics to rosettaNet specifications The use of Semantic Web Service ( SWS ) technologies have been suggested to enable more dynamic B2B integration of heterogeneous systems and partners . We present how we add semantics to RosettaNet specifications to enable the WSMX SWS environment to automate mediation of messages . The benefits of applying SWS technologies include flexibility in accepting heterogeneity in B2B integrations ."
"764","Simple authentication for the web Automated email-based password reestablishment ( EBPR ) is an efficient , cost-effective means to deal with forgotten passwords . In this technique , email providers authenticate users on behalf of web sites . This method works because web sites trust email providers to deliver messages to their intended recipients . Simple Authentication for the Web ( SAW ) improves upon this basic approach to user authentication to create an alternative to password-based logins . SAW : 1 ) Removes the setup and management costs of passwords at sites that accept the risks of EBPR ; 2 ) Provides single sign-on without a specialized identity provider ; 3 ) Thwarts all passive attacks ."
"765","Ontalk : ontology-based personal document management system In this paper , we present our development of a document management and retrieval tool , which is named Ontalk . Our system provides a semi-automatic metadata generator and an ontology-based search engine for electronic documents . Ontalk can create or import various ontologies in RDFS or OWL for describing the metadata . Our system that is built upon . NET technology is easily communicated with or flexibly plugged into many different programs ."
"766","Automatic identification of user interest for personalized search One hundred users , one hundred needs . As more and more topics are being discussed on the web and our vocabulary remains relatively stable , it is increasingly difficult to let the search engine know what we want . Coping with ambiguous queries has long been an important part of the research on Information Retrieval , but still remains a challenging task . Personalized search has recently got significant attention in addressing this challenge in the web search community , based on the premise that a user 's general preference may help the search engine disambiguate the true intention of a query . However , studies have shown that users are reluctant to provide any explicit input on their personal preference . In this paper , we study how a search engine can learn a user 's preference automatically based on her past click history and how it can use the user preference to personalize search results . Our experiments show that users ' preferences can be learned accurately even from little click-history data and personalized search based on user preference yields significant improvements over the best existing ranking mechanism in the literature ."
"767","GeoTV : navigating geocoded rss to create an iptv experience The Web is rapidly moving towards a platform for mass collaboration in content production and consumption from three screens : computers , mobile phones , and TVs . While there has been a surge of interests in making Web content accessible from mobile devices , there is a significant lack of progress when it comes to making the web experience suitable for viewing on a television . Towards this end , we describe a novel concept , namely GeoTV , where we explore a framework by which web content can be presented or pushed in a meaningful manner to create an entertainment experience for the TV audience . Fresh content on a variety of topics , people , and places is being created and made available on the Web at breathtaking speed . Navigating fresh content effectively on TV demands a new browsing paradigm that requires fewer mouse clicks or user interactions from the remote control . Novel geospatial and temporal browsing techniques are provided in GeoTV that allow users the capability of aggregating and navigating RSS-enabled content in a timely , personalized and automatic manner for viewing in an IPTV environment . This poster is an extension of our previous work on GeoTracker that utilizes both a geospatial representation and a temporal ( chronological ) presentation to help users spot the most relevant updates quickly within the context of a Web-enabled environment . We demonstrate 1 ) the usability of such a tool that greatly enhances a user . s ability in locating and browsing videos based on his or her geographical interests and 2 ) various innovative interface designs for showing RSS-enabled information in an IPTV environment ."
"768","Towards effective browsing of large scale social annotations This paper is concerned with the problem of browsing social annotations . Today , a lot of services ( e.g. , Del. icio . us , Filckr ) have been provided for helping users to manage and share their favorite URLs and photos based on social annotations . Due to the exponential increasing of the social annotations , more and more users , however , are facing the problem how to effectively find desired resources from large annotation data . Existing methods such as tag cloud and annotation matching work well only on small annotation sets . Thus , an effective approach for browsing large scale annotation sets and the associated resources is in great demand by both ordinary users and service providers . In this paper , we propose a novel algorithm , namely Effective Large Scale Annotation Browser ( ELSABer ) , to browse large-scale social annotation data . ELSABer helps the users browse huge number of annotations in a semantic , hierarchical and efficient way . More specifically , ELSABer has the following features : 1 ) the semantic relations between annotations are explored for browsing of similar resources ; 2 ) the hierarchical relations between annotations are constructed for browsing in a top-down fashion ; 3 ) the distribution of social annotations is studied for efficient browsing . By incorporating the personal and time information , ELSABer can be further extended for personalized and time-related browsing . A prototype system is implemented and shows promising results ."
"769","Web4CE : accessing web-based applications on consumer devices In a world where all devices will be interconnected , the boundaries between the different devices will start to disappear . Devices will be able to access each other 's applications ; sessions can be suspended on one device and resumed on another device ; devices can serve as each other 's input and output device , and all devices will be able to connect to the Internet . This will give true mobility to the user as he\/she will not be restricted to the time and location where he\/she accesses an application . Of course , we need a variety of different mechanisms and technologies to enable this , such as : Remote rendering of UIs on other devices in the network . Infrastructure for discovering client and servers in a network . Mechanisms to exchange capability information between devices , and to adapt the UI based on these capabilities . Mechanisms to deal with session migration . Support for a wide range of consumer devices , ranging from mobile phones to high-end TVs . This requires technologies that cross different domains , i.e. the PC domain , mobile domain , and TV domain . Several major companies within these different domains have decided to work together on these issues . One of the results is a framework for remote user interfaces for both UPnP '' networks and the Internet . This framework is called Web4CE ( a.k.a. CEA-2014 ) ( 1 ) , and has been accepted as the baseline remote user interface technology within the Digital Living Network Alliance ( DLNA ) ( 2 ) , which is a large industry-wide effort for creating true interoperability between network-enabled devices . This paper provides a short overview of the Web4CE framework , and some of the use cases that it enables ."
"770","Querying and maintaining a compact XML storage As XML database sizes grow , the amount of space used for storing the data and auxiliary data structures becomes a major factor in query and update performance . This paper presents a new storage scheme for XML data that supports all navigational operations in near constant time . In addition to supporting efficient queries , the space requirement of the proposed scheme is within a constant factor of the information theoretic minimum , while insertions and deletions can be performed in near constant time as well . As a result , the proposed structure features a small memory footprint that increases cache locality , whilst still supporting standard APIs , such as DOM , and necessary database operations , such as queries and updates , efficiently . Analysis and experiments show that the proposed structure is space and time efficient ."
"771","Learning information diffusion process on the web Many text documents on the Web are not originally created but forwarded or copied from other source documents . The phenomenon of document forwarding or transmission between various web sites is denoted as Web information diffusion . This paper focuses on mining information diffusion processes for specific topics on the Web . A novel system called LIDPW is proposed to address this problem using matching learning techniques . The source site and source document of each document are identified and the diffusion process composed of a sequence of diffusion relationships is visually presented to users . The effectiveness of LIDPW is validated on a real data set . A preliminary user study is performed and the results show that LIDPW does benefit users to monitor the information diffusion process of a specific topic , and aid them to discover the diffusion start and diffusion center of the topic ."
"772","Communities from seed sets Expanding a seed set into a larger community is a common procedure in link-based analysis . We show how to adapt recent results from theoretical computer science to expand a seed set into a community with small conductance and a strong relationship to the seed , while examining only a small neighborhood of the entire graph . We extend existing results to give theoretical guarantees that apply to a variety of seed sets from specified communities . We also describe simple and flexible heuristics for applying these methods in practice , and present early experiments showing that these methods compare favorably with existing approaches ."
"773","Image annotation using search and mining technologies In this paper , we present a novel solution to the image annotation problem which annotates images using search and data mining technologies . An accurate keyword is required to initialize this process , and then leveraging a large-scale image database , it 1 ) searches for semantically and visually similar images , 2 ) and mines annotations from them . A notable advantage of this approach is that it enables unlimited vocabulary , while it is not possible for all existing approaches . Experimental results on real web images show the effectiveness and efficiency of the proposed algorithm ."
"774","SLL : running my web services on your WS platforms Today , the choice for a particular programming language limits the alternative products that can be used to deploy the program . For instance , a Java program must be executed using a Java VM . This limitation is particularly harmful for the emergence of a new programming paradigm like SOA and Web Services because platforms for new innovative programming languages are typically not as stable and mature as the established platforms for traditional programming paradigms . The purpose of this work is to break the strong ties between programming languages and runtime environments and thus make it possible to innovate at both ends independently . Thereby , the specific focus is on Web Services and Service-Oriented Architectures ; focusing on this domain makes it possible to achieve this goal with affordable efforts . The key idea is to introduce a Service Language Layer ( SLL ) which gives a high-level abstraction of a service-oriented program and which can easily and efficiently be executed on alternative Web Services platforms ."
"775","Certified email with a light on-line trusted third party : design and implementation This paper presents a new protocol for certified email . The protocol aims to combine security , scalability , easy implementation , and viable deployment . The protocol relies on a light on-line trusted third party ; it can be implemented without any special software for the receiver beyond a standard email reader and web browser , and does not require any public-key infrastructure ."
"776","Toward tighter integration of web search with a geographic information system Integration of Web search with geographic information has recently attracted much attention . There are a number of local Web search systems enabling users to find location-specific Web content . In this paper , however , we point out that this integration is still at a superficial level . Most local Web search systems today only link local Web content to a map interface . They are extensions of a conventional stand-alone geographic information system ( GIS ) , applied to a Web-based client-server architecture . In this paper , we discuss the directions available for tighter integration of Web search with a GIS , in terms of extraction , knowledge discovery , and presentation . We also describe implementations to support our argument that the integration must go beyond the simple map-and hyperlink architecture ."
"777","Visualizing tags over time We consider the problem of visualizing the evolution of tags within the Flickr ( flickr.com ) online image sharing community . Any user of the Flickr service may append a tag to any photo in the system . Over the past year , users have on average added over a million tags each week . Understanding the evolution of these tags over time is therefore a challenging task . We present a new approach based on a characterization of the most interesting tags associated with a sliding interval of time . An animation provided via Flash in a web browser allows the user to observe and interact with the interesting tags as they evolve over time . New algorithms and data structures are required to support the efficient generation of this visualization . We combine a novel solution to an interval covering problem with extensions to previous work on score aggregation in order to create an efficient backend system capable of producing visualizations at arbitrary scales on this large dataset in real time ."
"778","A framework for the server-side management of conversations with web services The emerging standards for the publication of Web Services are focused on the specification of the static interfaces of the operations to be invoked , or on the service composition . Few efforts have been made to specify the interaction between a Web Service and the individual consumer , although this aspect is essential to the successful service execution . In fact , while `` one-shot '' services may be invoked in a straight forward way , the invocation of services requiring complex interactions , where multiple messages are needed to complete the service , depends on the fact that the consumer respects the business logic of the Web Service . In this paper , we propose a framework for the server-side management of the interaction between a Web Service and its consumers . In our approach , the Web Service is in charge of assisting the consumer during the service invocation , by managing the interaction context and instructing the consumer about the operations that can be invoked and their actual parameters , at each step of the conversation . Our framework is based on the exchange of SOAP messages specifying the invocation of Java-based operations . Moreover , in order to support the interoperability with other software environments , the conversation flow specification is exported to a WSDL format that enables heterogeneous consumers to invoke the Web Service in a seamless way ."
"779","Access control enforcement for conversation-based web services Service Oriented Computing is emerging as the main approach to build distributed enterprise applications on the Web . The widespread use of Web services is hindered by the lack of adequate security and privacy support . In this paper , we present a novel framework for enforcing access control in conversation-based Web services . Our approach takes into account the conversational nature of Web services . This is in contrast with existing approaches to access control enforcement that assume a Web service as a set of independent operations . Furthermore , our approach achieves a tradeoff between the need to protect Web service 's access control policies and the need to disclose to clients the portion of access control policies related to the conversations they are interested in . This is important to avoid situations where the client can not progress in the conversation due to the lack of required security requirements . We introduce the concept of k-trustworthiness that defines the conversations for which a client can provide credentials maximizing the likelihood that it will eventually hit a final state ."
"780","Finding group shilling in recommendation system In the age of information explosion , recommendation system has been proved effective to cope with information overload in e-commerce area . However , unscrupulous producers shill the systems in many ways to make profit , and it makes the system imprecise and unreliable in a long term . Among many shilling behaviors , a new form of attack , called group shilling , appears and does great harm to the system . Because group shilling users are now well organized and become more hidden among various normal users , it is hard to find them by traditional methods . However , these group shilling users are similar to some extent , for they both shill the target items . We bring out a similarity spreading algorithm to find these group shilling users and protect recommendation system from unfair ratings . In our algorithm , we try to find these cunning group shilling users through propagating similarities from items to users iteratively . The experiment shows our similarity spreading algorithm improves the precision of the system and provides the system a reliable protection ."
"781","A scalable application placement controller for enterprise data centers Given a set of machines and a set of Web applications with dynamically changing demands , an online application placement controller decides how many instances to run for each application and where to put them , while observing all kinds of resource constraints . This NP hard problem has real usage in commercial middleware products . Existing approximation algorithms for this problem can scale to at most a few hundred machines , and may produce placement solutions that are far from optimal when system resources are tight . In this paper , we propose a new algorithm that can produce within 30seconds high-quality solutions for hard placement problems with thousands of machines and thousands of applications . This scalability is crucial for dynamic resource provisioning in large-scale enterprise data centers . Our algorithm allows multiple applications to share a single machine , and strivesto maximize the total satisfied application demand , to minimize the number of application starts and stops , and to balance the load across machines . Compared with existing state-of-the-art algorithms , for systems with 100 machines or less , our algorithm is up to 134 times faster , reduces application starts and stops by up to 97 % , and produces placement solutions that satisfy up to 25 % more application demands . Our algorithm has been implemented and adopted in a leading commercial middleware product for managing the performance of Web applications ."
"782","Towards efficient dominant relationship exploration of the product items on the web In recent years , there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking . Unfortunately , current search engines can not effectively rank those relational data , which exists on dynamic websites supported by online databases . In this study , to rank such structured data ( i.e. , find the `` best '' items ) , we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data . Efficient querying strategies and updating scheme are devised to facilitate the ranking process . Extensive experiments illustrate the effectiveness and efficiency of our methods . As such , we believe the work in this poster can be complementary to traditional search engines ."
"783","Online mining of frequent query trees over XML data streams In this paper , we proposed an online algorithm , called FQT-Stream ( Frequent Query Trees of Streams ) , to mine the set of all frequent tree patterns over a continuous XML data stream . A new numbering method is proposed to represent the tree structure of a XML query tree . An effective sub-tree numeration approach is developed to extract the essential information from the XML data stream . The extracted information is stored in an effective summary data structure . Frequent query trees are mined from the current summary data structure by a depth-first-search manner ."
"784","Composite event queries for reactivity on the web Reactivity on the Web is an emerging issue . The capability to automatically react to events ( such as updates to Web resources ) is essential for both Web services and Semantic Web systems . Such systems need to have the capability to detect and react to complex , real life situations . This presentation gives flavors of the high-level language XChange , for programming reactive behavior on the Web ."
"785","Composite events for xml Recently , active behavior has received attention in the XML field to automatically react to occurred events . Aside from proprietary approaches for enriching XML with active behavior , the W3C standardized the Document Object Model ( DOM ) Event Module for the detection of events in XML documents . When using any of these approaches , however , it is often impossible to decide which event to react upon because not a single event but a combination of multiple events , i.e. , a composite event determines a situation to react upon . The paper presents the first approach for detecting composite events in XML documents by addressing the peculiarities of XML events which are caused by their hierarchical order in addition to their temporal order . It also provides for the detection of satisfied multiplicity constraints defined by XML schemas . Thereby the approach enables applications operating on XML documents to react to composite events which have richer semantics ."
"786","Turning portlets into services : the consumer profile Portlets strive to play at the front end the same role that Web services currently enjoy at the back end , namely , enablers of application assembly through reusable services . However , it is well-known in the component community that , the larger the component , the more reduced the reuse . Hence , the coarse-grained nature of portlets ( they encapsulate also the presentation layer ) can jeopardize this vision of portlets as reusable services . To avoid this situation , this work proposes a perspective shift in portlet development by introducing the notion of Consumer Profile . While the user profile characterizes the end user ( e.g. age , name , etc ) , the Consumer Profile captures the idiosyncrasies of the organization through which the portlet is being delivered ( e.g. the portal owner ) as far as the portlet functionality is concerned . The user profile can be dynamic and hence , requires the portlet to be customized at runtime . By contrast , the Consumer Profile is known at registration time , and it is not always appropriate\/possible to consider it at runtime . Rather , it is better to customize the code at development time , and produce an organization-specific portlet which built-in , custom functionality . In this scenario , we no longer have a portlet but a family of portlets , and the portlet provider becomes the `` assembly line '' of this family . This work promotes this vision by introducing an organization-aware , WSRPcompliant architecture that let portlet consumers registry and handle `` family portlets '' in the same way that `` traditional portlets '' . In so doing , portlets are nearer to become truly reusable services ."
"787","Index structures and algorithms for querying distributed RDF repositories A technical infrastructure for storing , querying and managing RDFdata is a key element in the current semantic web development . Systems like Jena , Sesame or the ICS-FORTH RDF Suite are widelyused for building semantic web applications . Currently , none ofthese systems supports the integrated querying of distributed RDF repositories . We consider this a major shortcoming since the semanticweb is distributed by nature . In this paper we present an architecture for querying distributed RDF repositories by extending the existing Sesame system . We discuss the implications of our architectureand propose an index structure as well as algorithms forquery processing and optimization in such a distributed context ."
"788","Learning to detect phishing emails Each month , more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information , logon credentials , and identity information in general . This attack method , commonly known as `` phishing , '' is most commonly initiated by sending out emails with links to spoofed websites that harvest information . We present a method for detecting these attacks , which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication . This method is applicable , with slight modification , to detection of phishing websites , or the emails used to direct victims to these sites . We evaluate this method on a set of approximately 860 such phishing emails , and 6950 non-phishing emails , and correctly identify over 96 % of the phishing emails while only mis-classifying on the order of 0.1 % of the legitimate emails . We conclude with thoughts on the future for such techniques to specifically identify deception , specifically with respect to the evolutionary nature of the attacks and information available ."
"789","Invisible participants : how cultural capital relates to lurking behavior The asymmetry of activity in virtual communities is of great interest . While participation in the activities of virtual communities is crucial for a community 's survival and development , many people prefer lurking , that is passive attention over active participation . Lurking can be measured and perhaps affected by both dispositional and situational variables . This work investigates the concept of cultural capital as situational antecedent of lurking and de-lurking ( the decision to start posting after a certain amount of lurking time ) . Cultural capital is defined as the knowledge that enables an individual to interpret various cultural codes . The main hypothesis states that a user 's cultural capital affects her level of activity in a community and her decision to de-lurk and cease to exist in very active communities because of information overload . This hypothesis is analyzed by mathematically defining a social communication network ( SCN ) of activities in authenticated discussion forums . We validate this model by examining the SCN using data collected in a sample of 636 online forums in Open University in Israel and 2 work based communities from IBM . The hypotheses verified here make it clear that fostering receptive participation may be as important and constructive as encouraging active contributions in online communities ."
"790","Combining classifiers to identify online databases We address the problem of identifying the domain of onlinedatabases . More precisely , given a set F of Web forms automaticallygathered by a focused crawler and an online databasedomain D , our goal is to select from F only the formsthat are entry points to databases in D. Having a set ofWebforms that serve as entry points to similar online databasesis a requirement for many applications and techniques thataim to extract and integrate hidden-Web information , suchas meta-searchers , online database directories , hidden-Webcrawlers , and form-schema matching and merging . We propose a new strategy that automatically and accuratelyclassifies online databases based on features that canbe easily extracted from Web forms . By judiciously partitioningthe space of form features , this strategy allows theuse of simpler classifiers that can be constructed using learningtechniques that are better suited for the features of eachpartition . Experiments using real Web data in a representativeset of domains show that the use of different classifiersleads to high accuracy , precision and recall . This indicatesthat our modular classifier composition provides an effectiveand scalable solution for classifying online databases ."
"791","Google news personalization : scalable online collaborative filtering Several approaches to collaborative filtering have been studied but seldom have studies been reported for large ( several millionusers and items ) and dynamic ( the underlying item set is continually changing ) settings . In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News . We generate recommendations using three approaches : collaborative filtering using MinHash clustering , Probabilistic Latent Semantic Indexing ( PLSI ) , and covisitation counts . We combine recommendations from different algorithms using a linear model . Our approach is content agnostic and consequently domain independent , making it easily adaptable for other applications and languages with minimal effort . This paper will describe our algorithms and system setup in detail , and report results of running the recommendations engine on Google News ."
"792","Using web browser interactions to predict task The automatic identification of a user 's task has the potential to improve information filtering systems that rely on implicit measures of interest and whose effectiveness may be dependant upon the task at hand . Knowledge of a user 's current task type would allow information filtering systems to apply the most useful measures of user interest . We recently conducted a field study in which we logged all participants ' interactions with their web browsers and asked participants to categorize their web usage according to a high-level task schema . Using the data collected during this study , we have conducted a preliminary exploration of the usefulness of logged web browser interactions to predict users ' tasks . The results of this initial analysis suggest that individual models of users ' web browser interactions may be useful in predicting task type ."
"793","Optimizing scoring functions and indexes for proximity search in type-annotated corpora We introduce a new , powerful class of text proximity queries : find an instance of a given `` answer type '' ( person , place , distance ) near `` selector '' tokens matching given literals or satisfying given ground predicates . An example query is type = distance NEAR Hamburg Munich . Nearness is defined as a flexible , trainable parameterized aggregation function of the selectors , their frequency in the corpus , and their distance from the candidate answer . Such queries provide a key data reduction step for information extraction , data integration , question answering , and other text-processing applications . We describe the architecture of a next-generation information retrieval engine for such applications , and investigate two key technical problems faced in building it . First , we propose a new algorithm that estimates a scoring function from past logs of queries and answer spans . Plugging the scoring function into the query processor gives high accuracy : typically , an answer is found at rank 2-4 . Second , we exploit the skew in the distribution over types seen in query logs to optimize the space required by the new index structures required by our system . Extensive performance studies with a 10GB , 2-million document TREC corpus and several hundred TREC queries show both the accuracy and the efficiency of our system . From an initial 4.3 GB index using 18,000 types from WordNet , we can discard 88 % of the space , while inflating query times by a factor of only 1.9 . Our final index overhead is only 20 % of the total index space needed ."
"794","Transforming web contents into a storybook with dialogues and animations This paper describes a medium , called Interactive e-Hon , for helping children to understand contents from the Web . It works by transforming electronic contents into an easily understandable `` storybook world . '' In this world , easy-to-understand contents are generated by creating 3D animations that include contents and metaphors , and by using a child-parent model with dialogue expression and a question-answering style comprehensible to children ."
"795","Semi-automated adaptation of service interactions In today 's Web , many functionality-wise similar Web services are offered through heterogeneous interfaces ( operation definitions ) and business protocols ( ordering constraints defined on legal operation invocation sequences ) . The typical approach to enable interoperation in such a heterogeneous setting is through developing adapters . There have been approaches for classifying possible mismatches between service interfaces and business protocols to facilitate adapter development . However , the hard job is that of identifying , given two service specifications , the actual mismatches between their interfaces and business protocols . In this paper we present novel techniques and a tool that provides semi-automated support for identifying and resolution of mismatches between service interfaces and protocols , and for generating adapter specification . We make the following main contributions : ( i ) we identify mismatches between service interfaces , which leads to finding mismatches of type of signature , merge\/split , and extra\/missing messages ; ( ii ) we identify all ordering mismatches between service protocols and generate a tree , called mismatch tree , for mismatches that require developers ' input for their resolution . In addition , we provide semi-automated support in analyzing the mismatch tree to help in resolving such mismatches . We have implemented the approach in a tool inside IBM WID ( WebSphere Integration Developer ) . Our experiments with some real-world case studies show the viability of the proposed approach . The methods and tool are significant in that they considerably simplify the problem of adapting services so that interoperation is possible ."
"796","Design and development of learning management system at universiti Putra Malaysia : a case study of e-SPRINT This paper reports the design and development of the e-SPRINT , Learning Management System , which has been derived from Sistem Pengurusan Rangkaian Integrasi Notakuliah dalam Talian - mod Elektronik ) and currently being implemented at Universiti Putra Malaysia ( UPM ) . The e-SPRINT was developed by utilizing PERL ( Practical Extraction and Report Language ) and was supported by standard database in Linux\/UNIX environment operating system . The system is currently being used to supplement and complement part of the classroom-based teaching . This paper covers the architecture and features of the e-SPRINT system which consists of five main modules . Some general issues and challenges of such e-learning initiatives implementation will also be discussed ."
"797","Protecting electronic commerce from distributed denial-of-service attacks It is widely recognized that distributed denial-of-service ( DDoS ) attacks can disrupt electronic commerce and cause large revenue losses . However , effective defenses continue to be mostly unavailable . We describe and evaluate VIPnet , a novel value-added network service for protecting e-commerce and other transaction-based sites from DDoS attacks . In VIPnet , e-merchants pay Internet Service Providers ( ISPs ) to carry the packets of the e-merchants ' best clients ( called VIPs ) in a privileged class of service ( CoS ) , protected from congestion , whether malicious or not , in the regular CoS. . VIPnet rewards VIPs with not only better quality of service , but also greater availability . Because VIP rights are client - and server-specific , can not be forged , are usage-limited , and are only replenished after successful client transactions ( e.g. , purchases ) , it is impractical for attackers to mount and sustain DDoS attacks against an e-merchant 's VIPs . VIPnet can be deployed incrementally and does not require universal adoption . Experiments demonstrate VIPnet 's benefits ."
"798","Efficient training on biased minimax probability machine for imbalanced text classification The Biased Minimax Probability Machine ( BMPM ) constructs a classifier which deals with the imbalanced learning tasks . In this paper , we propose a Second Order Cone Programming ( SOCP ) based algorithm to train the model . We outline the theoretical derivatives of the biased classification model , and address the text classification tasks where negative training documents significantly outnumber the positive ones using the proposed strategy . We evaluated the learning scheme in comparison with traditional solutions on three different datasets . Empirical results have shown that our method is more effective and robust to handle imbalanced text classification problems ."
"799","Providing session management as core business service It is extremely hard for a global organization with services over multiple channels to capture a consistent and unified view of its data , services , and interactions . While SOA and web services are addressing integration and interoperability problems , it is painful for an operational organization with legacy systems to quickly switch to service-based methods . We need methods to combine advantages of traditional ( i.e. web , desktop , or mobile ) application development environments and service-based deployments . In this paper , we focus on the design and implementation of session management as a core service to support business processes and go beyond application-specific sessions and web sessions . We develop local session components for different platforms and complement them with a remote `` session service '' that is independent of applications and platforms . We aim to close the gap between the two worlds by combining their performance , availability and interoperability advantages ."
"800","Selective hypertext induced topic search We address the problem of answering broad-topic queries on the World Wide Web . We present a link based analysis algorithm SelHITS , which is an improvement over Kleinberg 's HITS ( 2 ) algorithm . We introduce the concept of virtual links to exploit the latent information in the hyperlinked environment . We propose a novel approach to calculate hub and authority values . We also present a selective expansion method which avoids topic drift and provides results consistent with only one interpretation of the query , even if the query is ambiguous . Initial experimental evaluation and user feedback show that our algorithm indeed distills the most important and relevant pages for broad-topic queries . We also infer that there exists a uniform notion of quality of search results within users ."
"801","Text-based video blogging A video blogging system has been developed for easily producing your own video programs that can be made available to the public in much the same way that blogs are created . The user merely types a program script on a webpage , the same as creating a blog , selects a direction style , and pastes in some additional material content to create a CG-based video program that can be openly distributed to the general public . The script , direction style , and material content are automatically combined to create a movie file on the server side . The movie file can then be accessed by referring to an RSS feed and viewed on the screens of various devices ."
"802","On ranking techniques for desktop search This paper addresses the desktop search problem by considering varioustechniques for ranking results of a search query over thefile system . First , basic ranking techniques , which are based ona single file feature ( e.g. , file name , file content , access date , etc. ) are considered . Next , two learning-based ranking schemes are presented , and are shown to be significantly more effective than the basic ranking methods . Finally , a novel ranking technique , based on query selectiveness is considered , for use during the cold-start period of the system . This method isalso shown to be empirically effective , even though it does notinvolve any learning ."
"803","Hybrid semantic tagging for information extraction The semantic web is expected to have an impact at least as big as that of the existing HTML based web , if not greater . However , the challenge lays in creating this semantic web and in converting existing web information into the semantic paradigm . One of the core technologies that can help in migration process is automatic markup , the semantic markup of content , providing the semantic tags to describe the raw content . This paper describes a hybrid statistical and knowledge-based information extraction model , able to extract entities and relations at the sentence level . The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labor by relying on statistics drawn from a training corpus . The implementation of the model , called TEG ( Trainable Extraction Grammar ) , can be adapted to any IE domain by writing a suitable set of rules in a SCFG ( Stochastic Context Free Grammar ) based extraction language , and training them using an annotated corpus . The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems , while requiring orders of magnitude less manual rule writing and smaller amount of training data . We also demonstrate the robustness of our system under conditions of poor training data quality . This makes the system very suitable for converting legacy web pages to semantic web pages ."
"804","Using semantic rules to determine access control for web services Semantic Web technologies are bring increasingly employed to solve knowledge management issues in traditional Web technologies . This paper follows that trend and proposes using Semantic rule languages to construct rules for defining access control rules for Web Services . Using these rules , a system will be able to manage access to Web Services and also the information accessed via these services ."
"805","An information state-based dialogue manager for making voice web smarter In this paper we propose the integration of intelligent components technologies ( natural language and discourse management ) in voice web interfaces to make them smarter . We describe how we have integrated reusable components of dialogue management and language processing in a multilingual voice system to improve its friendliness and portability . The dialogue management component deals with complex dialogue phenomena , such as user-initiative dialogues , and follows the information state-based theory . The resulting dialogue system supports friendly communication ( through the telephone and the web ) in several languages : English , Spanish , Catalan and Italian . The dialogue system has been adapted to guide the users to access online public administration services ."
"806","A link classification based approach to website topic hierarchy generation Hierarchical models are commonly used to organize a Website 's content . A Website 's content structure can be represented by a topic hierarchy , a directed tree rooted at a Website 's homepage in which the vertices and edges correspond to Web pages and hyperlinks . In this work , we propose a new method for constructing the topic hierarchy of a Website . We model the Website 's link structure using weighted directed graph , in which the edge weights are computed using a classifier that predicts if an edge connects a pair of nodes representing a topic and a sub-topic . We then pose the problem of building the topic hierarchy as finding the shortest-path tree and directed minimum spanning tree in the weighted graph . We 've done extensive experiments using real Websites and obtained very promising results ."
"807","One document to bind them : combining XML , web services , and the semantic web We present a paradigm for uniting the diverse strands of XML-based Web technologies by allowing them to be incorporated within a single document . This overcomes the distinction between programs and data to make XML truly `` self-describing . '' A proposal for a lightweight yet powerful functional XML vocabulary called `` Semantic fXML '' is detailed , based on the well-understood functional programming paradigm and resembling the embedding of Lisp directly in XML . Infosets are made `` dynamic , '' since documents can now directly embed local processes or Web Services into their Infoset . An optional typing regime for info-sets is provided by Semantic Web ontologies . By regarding Web Services as functions and the Semantic Web as providing types , and tying it all together within a single XML vocabulary , the Web can compute . In this light , the real Web 2.0 can be considered the transformation of the Web from a universal information space to a universal computation space ."
"808","Texquery : a full-text search extension to xquery One of the key benefits of XML is its ability to represent a mix of structured and unstructured ( text ) data . Although current XML query languages such as XPath and XQuery can express rich queries over structured data , they can only express very rudimentary queries over text data . We thus propose TeXQuery , which is a powerful full-text search extension to XQuery . TeXQuery provides a rich set of fully composable full-text search primitives , such as Boolean connectives , phrase matching , proximity distance , stemming and thesauri . TeXQuery also enables users to seamlessly query over both structured and text data by embedding TeXQuery primitives in XQuery , and vice versa . Finally , TeXQuery supports a flexible scoring construct that can be used toscore query results based on full-text predicates . TeXQuery is the precursor ofthe full-text language extensions to XPath 2.0 and XQuery 1.0 currently being developed by the W3C ."
"809","Using symbolic objects to cluster web documents Web Clustering is useful for several activities in the WWW , from automatically building web directories to improve retrieval performance . Nevertheless , due to the huge size of the web , a linear mechanism must be employed to cluster web documents . The k-means is one classic algorithm used in this problem . We present a variant of the vector model to be used with the k-means algorithm . Our representation uses symbolic objects for clustering web documents . Some experiments were done with positive results and future work is optimistic ."
"810","An integrated method for social network extraction A social network can become bases for information infrastructure in the future . It is important to extract social networks that are not biased . Providing a simple means for users to register their social relation is also important . We propose a method that combines various approaches to extract social networks . Especially , three kinds of networks are extracted ; user-registered Know link network , Web-mined Web link network , and face-to-face Touch link network . In this paper , the combination of social network extraction for communities is described , and the analysis on the extracted social networks is shown ."
"811","Capturing the essentials of federated systems Today , the Web is increasingly used as a platform for distributed services , which transcend organizational boundaries to form federated applications . Consequently , there is a growing interest in the architectural aspect of Web-based systems , i.e. the composition of the overall solution into individual Web applications and Web services from different parties . The design and evolution of federated systems calls for models that give an overview of the structural as well as trust-specific composition and reflect the technical details of the various accesses . We introduce the WebComposition Architecture Model ( WAM ) as an overall modeling approach tailored to aspects of highly distributed systems with federation as an integral factor ."
"812","Exploring social annotations for the semantic web In order to obtain a machine understandable semantics for web resources , research on the Semantic Web tries to annotate web resources with concepts and relations from explicitly defined formal ontologies . This kind of formal annotation is usually done manually or semi-automatically . In this paper , we explore a complement approach that focuses on the `` social annotations of the web '' which are annotations manually made by normal web users without a pre-defined formal ontology . Compared to the formal annotations , although social annotations are coarse-grained , informal and vague , they are also more accessible to more people and better reflect the web resources ' meaning from the users ' point of views during their actual usage of the web resources . Using a social bookmark service as an example , we show how emergent semantics ( 2 ) can be statistically derived from the social annotations . Furthermore , we apply the derived emergent semantics to discover and search shared web bookmarks . The initial evaluation on our implementation shows that our method can effectively discover semantically related web bookmarks that current social bookmark service can not discover easily ."
"813","Image classification for mobile web browsing It is difficult for users of mobile devices such as cellular phones equipped with a small screen and a poor input interface to browse Web pages designed for desktop PCs with large displays . Many studies and commercial products have tried to solve this problem . Web pages include images that have various roles such as site menus , line headers for itemization , and page titles . However , most studies of mobile Web browsing have n't paid much attention to the roles of Web images . In this paper , we define eleven Web image categories according to their roles and use these categories for proper Web image handling . We manually categorized 3,901 Web images collected from forty Web sites and extracted image features of each category according to the classification . By making use of the extracted features , we devised an automatic Web image classification method . Furthermore , we evaluated the automatic classification of real Web pages and achieved up to 83.1 % classification accuracy . We also implemented an automatic Web page scrolling system as an application of our automatic image classification method ."
"814","Netprobe : a fast and scalable system for fraud detection in online auction networks Given a large online network of online auction users and their histories of transactions , how can we spot anomalies and auction fraud ? This paper describes the design and implementation of NetProbe , a system that we propose for solving this problem . NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create , and employs a Belief Propagation mechanism to detect likely fraudsters . Our experiments show that NetProbe is both efficient and effective for fraud detection . We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges , where NetProbe was able to spot fraudulent nodes with over 90 % precision and recall , within a matter of seconds . We also report experiments on a real dataset crawled from eBay , with nearly 700,000 transactions between more than 66,000 users , where NetProbe was highly effective at unearthing hidden networks of fraudsters , within a realistic response time of about 6 minutes . For scenarios where the underlying data is dynamic in nature , we propose IncrementalNetProbe , which is an approximate , but fast , variant of NetProbe . Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe , while retaining over 99 % of its accuracy ."
"815","A storage and indexing framework for p2p systems We present a modularized storage and indexing framework that cleanly separates the functional components of a P2P system , enabling us to tailor the P2P infrastructure to the specific needs of various Internet applications eat , without having to devise completely new storage management and index structures for each application ."
"816","Model checking cobweb protocols for verification of HTML frames behavior HTML documents composed of frames can be difficult to write correctly . We demonstrate a technique that can be used by authors manually creating HTML documents ( or by document editors ) to verify that complex frame construction exhibits the intended behavior when browsed . The method is based on model checking ( an automated program verification technique ) , and on temporal logic specifications of expected frames behavior . We show how to model the HTML frames source as a CobWeb protocol , related to the Trellis model of hypermedia documents . We show how to convert the CobWeb protocol to input for a model checker , and discuss several ways for authors to create the necessary behavior specifications . Our solution allows Web documents to be built containing a large number of frames and content pages interacting in complex ways . We expect such Web structures to be more useful in `` literary '' hypermedia than for Web `` sites '' used as interfaces to organizational information or databases ."
"817","A management and performance framework for semantic web servers The unification of Semantic Web query languages under the SPARQL standard and the development of commercial-quality implementations are encouraging industries to use semantic technologies for managing information . Current implementations , however , lack the performance monitoring and management services that the industry expects . In this paper , we present a performance and management framework interface to a generic SPARQL web server . We leverage existing standards for instrumentation to make the system ready-to-manage through existing monitoring applications , and we provide a performance framework which has the distinct feature of providing measurement results through the same SPARQL interface used to query data , eliminating the need for special interfaces ."
"818","The ScratchPad : sensemaking support for the web The World Wide Web is a powerful platform for a wide range of information tasks . Dramatic advances in technology , such as improved search capabilities and the AJAX application model , have enabled entirely new web-based applications and usage patterns , making many tasks easier to perform than ever before . However , few tools have been developed to assist with sensemaking tasks : complex research behaviors in which users gather and comprehend information from many sources to answer potentially vague , non-procedural questions . Sensemaking tasks are common and include , for example , researching vacation destinations or deciding how to invest . This paper presents the ScratchPad , an extension to the standard browser interface that is designed to capture , organize , and exploit the information discovered while performing a sensemaking task ."
"819","Multiway SLCA-based keyword search in XML data Keyword search for smallest lowest common ancestors ( SLCAs ) in XML data has recently been proposed as a meaningful way to identify interesting data nodes inXML data where their subtrees contain an input set of keywords . In this paper , we generalize this useful search paradigm to support keyword search beyond the traditional AND semantics to include both AND and OR boolean operators as well . We first analyze properties of the LCA computation and propose improved algorithms to solve the traditional keyword search problem ( with only AND semantics ) . We then extend our approach to handle general keyword search involving combinations of AND and OR boolean operators . The effectiveness of our new algorithms is demonstrated with a comprehensive experimental performance study ."
"820","FLUX : fuzzy content and structure matching of XML range queries An XML range query may impose predicates on the numerical or textual contents of the elements and\/or their respective path structures . In order to handle content and structure range queries efficiently , an XML query processing engine needs to incorporate effective indexing and summarization techniques to efficiently partition the XML document and locate the results . In this paper , we propose a dynamic summarization and indexing method , FLUX , based on Bloom filters and B + - trees to tackle these problems . The results of our extensive experimental evaluations indicated the efficiency of the proposed system ."
"821","Information flow using edge stress factor This paper shows how a corpus of instant messages can be employed to detect de facto communities of practice automatically . A novel algorithm based on the concept of Edge Stress Factor is proposed and validated . Results show that this approach is fast and effective in studying collaborative behavior ."
"822","Geotracker : geospatial and temporal RSS navigation The Web is rapidly moving towards a platform for mass collaboration in content production and consumption . Fresh content on a variety of topics , people , and places is being created and made available on the Web at breathtaking speed . Navigating the content effectively not only requires techniques such as aggregating various RSS-enabled feeds , but it also demands a new browsing paradigm . In this paper , we present novel geospatial and temporal browsing techniques that provide users with the capability of aggregating and navigating RSS-enabled content in a timely , personalized and automatic manner . In particular , we describe a system called GeoTracker that utilizes both a geospatial representation and a temporal ( chronological ) presentation to help users spot the most relevant updates quickly . Within the context of this work , we provide a middleware engine that supports intelligent aggregation and dissemination of RSS feeds with personalization to desktops and mobile devices . We study the navigation capabilities of this system on two kinds of data sets , namely , 2006 World Cup soccer data collected over two months and breaking news items that occur every day . We also demonstrate that the application of such technologies to the video search results returned by YouTube and Google greatly enhances a user . s ability in locating and browsing videos based on his or her geographical interests . Finally , we demonstrate that the location inference performance of GeoTracker compares well against machine learning techniques used in the natural language processing\/information retrieval community . Despite its algorithm simplicity , it preserves high recall percentages ."
"823","Geographic locations of web servers The ccTLD ( country code Top Level Domain ) in a URL does not necessarily point to the geographic location of the server concerned . The authors have surveyed sample servers belonging to 60 ccTLDs in Africa , with regard to the number of hops required to reach the target site from Japan , the response time , and the NIC registration information of each domain . The survey has revealed the geographical distribution of server sites as well as their connection environments . It has been found that the percentage of offshore ( out of home country ) servers is as high as 80 % and more than half of these are located in Europe . Offshore servers not only provide little benefit to the people of the country to which each ccTLD rightly belongs but their existence also heightens the risk of a country being unable to control them with its own policies and regulations . Offshore servers constitute a significant aspect of the digital divide problem ."
"824","Exploring social dynamics in online media sharing It is now feasible to view media at home as easily as text-based pages were viewed when the World Wide Web ( WWW ) first emerged . This development has supported media sharing and search services providing hosting , indexing and access to large , online media repositories . Many of these sharing services also have a social aspect to them . This paper provides an initial analysis of the social interactions on a video sharing and search service ( www.youtube.com ) . Results show that many users do not form social networks in the online community and a very small number do not appear to contribute to the wider community . However , it does seem those people who do use the available tools have much a greater tendency to form social connections ."
"825","Exploit sequencing views in semantic cache to accelerate xpath query evaluation In XML databases , materializing queries and their results into views in a semantic cache can improve the performance of query evaluation by reducing computational complexity and I\/O cost . Although there are a number of proposals of semantic cache for XML queries , the issues of fast cache lookup and compensation query construction could be further studied . In this paper , based on sequential XPath queries , we propose fastCLU , a fast Cache LookUp algorithm and effiCQ , an efficient Compensation Query constructing algorithm to solve these two problems . Experimental results show that our algorithms outperform previous algorithms and can achieve good performance of query evaluation ."
"826","GlobeDB : autonomic data replication for web applications We present GlobeDB , a system for hosting Web applications that performs autonomic replication of application data . GlobeDB offers data-intensive Web applications the benefits of low access latencies and reduced update traffic . The major distinction in our system compared to existing edge computing infrastructures is that the process of distribution and replication of application data is handled by the system automatically with very little manual administration . We show that significant performance gains can be obtained this way . Performance evaluations with the TPC-W benchmark over an emulated wide-area network show that GlobeDB reduces latencies by a factor of 4 compared to non-replicated systems and reduces update traffic by a factor of 6 compared to fully replicated systems ."
"827","Automatic search engine performance evaluation with click-through data analysis Performance evaluation is an important issue in Web search engine researches . Traditional evaluation methods rely on much human efforts and are therefore quite time-consuming . With click-through data analysis , we proposed an automatic search engine performance evaluation method . This method generates navigational type query topics and answers automatically based on search users . querying and clicking behavior . Experimental results based on a commercial Chinese search engine 's user logs show that the automatically method gets a similar evaluation result with traditional assessor-based ones ."
"828","Communication as information-seeking : the case for mobile social software for developing regions In this paper , we describe several findings from a multi-year , multi-method study of how information and communication technologies have been adopted and adapted in Central Asia . We have found that mobile phone usage is outpacing the rate of Internet adoption , that access to the Internet is primarily through public access sites carrying with it issues regarding privacy and surveillance , that people rely on their social networks as information sources , that public institutions tend to be fairly weak as citizen resources , and that information seeking and communication are conflated in people 's usage patterns with different technologies . In addition , in the developed world social networking software has grown rapidly and shown itself to have significant potential for mobilizing a population . Based on the collection of findings from Central Asia and observing patterns of technology usage in other parts of the world , our research leads to the conclusion that exploring mobile social software holds significant potential as an ICT that meshes well with preexisting patterns of communication and information seeking and also leverages the most predominant pattern of technology adoption . Many of the findings from this research echo results from studies in other geographic areas , and so we anticipate that much of this research will be relevant to developing regions generally ."
"829","SPARQ2L : towards support for subgraph extraction queries in rdf databases Many applications in analytical domains often have the need to `` connect the dots '' i.e. , query about the structure of data . In bioinformatics for example , it is typical to want to query about interactions between proteins . The aim of such queries is to `` extract '' relationships between entities i.e. paths from a data graph . Often , such queries will specify certain constraints that qualifying results must satisfy e.g. paths involving a set of mandatory nodes . Unfortunately , most present day Semantic Web query languages including the current draft of the anticipated recommendation SPARQL , lack the ability to express queries about arbitrary path structures in data . In addition , many systems that support some limited form of path queries rely on main memory graph algorithms limiting their applicability to very large scale graphs . In this paper , we present an approach for supporting Path Extraction queries . Our proposal comprises ( i ) a query language SPARQ2L which extends SPARQL with path variables and path variable constraint expressions , and ( ii ) a novel query evaluation framework based on efficient algebraic techniques for solving path problems which allows for path queries to be efficiently evaluated on disk resident RDF graphs . The effectiveness of our proposal is demonstrated by a performance evaluation of our approach on both real world based and synthetic dataset ."
"830","A pruning-based approach for supporting Top-K join queries An important issue arising from large scale data integration is how to efficiently select the top-K ranking answers from multiple sources while minimizing the transmission cost . This paper resolves this issue by proposing an efficient pruning-based approach to answer top-K join queries . The total amount of transmitted data can be greatly reduced by pruning tuples that can not produce the desired join results with a rank value greater than or equal to the rank value generated so far ."
"831","Topic-oriented query expansion for web search The contribution of this paper includes three folders : ( 1 ) To introduce a topic-oriented query expansion model based on the Information Bottleneck theory that classify terms into distinct topical clusters in order to find out candidate terms for the query expansion . ( 2 ) To define a term-term similarity matrix that is available to improve the term ambiguous problem . ( 3 ) To propose two measures , intracluster and intercluster similarities , that are based on proximity between the topics represented by two clusters in order to evaluate the retrieval effectiveness . Results of several evaluation experiments in Web search exhibit the average intracluster similarity was improved for the gain of 79.1 % while the average intercluster similarity was decreased for the loss of 36.0 % ."
"832","A framework for XML data streams history checking and monitoring The need of formal verification is a problem that involves all the fields in which sensible data are managed . In this context the verification of data streams became a fundamental task . The purpose of this paper is to present a framework , based on the model checker SPIN , for the verification of data streams . The proposed method uses a linear temporal logic , called TRIO , to describe data constraints and properties . Constraints are automatically translated into Promela , the input language of the model checker SPIN in order to verify them ."
"833","Web mashup scripting language The Web Mashup Scripting Language ( WMSL ) enables an end-user ( you ) working from his browser , e.g. not needing any other infrastructure , to quickly write mashups that integrate any two , or more , web services on the Web . The end-user accomplishes this by writing a web page that combines HTML , metadata in the form of mapping relations , and small piece of code , or script . The mapping relations enable not only the discovery and retrieval of the WMSL pages , but also affect a new programming paradigm that abstracts many programming complexities from the script writer . Furthermore , the WMSL Web pages or scripts that disparate end-users ( you ) write , can be harvested by Crawlers to automatically generate the concepts needed to build lightweight ontologies containing local semantics of a web service and its data model , to extend context ontologies or middle ontologies , and to develop links , or mappings , between these ontologies . This enables an open-source model of building ontologies based on the WMSL Web page or scripts that end users ( you ) write ."
"834","Query topic detection for reformulation In this paper , we show that most multiple term queries include more than one topic and users usually reformulate their queries by topics instead of terms . In order to provide empirical evidence on user 's reformulation behavior and to help search engines better handle the query reformulation problem , we focus on detecting internal topics in the original query and analyzing users . reformulation to those topics . Particularly , we utilize the Interaction Information ( II ) to measure the degree of one sub-query being a topic based on the local search results . The experimental results on query log show that : most users reformulate query at the topical level ; and our proposed II-based algorithm is a good method to detect topics from original queries ."
"835","Automatic extraction of web search interfaces for interface schema integration This paper provides an overview of a technique for extracting information from the Web search interfaces of e-commerce search engines that is useful for supporting automatic search interface integration . In particular , we discuss how to group elements and labels on a search interface into attributes and how to derive certain meta-information for each attribute ."
"836","Improving portlet interoperability through deep annotation Portlets ( i.e. multi-step , user-facing applications to be syndicated within a portal ) are currently supported by most portal frameworks . However , there is not yet a definitive answer to portlet interoperation whereby data flows smoothly from one portlet to a neighbouring one . Both data-based and API-based approaches exhibit some drawbacks in either the limitation of the sharing scope or the standardization effort required . We argue that these limitations can be overcome by using deep annotation techniques . By providing additional markup about the background services , deep annotation strives to interact with these underlying services rather than with the HTML surface that conveys the markup . In this way , the portlet producer can extend a portlet markup , a fragment , with data about the processes whose rendering this fragment supports . Then , the portlet consumer ( e.g. a portal ) can use deep annotation to map an output process in fragment A to an input process in fragment B. This mapping results in fragment B having its input form ( or other `` input '' widget ) filled up . We consider deep annotation as particularly valid for portlet interoperation due to the controlled and cooperative environment that characterizes the portal setting ."
"837","Object-level ranking : bringing order to Web objects In contrast with the current Web search methods that essentially do document-level ranking and retrieval , we are exploring a new paradigm to enable Web search at the object level . We collect Web information for objects relevant for a specific application domain and rank these objects in terms of their relevance and popularity to answer user queries . Traditional PageRank model is no longer valid for object popularity calculation because of the existence of heterogeneous relationships between objects . This paper introduces PopRank , a domain-independent object-level link analysis model to rank the objects within a specific domain . Specifically we assign a popularity propagation factor to each type of object relationship , study how different popularity propagation factors for these heterogeneous relationships could affect the popularity ranking , and propose efficient approaches to automatically decide these factors . Our experiments are done using 1 million CS papers , and the experimental results show that PopRank can achieve significantly better ranking results than naively applying PageRank on the object graph ."
"838","An adaptive crawler for locating hidden-Web entry points In this paper we describe new adaptive crawling strategies to efficiently locate the entry points to hidden-Web sources . The fact that hidden-Web sources are very sparsely distributedmakes the problem of locating them especially challenging . We deal with this problem by using the contents ofpages to focus the crawl on a topic ; by prioritizing promisinglinks within the topic ; and by also following links that may not lead to immediate benefit . We propose a new frameworkwhereby crawlers automatically learn patterns of promisinglinks and adapt their focus as the crawl progresses , thus greatly reducing the amount of required manual setup andtuning . Our experiments over real Web pages in a representativeset of domains indicate that online learning leadsto significant gains in harvest rates ' the adaptive crawlers retrieve up to three times as many forms as crawlers thatuse a fixed focus strategy ."
"839","The powerrank web link analysis algorithm The web graph follows the power law distribution and has a hierarchy structure . But neither the PageRank algorithm nor any of its improvements leverage these attributes . In this paper , we propose a novel link analysis algorithm `` the PowerRank algorithm '' , which makes use of the power law distribution attribute and the hierarchy structure of the web graph . The algorithm consists two parts . In the first part , special treatment is applied to the web pages with low `` importance '' score . In the second part , the global `` importance '' score for each web page is obtained by combining those scores together . Our experimental results show that : 1 ) The PowerRank algorithm computes 10 % -30 % faster than PageRank algorithm . 2 ) Top web pages in PowerRank algorithm remain similar to that of the PageRank algorithm ."
"840","A web personalization system based on web usage mining techniques In the past few years , web usage mining techniques have grown rapidly together with the explosive growth of the web , both in the research and commercial areas . In this work we present a Web mining strategy for Web personalization based on a novel pattern recognition strategy which analyzes and classifies both static and dynamic features . The results of experiments on the data from a large commercial web site are presented to show the effectiveness of the proposed system ."
"841","Reliable QoS monitoring based on client feedback Service-level agreements ( SLAs ) establish a contract between service providersand clients concerning Quality of Service ( QoS ) parameters . Without properpenalties , service providers have strong incentives to deviate from theadvertised QoS , causing losses to the clients . Reliable QoS monitoring ( andproper penalties computed on the basis of delivered QoS ) are thereforeessential for the trustworthiness of a service-oriented environment . In thispaper , we present a novel QoS monitoring mechanism based on quality ratings from theclients . A reputation mechanism collects the ratings and computes theactual quality delivered to the clients . The mechanism provides incentives forthe clients to report honestly , and pays special attention to minimizing costand overhead1 ."
"842","Budget constrained bidding in keyword auctions and online knapsack problems We consider the budget-constrained bidding optimization problem for sponsored search auctions , and model it as an online ( multiple-choice ) knapsack problem . We design both deterministic and randomized algorithms for the online ( multiple-choice ) knapsack problems achieving a provably optimal competitive ratio . This translates back to fully automatic bidding strategies maximizing either profit or revenue for the budget-constrained advertiser . Our bidding strategy for revenue maximization is oblivious ( i.e. , without knowledge ) of other bidders ' prices and\/or click-through-rates for those positions . We evaluate our bidding algorithms using both synthetic data and real bidding data gathered manually , and also discuss a sniping heuristic that strictly improves bidding performance . With sniping and parameter tuning enabled , our bidding algorithms can achieve a performance ratio above 90 % against the optimum by the omniscient bidder ."
"843","Analyzing web access control policies XACML has emerged as a popular access control language on the Web , but because of its rich expressiveness , it has proved difficult to analyze in an automated fashion . In this paper , we present a formalization of XACML using description logics ( DL ) , which are a decidable fragment of First-Order logic . This formalization allows us to cover a more expressive subset of XACML than propositional logic-based analysis tools , and in addition we provide a new analysis service ( policy redundancy ) . Also , mapping XACML to description logics allows us to use off-the-shelf DL reasoners for analysis tasks such as policy comparison , verification and querying . We provide empirical evaluation of a policy analysis tool that was implemented on top of open source DL reasoner Pellet ."
"844","Towards the theoretical foundation of choreography With the growth of interest on the web services , people pay increasinglyattention to the choreography , that is , to describe collaborations ofparticipants in accomplishing a common business goal from a globalviewpoint . In this paper , based on a simple choreography language and arole-oriented process language , we study some fundamental issues relatedto choreography , especially those related to implementation , includingsemantics , projection and natural projection , dominant role in choices anditerations , etc. . We propose the concept of dominant role and somenovel languages structures related to it . The study reveals some cluesabout the language , the semantics , the specification and theimplementation of choreography ."
"845","Rapid prototyping of web applications combining domain specific languages and model driven design There have been several authoring methods proposed in the literature that are model based , essentially following the Model Driven Design philosophy . While useful , such methods need an effective way to allow the application designer to somehow synthesize the actual running application from the specification . In this paper , we describe HyperDE , an environment that combines Model Driven Design and Domain Specific Languages to enable rapid prototyping of Web applications ."
"846","Generating query substitutions We introduce the notion of query substitution , that is , generating a new query to replace a user 's original search query . Our technique uses modifications based on typical substitutions web searchers make to their queries . In this way the new query is strongly related to the original query , containing terms closely related to all of the original terms . This contrasts with query expansion through pseudo-relevance feedback , which is costly and can lead to query drift . This also contrasts with query relaxation through boolean or TFIDF retrieval , which reduces the specificity of the query . We define a scale for evaluating query substitution , and show that our method performs well at generating new queries related to the original queries . We build a model for selecting between candidates , by using a number of features relating the query-candidate pair , and by fitting the model to human judgments of relevance of query suggestions . This further improves the quality of the candidates generated . Experiments show that our techniques significantly increase coverage and effectiveness in the setting of sponsored search ."
"847","Towards domain-independent information extraction from web tables Traditionally , information extraction from web tables has focused on small , more or less homogeneous corpora , often based on assumptions about the use of"
"848","Providing ranked relevant results for web database queries Often Web database users experience difficulty in articulating their needs using a precise query . Providing ranked set of possible answers would benefit such users . We propose to provide ranked answers to user queries by identifying a set of queries from the query log whose answers are relevant to the given user query . The relevance detection is done using a domain and end-user independent content similarity estimation technique ."
"849","Ranking a stream of news According to a recent survey made by Nielsen NetRatings , searching on news articles is one of the most important activity online . Indeed , Google , Yahoo , MSN and many others have proposed commercial search engines for indexing news feeds . Despite this commercial interest , no academic research has focused on ranking a stream of news articles and a set of news sources . In this paper , we introduce this problem by proposing a ranking framework which models : ( 1 ) the process of generation of a stream of news articles , ( 2 ) the news articles clustering by topics , and ( 3 ) the evolution of news story over the time . The ranking algorithm proposed ranks news information , finding the most authoritative news sources and identifying the most interesting events in the different categories to which news article belongs . All these ranking measures take in account the time and can be obtained without a predefined sliding window of observation over the stream . The complexity of our algorithm is linear in the number of pieces of news still under consideration at the time of a new posting . This allow a continuous on-line process of ranking . Our ranking framework is validated on a collection of more than 300,000 pieces of news , produced in two months by more then 2000 news sources belonging to 13 different categories ( World , U. S , Europe , Sports , Business , etc ) . This collection is extracted from the index of comeToMyHead , an academic news search engine available online ."
"850","The anatomy of a news search engine Today , news browsing and searching is one of the most important Internet activity . This paper introduces a general framework to build a News search engine by describing Velthune , an academic News search engine available on line ."
"851","Conversation specification : a new approach to design and analysis of e-service composition This paper introduces a framework for modeling and specifying the global behavior of e-service compositions . Under this framework , peers ( individual e-services ) communicate through asynchronous messages and each peer maintains a queue for incoming messages . A global `` watcher '' keeps track of messages as they occur . We propose and study a central notion of a `` conversation '' , which is a sequence of ( classes of ) messages observed by the watcher . We consider the case where the peers are represented by Mealy machines ( finite state machines with input and output ) . The sets of conversations exhibit unexpected behaviors . For example , there exists a composite e-service based on Mealy peers whose set of conversations is not context free ( and not regular ) . ( The set of conversations is always context sensitive . ) One cause for this is the queuing of messages ; we introduce an operator `` prepone '' that simulates queue delays from a global perspective and show that the set of conversations of each Mealy e-service is closed under prepone . We illustrate that the global prepone fails to completely capture the queue delay effects and refine prepone to a `` local '' version on conversations seen by individual peers . On the other hand , Mealy implementations of a composite e-service will always generate conversations whose `` projections '' are consistent with individual e-services . We use projection-join to reflect such situations . However , there are still Mealy peers whose set of conversations is not the local prepone and projection-join closure of any regular language . Therefore , we propose conversation specifications as a formalism to define the conversations allowed by an e-service composition . We give two technical results concerning the interplay between the local behaviors of Mealy peers and the global behaviors of their compositions . One result shows that for each regular language , its local prepone and projection-join closure corresponds to the set of conversations by some Mealy peers effectively constructed from . The second result gives a condition on the shape of a composition which guarantees that the set of conversations that can be realized is the local prepone and projection-join closure of a regular language ."
"852","Updating pagerank with iterative aggregation We present an algorithm for updating the PageRank vector ( 1 ) . Due to the scale of the web , Google only updates its famous PageRank vector on a monthly basis . However , the Web changes much more frequently . Drastically speeding the PageRank computation can lead to fresher , more accurate rankings of the webpages retrieved by search engines . It can also make the goal of real-time personalized rankings within reach . On two small subsets of the web , our algorithm updates PageRank using just 25 % and 14 % , respectively , of the time required by the original PageRank algorithm . Our algorithm uses iterative aggregation techniques ( 7 , 8 ) to focus on the slow-converging states of the Markov chain . The most exciting feature of this algorithm is that it can be joined with other PageRank acceleration methods , such as the dangling node lumpability algorithm ( 6 ) , quadratic extrapolation ( 4 ) , and adaptive PageRank ( 3 ) , to realize even greater speedups ( potentially a factor of 60 or more speedup when all algorithms are combined ) . every few weeks . Our solution harnesses the power of iterative aggregation principles for Markov chains to allow for much more frequent updates to the valuable ranking vectors ."
"853","A large-scale study of robots . txt Search engines largely rely on Web robots to collect information from the Web . Due to the unregulated open-access nature of the Web , robot activities are extremely diverse . Such crawling activities can be regulated from the server side by deploying the Robots Exclusion Protocol in a file called robots . txt . Although it is not an enforcement standard , ethical robots ( and many commercial ) will follow the rules specified in robots . txt . With our focused crawler , we investigate 7,593 websites from education , government , news , and business domains . Five crawls have been conducted in succession to study the temporal changes . Through statistical analysis of the data , we present a survey of the usage of Web robots rules at the Web scale . The results also show that the usage of robots . txt has increased over time ."
"854","METEOR : metadata and instance extraction from object referral lists on the web The Web has established itself as the largest public data repository ever available . Even though the vast majority of information on the Web is formatted to be easily readable by the human eye , `` meaningful information '' is still largely inaccessible for the computer applications . In this paper we present the METEOR system which utilizes various presentation and linkage regularities from referral lists of various sorts to automatically separate and extract metadata and instance information . Experimental results for the university domain with 12 computer science department Web sites , comprising 361 individual faculty and course home pages indicate that the performance of the metadata and instance extraction averages 85 % , 88 % F-measure respectively . METEOR achieves this performance without any domain specific engineering requirement ."
"855","Automatic detection of fragments in dynamically generated web pages Dividing web pages into fragments has been shown to provide significant benefits for both content generation and caching . In order for a web site to use fragment-based content generation , however , good methods are needed for dividing web pages into fragments . Manual fragmentation of web pages is expensive , error prone , and unscalable . This paper proposes a novel scheme to automatically detect and flag fragments that are cost-effective cache units in web sites serving dynamic content . We consider the fragments to be interesting if they are shared among multiple documents or they have different lifetime or personalization characteristics . Our approach has three unique features . First , we propose a hierarchical and fragment-aware model of the dynamic web pages and a data structure that is compact and effective for fragment detection . Second , we present an efficient algorithm to detect maximal fragments that are shared among multiple documents . Third , we develop a practical algorithm that effectively detects fragments based on their lifetime and personalization characteristics . We evaluate the proposed scheme through a series of experiments , showing the benefits and costs of the algorithms . We also study the impact of adopting the fragments detected by our system on disk space utilization and network bandwidth consumption ."
"856","High-performance spatial indexing for location-based services Much attention has been accorded to Location-Based Services and location tracking , a necessary component in active , trigger-based LBS applications . Tracking the location of a large population of moving objects requires very high update and query performance of the underlying spatial index . In this paper we investigate the performance and scalability of three main-memory based spatial indexing methods under dynamic update and query loads : an R-tree , a ZB-tree , and an array\/hashtable method . By leveraging the LOCUS performance evaluation testbed and the City Simulator dynamic spatial data generator , we are able to demonstrate the scalability of these methods and determine the maximum population size supported by each method , a useful parameter for capacity planning by wireless carriers ."
"857","Semantic similarity between search engine queries using temporal correlation We investigate the idea of finding semantically related search engine queries based on their temporal correlation ; in other words , we infer that two queries are related if their popularities behave similarly over time . To this end , we first define a new measure of the temporal correlation of two queries based on the correlation coefficient of their frequency functions . We then conduct extensive experiments using our measure on two massive query streams from the MSN search engine , revealing that this technique can discover a wide range of semantically similar queries . Finally , we develop a method of efficiently finding the highest correlated queries for a given input query using far less space and time than the naive approach , making real-time implementation possible ."
"858","Modeling redirection in geographically diverse server sets Internet server selection mechanisms attempt to optimize , subject to a variety of constraints , the distribution of client requests to a geographically and topologically diverse pool of servers . Research on server selection has thus far focused primarily on techniques for choosing a server from a group administered by single entity , like a content distribution network provider . In a federated , multi-provider computing system , however , selection must occur over distributed server sets deployed by the participating providers , without the benefit of the full information available in the single-provider case . Intelligent server set selection algorithms will require a model of the expected performance clients would receive from a candidate server set . In this paper , we study whether the complex policies and dynamics of intelligent server selection can be effectively modeled in order to predict client performance for server sets . We introduce a novel server set distance metric , and use it in a measurement study of several million server selection transactions to develop simple models of existing server selection schemes . We then evaluate these models in terms of their ability to accurately predict performance for a second , larger set of distributed clients . We show that our models are able to predict performance within 20ms for over 90 % of the observed samples . Our analysis demonstrates that although existing deployments use a variety of complex and dynamic server selection criteria , most of which are proprietary , these schemes can be modeled with surprising accuracy ."
"859","Profiles for the situated web The World Wide Web is evolving into a medium that will soon make it possible for conceiving and implementing situation-aware services . A situation-aware or situated web application is one that renders the user with an experience ( content , interaction and presentation ) that is so tailored to his\/her current situation . This requires the facts and opinions regarding the context to be communicated to the server by means of a profile , which is then applied against the description of the application objects at the server in order to generate the required experience . This paper discusses a profiles view of the situated web architecture and analyzes the key technologies and capabilities that enable them . We conclude that trusted frameworks wherein rich vocabularies describing users and their context , applications and documents , along with rules for processing them , are critical elements of such architectures ."
"860","Design , implementation , and evaluation of a client characterization driven web server In earlier work we proposed a way for a Web server to detect connectivity information about clients accessing it in order to take tailored actions for a client request . This paper describes the design , implementation , and evaluation of such a working system . A Web site has a strong incentive to reduce the ` time-to-glass ' to retain users who may otherwise lose interest and leave the site . We have performed a measurement study from multiple client sites around the world with various levels of connectivity to the Internet communicating with modified Apache Web servers under our control . The results show that clients can be classified in a correct and stable manner and that user-perceived latency can be reduced via tailored actions . Our measurements show that classification and determination of server actions are done without significant overhead on the Web server . We explore a variety of modified actions ranging from selecting a lower quality version of the resource to altering the manner of content delivery . By studying numerous performance related factors in a single unified framework and examining both individual actions as well as combination of actions , our modified Web server implementation shows the efficacy of various server actions ."
"861","SCTP : an innovative transport layer protocol for the web We propose using the Stream Control Transmission Protocol ( SCTP ) , a recent IETF transport layer protocol , for reliable web transport . Although TCP has traditionally been used , we argue that SCTP better matches the needs of HTTP-based network applications . This position paper discusses SCTP features that address : ( i ) head-of-line blocking within a single TCP connection , ( ii ) vulnerability to network failures , and ( iii ) vulnerability to denial-of-service SYN attacks . We discuss our experience in modifying the Apache server and the Firefox browser to benefit from SCTP , and demonstrate our HTTP over SCTP design via simple experiments . We also discuss the benefits of using SCTP in other web domains through two example scenarios ? multiplexing user requests , and multiplexing resource access . Finally , we highlight several SCTP features that will be valuable to the design and implementation of current HTTP-based client-server applications ."
"862","Probabilistic models for discovering e-communities The increasing amount of communication between individuals in e-formats ( e.g. email , Instant messaging and the Web ) has motivated computational research in social network analysis ( SNA ) . Previous work in SNA has emphasized the social network ( SN ) topology measured by communication frequencies while ignoring the semantic information in SNs . In this paper , we propose two generative Bayesian models for semantic community discovery in SNs , combining probabilistic modeling with community detection in SNs . To simulate the generative models , an EnF-Gibbs sampling algorithm is proposed to address the efficiency and performance problems of traditional methods . Experimental studies on Enron email corpus show that our approach successfully detects the communities of individuals and in addition provides semantic topic descriptions of these communities ."
"863","Exploiting the web for point-in-time file sharing We describe a simple approach to `` point-in-time '' file sharing based on time expiring web links and personal webservers . This approach to file sharing is useful in environments where instant messaging clients are varied and do n't necessarily support ( compatible ) file transfer protocols . We discuss the features of such an approach along with a successfully deployed implementation now in wide use throughout the IBM corporation ."
"864","Speeding up adaptation of web service compositions using expiration times Web processes must often operate in volatile environments where the quality of service parameters of the participating service providers change during the life time of the process . In order to remain optimal , the Web process must adapt to these changes . Adaptation requires knowledge about the parameter changes of each of the service providers and using this knowledge to determine whether the Web process should make a different more optimal decision . Previously , we defined a mechanism called the value of changed information which measures the impact of expected changes in the service parameters on the Web process , thereby offering a way to query and incorporate those changes that are useful and cost-efficient . However , computing the value of changed information incurs a substantial computational overhead . In this paper , we use service expiration times obtained from pre-defined service level agreements to reduce the computational overhead of adaptation . We formalize the intuition that services whose parameters have not expired need not be considered for querying for revised information . Using two realistic scenarios , we illustrate our approach and demonstrate the associated computational savings ."
"865","Improvement of HITS-based algorithms on web documents In this paper , we present two ways to improve the precision of HITS-based algorithms on Web documents . First , by analyzing the limitations of current HITS-based algorithms , we propose a new weighted HITS-based method that assigns appropriate weights to in-links of root documents . Then , we combine content analysis with HITS-based algorithms and study the effects of four representative relevance scoring methods , VSM , Okapi , TLS , and CDR , using a set of broad topic queries . Our experimental results show that our weighted HITS-based method performs significantly better than Bharat 's improved HITS algorithm . When we combine our weighted HITS-based method or Bharat 's HITS algorithm with any of the four relevance scoring methods , the combined methods are only marginally better than our weighted HITS-based method . Between the four relevance-scoring methods , there is no significant quality difference when they are combined with a HITS-based algorithm ."
"866","Super-peer-based routing and clustering strategies for RDF-based peer-to-peer networks RDF-based P2P networks have a number of advantages compared with simpler P2P networks such as Napster , Gnutella or with approaches based on distributed indices such as CAN and CHORD . RDF-based P2P networks allow complex and extendable descriptions of resources instead of fixed and limited ones , and they provide complex query facilities against these metadata instead of simple keyword-based searches . In previous papers , we have described the Edutella infrastructure and different kinds of Edutella peers implementing such an RDF-based P2P network . In this paper we will discuss these RDF-based P2P networks as a specific example of a new type of P2P networks , schema-based P2P networks , and describe the use of super-peer based topologies for these networks . Super-peer based networks can provide better scalability than broadcast based networks , and do provide perfect support for inhomogeneous schema-based networks , which support different metadata schemas and ontologies ( crucial for the Semantic Web ) . Furthermore , as we will show in this paper , they are able to support sophisticated routing and clustering strategies based on the metadata schemas , attributes and ontologies used . Especially helpful in this context is the RDF functionality to uniquely identify schemas , attributes and ontologies . The resulting routing indices can be built using dynamic frequency counting algorithms and support local mediation and transformation rules , and we will sketch some first ideas for implementing these advanced functionalities as well ."
"867","The chatty web : emergent semantics through gossiping This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up , semi-automatic manner without relying on pre-existing , global semantic models . We assume that large amounts of data exist that have been organized and annotated according to local schemas . Seeing semantics as a form of agreement , our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise , local interactions : Participants provide translations between schemas they are interested in and can learn about other translations by routing queries ( gossiping ) . To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria . The assessment process is incremental and the quality ratings are adjusted along with the operation of the system . Ultimately , this process results in global agreement , i.e. , the semantics that all participants understand . We discuss strategies to efficiently find translations and provide results from a case study to justify our claims . Our approach applies to any system which provides a communication infrastructure ( existing websites or databases , decentralized systems , P2P systems ) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties ."
"868","Performance of compressed inverted list caching in search engines Due to the rapid growth in the size of the web , web search engines are facing enormous performance challenges . The larger engines in particular have to be able to process tens of thousands of queries per second on tens of billions of documents , making query throughput a critical issue . To satisfy this heavy workload , search engines use a variety of performance optimizations including index compression , caching , and early termination . We focus on two techniques , inverted index compression and index caching , which play a crucial rule in web search engines as well as other high-performance information retrieval systems . We perform a comparison and evaluation of several inverted list compression algorithms , including new variants of existing algorithms that have not been studied before . We then evaluate different inverted list caching policies on large query traces , and finally study the possible performance benefits of combining compression and caching . The overall goal of this paper is to provide an updated discussion and evaluation of these two techniques , and to show how to select the best set of approaches and settings depending on parameter such as disk speed and main memory cache size ."
"869","Privacy-enhancing personalized web search Personalized web search is a promising way to improve search quality by customizing search results for people with individual information goals . However , users are uncomfortable with exposing private preference information to search engines . On the other hand , privacy is not absolute , and often can be compromised if there is a gain in service or profitability to the user . Thus , a balance must be struck between search quality and privacy protection . This paper presents a scalable way for users to automatically build rich user profiles . These profiles summarize a user . s interests into a hierarchical organization according to specific interests . Two parameters for specifying privacy requirements are proposed to help the user to choose the content and degree of detail of the profile information that is exposed to the search engine . Experiments showed that the user profile improved search quality when compared to standard MSN rankings . More importantly , results verified our hypothesis that a significant improvement on search quality can be achieved by only sharing some higher-level user profile information , which is potentially less sensitive than detailed personal information ."
"870","Towards a flash search engine based on expressive semantics Flash , as a multimedia format , becomes more and more popular on the Web . However , previous works on Flash are totally based on low-level features , which make it unpractical to build a content-based Flash search engine . To address this problem , our paper proposes expressive semantics for bridging the gap between low-level features and user queries . To smoothly incorporate expressive semantics into a search engine , an eigenvector-based model is devised to map a user query to expressive semantics with the aid of link analysis method . Our experiment results confirm that expressive semantics is a promising approach to understanding and hence searching Flash movies more efficiently ."
"871","Trust-based recommendation systems : an axiomatic approach High-quality , personalized recommendations are a key feature in many online systems . Since these systems often have explicit knowledge of social network structures , the recommendations may incorporate this information . This paper focuses on networks that represent trust and recommendation systems that incorporate these trust relationships . The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network . In analogy to prior work on voting and ranking systems , we use the axiomatic approach from the theory of social choice . We develop a set of five natural axioms that a trust-based recommendation system might be expected to satisfy . Then , we show that no system can simultaneously satisfy all the axioms . However , for any subset of four of the five axioms we exhibit a recommendation system that satisfies those axioms . Next we consider various ways of weakening the axioms , one of which leads to a unique recommendation system based on random walks . We consider other recommendation systems , including systems based on personalized PageRank , majority of majorities , and minimum cuts , and search for alternative axiomatizations that uniquely characterize these systems . Finally , we determine which of these systems are incentive compatible , meaning that groups of agents interested in manipulating recommendations can not induce others to share their opinion by lying about their votes or modifying their trust links . This is an important property for systems deployed in a monetized environment ."
"872","Using visual cues for extraction of tabular data from arbitrary HTML documents We describe a method to extract tabular data from web pages . Rather than just analyzing the DOM tree , we also exploit visual cues in the rendered version of the document to extract data from tables which are not explicitly marked with an HTML table element . To detect tables , we rely on a variant of the well-known X-Y cut algorithm as used in the OCR community . We implemented the system by directly accessing Mozilla 's box model that contains the positional data for all HTML elements of a given web page ."
"873","The XML web : a first study Although originally designed for large-scale electronic publishing , XML plays an increasingly important role in the exchange of data on the Web . In fact , it is expected that XML will become the lingua franca of the Web , eventually replacing HTML . Not surprisingly , there has been a great deal of interest on XML both in industry and in academia . Nevertheless , to date no comprehensive study on the XML Web ( i.e. , the subset of the Web made of XML documents only ) nor on its contents has been made . This paper is the first attempt at describing the XML Web and the documents contained in it . Our results are drawn from a sample of a repository of the publicly available XML documents on the Web , consisting of about 200,000 documents . Our results show that , despite its short history , XML already permeates the Web , both in terms of generic domains and geographically . Also , our results about the contents of the XML Web provide valuable input for the design of algorithms , tools and systems that use XML in one form or another ."
"874","The web structure of e-government - developing a methodology for quantitative evaluation In this paper we describe preliminary work that examines whether statistical properties of the structure of websites can be an informative measure of their quality . We aim to develop a new method for evaluating e-government . E-government websites are evaluated regularly by consulting companies , international organizations and academic researchers using a variety of subjective measures . We aim to improve on these evaluations using a range of techniques from webmetric and social network analysis . To pilot our methodology , we examine the structure of government audit office sites in Canada , the USA , the UK , New Zealand and the Czech Republic . We report experimental values for a variety of characteristics , including the connected components , the average distance between nodes , the distribution of paths lengths , and the indegree and outdegree . These measures are expected to correlate with ( i ) the navigability of a website and ( ii ) with its `` nodalityö which is a combination of hubness and authority . Comparison of websites based on these characteristics raised a number of issues , related to the proportion of non-hyperlinked content ( e.g. pdf and doc files ) within a site , and both the very significant differences in the size of the websites and their respective national populations . Methods to account for these issues are proposed and discussed . There appears to be some correlation between the values measured and the league tables reported in the literature . However , this multi dimensional analysis provides a richer source of evaluative techniques than previous work . Our analysis indicates that the US and Canada provide better navigability , much better than the UK ; however , the UK site is shown to have the strongest `` nodalityö on the Web ."
"875","CC-Buddy : an adaptive framework for maintaining cache coherency using peers In this paper , we propose a framework called CC-Buddy , for maintaining dynamic data coherency in peer-to-peer environment . Working on the basis of peer heterogeneity in data coherency requirement , peers in CC-Buddy cooperate with each other to disseminate the updates by pushing . Simulation results show that our solution not only improves the fidelity in data , but also reduces the workload of servers , therefore achieves high-scalability ."
"876","An investigation of cloning in web applications Cloning ( ad hoc reuse by duplication of design or code ) speeds up development , but also hinders future maintenance . Cloning also hints at reuse opportunities that , if exploited systematically , might have positive impact on development and maintenance productivity . Unstable requirements and tight schedules pose unique challenges for Web Application engineering that encourage cloning . We are conducting a systematic study of cloning in Web Applications of different sizes , developed using a range of Web technologies , and serving diverse purposes . Our initial results show cloning rates up to 63 % in both newly developed and already maintained Web Applications . Expected contribution of this work is two-fold : ( 1 ) to confirm potential benefits of reuse-based methods in addressing clone related problems of Web engineering , and ( 2 ) to create a framework of metrics and presentation views to be used in other similar studies ."
"877","Shared lexicon for distributed annotations on the Web The interoperability among distributed and autonomous systems is the ultimate challenge facing the semantic web . Heterogeneity of data representation is the main source of problems . This paper proposes an innovative solution that combines lexical approaches and language games . The benefits for distributed annotation systems on the web are twofold : firstly , it will reduce the complexity of the semantic problem by moving the focus from the full-featured ontology level to the simpler lexicon level ; secondly , it will avoid the drawback of a centralized third party mediator that may become a single point of failure . The main contributions of this work are concerned with : providing a proof of concept that language games can be an effective solution to creating and managing a distributed process of agreement on a shared lexicon , describing a fully distributed service oriented architecture for language games , providing empirical evidence on a real world case study in the domain of ski mountaineering ."
"878","Improved annotation of the blogosphere via autotagging and hierarchical clustering Tags have recently become popular as a means of annotating and organizing Web pages and blog entries . Advocates of tagging argue that the use of tags produces a ` folksonomy ' , a system in which the meaning of a tag is determined by its use among the community as a whole . We analyze the effectiveness of tags for classifying blog entries by gathering the top 350 tags from Technorati and measuring the similarity of all articles that share a tag . We find that tags are useful for grouping articles into broad categories , but less effective in indicating the particular content of an article . We then show that automatically extracting words deemed to be highly relevant can produce a more focused categorization of articles . We also show that clustering algorithms can be used to reconstruct a topical hierarchy among tags , and suggest that these approaches may be used to address some of the weaknesses in current tagging systems ."
"879","From user-centric web traffic data to usage data In this paper , we describe a user-centric Internet usage data processing platform . Raw usage data is collected using a software probe installed on a panel of Internet users ' workstations . It is then processed by our platform . The transformation of raw usage data into qualified and usable information by Internet usage sociology researchers means setting up a series of relatively complex processes using quite a wide variety of resources . We use a combination of ad hoc rule-based systems and external resources to qualify the visited Web pages . We also implemented topological and temporal indicators in order to describe the dynamics of Web sessions ."
"880","Wake-on-WLAN In bridging the digital divide , two important criteria are cost-effectiveness , and power optimization . While 802.11 is cost-effective and is being used in several installations in the developing world , typical system configurations are not really power efficient . In this paper , we propose a novel `` Wake-on-WLAN '' mechanism for coarse-grained , on-demand power on\/off of the networking equipment at a remote site . The novelty also lies in our implementation of a prototype system using low-power 802.15.4-based sensor motes . We describe the prototype , as well as its evaluation on field in a WiFi testbed . Preliminary estimates indicate that the proposed mechanism can save significant power in typical rural networking settings ."
"881","Mining models of human activities from the web The ability to determine what day-to-day activity ( such as cooking pasta , taking a pill , or watching a video ) a person is performing is of interest in many application domains . A system that can do this requires models of the activities of interest , but model construction does not scale well : humans must specify low-level details , such as segmentation and feature selection of sensor data , and high-level structure , such as spatio-temporal relations between states of the model , for each and every activity . As a result , previous practical activity recognition systems have been content to model a tiny fraction of the thousands of human activities that are potentially useful to detect . In this paper , we present an approach to sensing and modeling activities that scales to a much larger class of activities than before . We show how a new class of sensors , based on Radio Frequency Identification ( RFID ) tags , can directly yield semantic terms that describe the state of the physical world . These sensors allow us to formulate activity models by translating labeled activities , such as ` cooking pasta ' , into probabilistic collections of object terms , such as ` pot ' . Given this view of activity models as text translations , we show how to mine definitions of activities in an unsupervised manner from the web . We have used our technique to mine definitions for over 20,000 activities . We experimentally validate our approach using data gathered from actual human activity as well as simulated data ."
"882","XML design for relational storage Design principles for XML schemas that eliminate redundancies and avoid update anomalies have been studied recently . Several normal forms , generalizing those for relational databases , have been proposed . All of them , however , are based on the assumption of anative XML storage , while in practice most of XML data is stored inrelational databases . In this paper we study XML design and normalization for relational storage of XML documents . To be able to relate and compare XML and relational designs , we use an information-theoretic framework that measures information content in relations and documents , with higher values corresponding to lower levels of redundancy . We show that most common relational storage schemes preserve the notion of being well-designed ( i.e. , anomalies - and redundancy-free ) . Thus , existing XML normal forms guarantee well-designed relational storagesas well . We further show that if this perfect option is not achievable , then a slight restriction on XML constraints guarantees a `` second-best '' relational design , according to possible values of the information-theoretic measure . We finally consider an edge-based relational representation of XML documents , and show that while it has similar information-theoretic properties with other relational representations , it can behave significantly worse in terms of enforcing integrity constraints ."
"883","An infrastructure for searching , reusing and evolving distributed ontologies The vision of the Semantic Web can only be realized through proliferation of well-known ontologies describing different domains . To enable interoperability in the Semantic Web , it will be necessary to break these ontologies down into smaller , well-focused units that may be reused . Currently , three problems arise in that scenario . Firstly , it is difficult to locate ontologies to be reused , thus leading to many ontologies modeling the same thing . Secondly , current tools do not provide means for reusing existing ontologies while building new ontologies . Finally , ontologies are rarely static , but are being adapted to changing requirements . Hence , an infrastructure for management of ontology changes , taking into account dependencies between ontologies is needed . In this paper we present such an infrastructure addressing the aforementioned problems ."
"884","CTR-S : a logic for specifying contracts in semantic web services A requirements analysis in the emerging field of Semantic Web Services ( SWS ) ( see http:\/\/daml.org\/services\/swsl\/requirements\/ ) has identified four major areas of research : intelligent service discovery , automated contracting of services , process modeling , and service enactment . This paper deals with the intersection of two of these areas : process modeling as it pertains to automated contracting . Specifically , we propose a logic , called CTR-S , which captures the dynamic aspects of contracting for services . Since CTR-S is an extension of the classical first-order logic , it is well-suited to model the static aspects of contracting as well . A distinctive feature of contracting is that it involves two or more parties in a potentially adversarial situation . CTR-S is designed to model this adversarial situation through its novel model theory , which incorporates certain game-theoretic concepts . In addition to the model theory , we develop a proof theory for CTR-S and demonstrate the use of the logic formodeling and reasoning about Web service contracts ."
"885","The distribution of pageRank follows a power-law only for particular values of the damping factor We show that the empirical distribution of the PageRank values in a large set of Web pages does not follow a power-law except for some particular choices of the damping factor . We argue that for a graph with an in-degree distribution following a power-law with exponent between 2.1 and 2.2 , choosing a damping factor around 0.85 for PageRank yields a power-law distribution of its values . We suggest that power-law distributions of PageRank in Web graphs have been observed because the typical damping factor used in practice is between 0.85 and 0.90 ."
"886","Trust-serv : model-driven lifecycle management of trust negotiation policies for web services A scalable approach to trust negotiation is required in Web service environments that have large and dynamic requester populations . We introduce Trust-Serv , a model-driven trust negotiation framework for Web services . The framework employs a model for trust negotiation that is based on state machines , extended with security abstractions . Our policy model supports lifecycle management , an important trait in the dynamic environments that characterize Web services . In particular , we provide a set of change operations to modify policies , and migration strategies that permit ongoing negotiations to be migrated to new policies without being disrupted . Experimental results show the performance benefit of these strategies . The proposed approach has been implemented as a container-centric mechanism that is transparent to the Web services and to the developers of Web services , simplifying Web service development and management as well as enabling scalable deployments ."
"887","Beyond PageRank : machine learning for static ranking Since the publication of Brin and Page 's paper on PageRank , many in the Web community have depended on PageRank for the static ( query-independent ) ordering of Web pages . We show that we can significantly outperform PageRank using features that are independent of the link structure of the Web . We gain a further boost in accuracy by using data on the frequency at which users visit Web pages . We use RankNet , a ranking machine learning algorithm , to combine these and other static features based on anchor text and domain characteristics . The resulting model achieves a static ranking pairwise accuracy of 67.3 % ( vs. 56.7 % for PageRank or 50 % for random ) ."
"888","Debugging OWL ontologies As an increasingly large number of OWL ontologies become available on the Semantic Web and the descriptions in the ontologies become more complicated , finding the cause of errors becomes an extremely hard task even for experts . Existing ontology development environments provide some limited support , in conjunction with a reasoner , for detecting and diagnosing errors in OWL ontologies . Typically these are restricted to the mere detection of , for example , unsatisfiable concepts . We have integrated a number of simple debugging cues generated from our description logic reasoner , Pellet , in our hypertextual ontology development environment , Swoop . These cues , in conjunction with extensive undo\/redo and Annotea based collaboration support in Swoop , significantly improve the OWL debugging experience , and point the way to more general improvements in the presentation of an ontology to new users ."
"889","Crawling a country : better strategies than breadth-first for web page ordering This article compares several page ordering strategies for Web crawling under several metrics . The objective of these strategies is to download the most `` important '' pages `` early '' during the crawl . As the coverage of modern search engines is small compared to the size of the Web , and it is impossible to index all of the Web for both theoretical and practical reasons , it is relevant to index at least the most important pages . We use data from actual Web pages to build Web graphs and execute a crawler simulator on those graphs . As the Web is very dynamic , crawling simulation is the only way to ensure that all the strategies considered are compared under the same conditions . We propose several page ordering strategies that are more efficient than breadth - first search and strategies based on partial Pagerank calculations ."
"890","Meaning and the semantic web The meaning of names ( URI references ) is a contentious issue in the Semantic Web . Numerous proposals have been given for how to provide meaning for names in the Semantic Web , ranging from a strict localized model-theoretic semantics to proposals for a unified single meaning . We argue that a slight expansion of the standard model-theoretic semantics for names is sufficient for the present , and can easily be augmented where necessary to allow communities of interest to strengthen this spartan theory of meaning ."
"891","Mining anchor text for query refinement When searching large hypertext document collections , it is often possible that there are too many results available for ambiguous queries . Query refinement is an interactive process of query modification that can be used to narrow down the scope of search results . We propose a new method for automatically generating refinements or related terms to queries by mining anchor text for a large hypertext document collection . We show that the usage of anchor text as a basis for query refinement produces high quality refinement suggestions that are significantly better in terms of perceived usefulness compared to refinements that are derived using the document content . Furthermore , our study suggests that anchor text refinements can also be used to augment traditional query refinement algorithms based on query logs , since they typically differ in coverage and produce different refinements . Our results are based on experiments on an anchor text collection of a large corporate intranet ."
"892","Defection detection : predicting search engine switching Searchers have a choice about which Web search engine they use when looking for information online . If they are unsuccessful on one engine , users may switch to a different engine to continue their search . By predicting when switches are likely to occur , the search experience can be modified to retain searchers or ensure a quality experience for incoming searchers . In this poster , we present research on a technique for predicting search engine switches . Our findings show that prediction is possible at a reasonable level of accuracy , particularly when personalization or user grouping is employed . These findings have implications for the design of applications to support more effective online searching ."
"893","Spatial variation in search engine queries Local aspects of Web search - associating Web content and queries with geography - is a topic of growing interest . However , the underlying question of how spatial variation is manifested in search queries is still not well understood . Here we develop a probabilistic framework for quantifying such spatial variation ; on complete Yahoo ! query logs , we find that our model is able to localize large classes of queries to within a few miles of their natural centers based only on the distribution of activity for the query . Our model provides not only an estimate of a query 's geographic center , but also a measure of its spatial dispersion , indicating whether it has highly local interest or broader regional or national appeal . We also show how variations on our model can track geographically shifting topics over time , annotate a map with each location 's `` distinctive queries '' , and delineate the `` spheres of influence '' for competing queries in the same general domain ."
"894","XQuery at your web service XML messaging is at the heart of Web services , providing the flexibility required for their deployment , composition , and maintenance . Yet , current approaches to Web services development hide the messaging layer behind Java or C #_# APIs , preventing the application to get direct access to the underlying XML information . To address this problem , we advocate the use of a native XML language , namely XQuery , as an integral part of the Web services development infrastructure . The main contribution of the paper is a binding between WSDL , the Web Services Description Language , and XQuery . The approach enables the use of XQuery for both Web services deployment and composition . We present a simple command-line tool that can be used to automatically deploy a Web service from a given XQuery module , and extend the XQuery language itself with a statement for accessing one or more Web services . The binding provides tight-coupling between WSDL and XQuery , yielding additional benefits , notably : the ability to use WSDL as an interface language for XQuery , and the ability to perform static typing on XQuery programs that include Web service calls . Last but not least , the proposal requires only minimal changes to the existing infrastructure . We report on our experience implementing this approach in the Galax XQuery processor ."
"895","A graph-theoretic approach to webpage segmentation We consider the problem of segmenting a webpage into visually and semantically cohesive pieces . Our approach is based on formulating an appropriate optimization problem on weighted graphs , where the weights capture if two nodes in the DOM tree should be placed together or apart in the segmentation ; we present a learning framework to learn these weights from manually labeled data in a principled manner . Our work is a significant departure from previous heuristic and rule-based solutions to the segmentation problem . The results of our empirical analysis bring out interesting aspects of our framework , including variants of the optimization problem and the role of learning ."
"896","Random surfer with back step We present a novel link-based ranking algorithm RBS , which may be viewed as an extension of PageRank by back-step feature ."
"897","Extraction and classification of dense communities in the web The World Wide Web ( WWW ) is rapidly becoming important for society as a medium for sharing data , information and services , and there is a growing interest in tools for understanding collective behaviors and emerging phenomena in the WWW . In this paper we focus on the problem of searching and classifying communities in the web . Loosely speaking a community is a group of pages related to a common interest . More formally communities have been associated in the computer science literature with the existence of a locally dense sub-graph of the web-graph ( where web pages are nodes and hyper-links are arcs of the web-graph ) . The core of our contribution is a new scalable algorithm for finding relatively dense subgraphs in massive graphs . We apply our algorithm on web-graphs built on three publicly available large crawls of the web ( with raw sizes up to 120M nodes and 1G arcs ) . The effectiveness of our algorithm in finding dense subgraphs is demonstrated experimentally by embedding artificial communities in the web-graph and counting how many of these are blindly found . Effectiveness increases with the size and density of the communities : it is close to 100 % for communities of a thirty nodes or more ( even at low density ) . It is still about 80 % even for communities of twenty nodes with density over 50 % of the arcs present . At the lower extremes the algorithm catches 35 % of dense communities made of ten nodes . We complete our Community Watch system by clustering the communities found in the web-graph into homogeneous groups by topic and labeling each group by representative keywords ."
"898","Efficient URL caching for world wide web crawling Crawling the web is deceptively simple : the basic algorithm is ( a ) Fetch a page ( b ) Parse it to extract all linked URLs ( c ) For all the URLs not seen before , repeat ( a ) - ( c ) . However , the size of the web ( estimated at over 4 billion pages ) and its rate of change ( estimated at 7 % per week ) move this plan from a trivial programming exercise to a serious algorithmic and system design challenge . Indeed , these two factors alone imply that for a reasonably fresh and complete crawl of the web , step ( a ) must be executed about a thousand times per second , and thus the membership test ( c ) must be done well over ten thousand times per second against a set too large to store in main memory . This requires a distributed architecture , which further complicates the membership test . A crucial way to speed up the test is to cache , that is , to store in main memory a ( dynamic ) subset of the `` seen '' URLs . The main goal of this paper is to carefully investigate several URL caching techniques for web crawling . We consider both practical algorithms : random replacement , static cache , LRU , and CLOCK , and theoretical limits : clairvoyant caching and infinite cache . We performed about 1,800 simulations using these algorithms with various cache sizes , using actual log data extracted from a massive 33 day web crawl that issued over one billion HTTP requests . Our main conclusion is that caching is very effective - in our setup , a cache of roughly 50,000 entries can achieve a hit rate of almost 80 % . Interestingly , this cache size falls at a critical point : a substantially smaller cache is much less effective while a substantially larger cache brings little additional benefit . We conjecture that such critical points are inherent to our problem and venture an explanation for this phenomenon ."
"899","Clustering e-commerce search engines In this paper , we sketch a method for clustering e-commerce search engines by the type of products\/services they sell . This method utilizes the special features of interface pages of such search engines . We also provide an analysis of different types of ESE interface pages ."
"900","Fractal summarization for mobile devices to access large documents on the web Wireless access with mobile ( or handheld ) devices is a promising addition to the WWW and traditional electronic business . Mobile devices provide convenience and portable access to the huge information space on the Internet without requiring users to be stationary with network connection . However , the limited screen size , narrow network bandwidth , small memory capacity and low computing power are the shortcomings of handheld devices . Loading and visualizing large documents on handheld devices become impossible . The limited resolution restricts the amount of information to be displayed . The download time is intolerably long . In this paper , we introduce the fractal summarization model for document summarization on handheld devices . Fractal summarization is developed based on the fractal theory . It generates a brief skeleton of summary at the first stage , and the details of the summary on different levels of the document are generated on demands of users . Such interactive summarization reduces the computation load in comparing with the generation of the entire summary in one batch by the traditional automatic summarization , which is ideal for wireless access . Three-tier architecture with the middle-tier conducting the major computation is also discussed . Visualization of summary on handheld devices is also investigated ."
"901","A combinatorial allocation mechanism with penalties for banner advertising Most current banner advertising is sold through negotiation thereby incurring large transaction costs and possibly suboptimal allocations . We propose a new automated system for selling banner advertising . In this system , each advertiser specifies a collection of host webpages which are relevant to his product , a desired total quantity of impressions on these pages , and a maximum per-impression price . The system selects a subset of advertisers as ` winners ' and maps each winner to a set of impressions on pages within his desired collection . The distinguishing feature of our system as opposed to current combinatorial allocation mechanisms is that , mimicking the current negotiation system , we guarantee that winners receive at least as many advertising opportunities as they requested or else receive ample compensation in the form of a monetary payment by the host . Such guarantees are essential in markets like banner advertising where a major goal of the advertising campaign is developing brand recognition . As we show , the problem of selecting a feasible subset of advertisers with maximum total value is inapproximable . We thus present two greedy heuristics and discuss theoretical techniques to measure their performances . Our first algorithm iteratively selects advertisers and corresponding sets of impressions which contribute maximum marginal per-impression profit to the current solution . We prove a bi-criteria approximation for this algorithm , showing that it generates approximately as much value as the optimum algorithm on a slightly harder problem . However , this algorithm might perform poorly on instances in which the value of the optimum solution is quite large , a clearly undesirable failure mode . Hence , we present an adaptive greedy algorithm which again iteratively selects advertisers with maximum marginal per-impression profit , but additionally reassigns impressions at each iteration . For this algorithm , we prove a structural approximation result , a newly defined framework for evaluating heuristics ( 10 ) . We thereby prove that this algorithm has a better performance guarantee than the simple greedy algorithm ."
"902","Semantic information portals In this paper , we describe the notion of a semantic information portal . This is a community information portal that exploits the semantic web standards to improve structure , extensibility , customization and sustainability . We are in the process of developing a prototype directory of environmental organizations as a demonstration of the approach and outline the design challenges involved and the current status of the work ."
"903","Upgrading relational legacy data to the semantic web In this poster , we describe a framework composed of the R2O mapping language and the ODEMapster processor to upgrade relational legacy data to the Semantic Web . The framework is based on the declarative description of mappings between relational and ontology elements and the exploitation of such mapping descriptions by a generic processor capable of performing both massive and query driven data upgrade ."
"904","Compiling XSLT 2.0 into XQuery 1.0 As XQuery is gathering momentum as the standard query language for XML , there is a growing interest in using it as an integral part of the XML application development infrastructure . In that context , one question which is often raised is how well XQuery interoperates with other XML languages , and notably with XSLT . XQuery 1.0 ( 16 ) and XSLT 2.0 ( 7 ) share a lot in common : they share XPath 2.0 as a common sub-language and have the same expressiveness . However , they are based on fairly different programming paradigms . While XSLT has adopted a highly declarative template based approach , XQuery relies on a simpler , and more operational , functional approach . In this paper , we present an approach to compile XSLT 2.0 into XQuery 1.0 , and a working implementation of that approach . The compilation rules explain how XSLT 's template-based approach can be implemented using the functional approach of XQuery and underpins the tight connection between the two languages . The resulting compiler can be used to migrate a XSLT code base to XQuery , or to enable the use of XQuery runtimes ( e.g. , as will soon be provided by most relational database management systems ) for XSLT users . We also identify a number of areas where compatibility between the two languages could be improved . Finally , we show experiments on actual XSLT stylesheets , demonstrating the applicability of the approach in practice ."
"905","OCTOPUS : aggressive search of multi-modality data using multifaceted knowledge base An important trend in Web information processing is the support of multimedia retrieval . However , the most prevailing paradigm for multimedia retrieval , content-based retrieval ( CBR ) , is a rather conservative one whose performance depends on a set of specifically defined low-level features and a carefully chosen sample object . In this paper , an aggressive search mechanism called Octopus is proposed which addresses the retrieval of multi-modality data using multifaceted knowledge . In particular , Octopus promotes a novel scenario in which the user supplies seed objects of arbitrary modality as the hint of his information need , and receives a set of multi-modality objects satisfying his need . The foundation of Octopus is a multifaceted knowledge base constructed on a layered graph model ( LGM ) , which describes the relevance between media objects from various perspectives . Link analysis based retrieval algorithm is proposed based on the LGM . A unique relevance feedback technique is developed to update the knowledge base by learning from user behaviors , and to enhance the retrieval performance in a progressive manner . A prototype implementing the proposed approach has been developed to demonstrate its feasibility and capability through illustrative examples ."
"906","Tag-based social interest discovery The success and popularity of social network systems , such as del. icio . us , Facebook , MySpace , and YouTube , have generated many interesting and challenging problems to the research community . Among others , discovering social interests shared by groups of users is very important because it helps to connect people with common interests and encourages people to contribute and share more contents . The main challenge to solving this problem comes from the difficulty of detecting and representing the interest of the users . The existing approaches are all based on the online connections of users and so unable to identify the common interest of users who have no online connections . In this paper , we propose a novel social interest discovery approach based on user-generated tags . Our approach is motivated by the key observation that in a social network , human users tend to use descriptive tags to annotate the contents that they are interested in . Our analysis on a large amount of real-world traces reveals that in general , user-generated tags are consistent with the web content they are attached to , while more concise and closer to the understanding and judgments of human users about the content . Thus , patterns of frequent co-occurrences of user tags can be used to characterize and capture topics of user interests . We have developed an Internet Social Interest Discovery system , ISID , to discover the common user interests and cluster users and their saved URLs by different interest topics . Our evaluation shows that ISID can effectively cluster similar documents by interest topics and discover user communities with common interests no matter if they have any online connections ."
"907","Topic modeling with network regularization In this paper , we formally define the problem of topic modeling with network structure ( TMN ) . We propose a novel solution to this problem , which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data . The proposed method bridges topic modeling and social network analysis , which leverages the power of both statistical topic models and discrete regularization . The output of this model well summarizes topics in text , maps a topic on the network , and discovers topical communities . With concrete selection of a topic model and a graph-based regularizer , our model can be applied to text mining problems such as author-topic analysis , community discovery , and spatial text mining . Empirical experiments on two different genres of data show that our approach is effective , which improves text-oriented methods as well as network-oriented methods . The proposed model is general ; it can be applied to any text collections with a mixture of topics and an associated network structure ."
"908","Fully automatic wrapper generation for search engines When a query is submitted to a search engine , the search engine returns a dynamically generated result page containing the result records , each of which usually consists of a link to and\/or snippet of a retrieved Web page . In addition , such a result page often also contains information irrelevant to the query , such as information related to the hosting site of the search engine and advertisements . In this paper , we present a technique for automatically producing wrappers that can be used to extract search result records from dynamically generated result pages returned by search engines . Automatic search result record extraction is very important for many applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling . The novel aspect of the proposed technique is that it utilizes both the visual content features on the result page as displayed on a browser and the HTML tag structures of the HTML source file of the result page . Experimental results indicate that this technique can achieve very high extraction accuracy ."
"909","Authoring of learning styles in adaptive hypermedia : problems and solutions Learning styles , as well as the best ways of responding with corresponding instructional strategies , have been intensively studied in the classical educational ( classroom ) setting . There is much less research of application of learning styles in the new educational space , created by the Web . Moreover , authoring applications are scarce , and they do not provide explicit choices and creation of instructional strategies for specific learning styles . The main objective of the research described in this paper is to provide the authors with a tool which will allow them to incorporate different learning styles in their adaptive educational hypermedia applications . In this way , we are creating a semantically significant interface between classical learning styles and instructional strategies and the modern field of adaptive educational hypermedia ."
"910","On admission control for profit maximization of networked service providers Variability and diverseness among incoming requests to a service hosted on a finite capacity resource necessitates sophisticated request admission control techniques for providing guaranteed quality of service ( QoS ) . We propose in this paper a service time based online admission control methodology for maximizing profits of a service provider . The proposed methodology chooses a subset of incoming requests such that the revenue of the provider is maximized . Admission control decision in our proposed system is based upon an estimate of the service time of the request , QoS bounds , prediction of arrivals and service times of requests to come in the short-term future , and rewards associated with servicing a request within its QoS bounds . Effectiveness of the proposed admission control methodology is demonstrated using experiments with a content-based messaging middleware service ."
"911","Analysis of communication models in web service compositions In this paper we describe an approach for the verification of Web service compositions defined by sets of BPEL processes . The key aspect of such a verification is the model adopted for representing the communications among the services participating in the composition . Indeed , these communications are asynchronous and buffered in the existing execution frameworks , while most verification approaches assume a synchronous communication model for efficiency reasons . In our approach , we develop a parametric model for describing Web service compositions , which allows us to capture a hierarchy of communication models , ranging from synchronous communications to asynchronous communications with complex buffer structures . Moreover , we develop a technique to associate with a Web service composition the most adequate communication model , i.e. , the simplest model that is sufficient to capture all the behaviors of the composition . This way , we can provide an accurate model of a wider class of service composition scenarios , while preserving as much as possible an efficient performance in verification ."
"912","Thresher : automating the unwrapping of semantic content from the World Wide Web We describe Thresher , a system that lets non-technical users teach their browsers how to extract semantic web content from HTML documents on the World Wide Web . Users specify examples of semantic content by highlighting them in a web browser and describing their meaning . We then use the tree edit distance between the DOM subtrees of these examples to create a general pattern , or wrapper , for the content , and allow the user to bind RDF classes and predicates to the nodes of these wrappers . By overlaying matches to these patterns on standard documents inside the Haystack semantic web browser , we enable a rich semantic interaction with existing web pages , `` unwrapping '' semantic data buried in the pages ' HTML . By allowing end-users to create , modify , and utilize their own patterns , we hope to speed adoption and use of the Semantic Web and its applications ."
"913","Popular web hot spots identification and visualization This work aims a two-fold contribution : it presents a software to analyze logfiles and visualize popular web hot spots and , additionally , presents an algorithm to use this information in order to identify subsets of the website that display large access patterns . Such information is extremely valuable to the site maintainer , since it indicates points that may need content intervention or\/and site graph restructuring . Experimental validation verified that the visualization tool , when coupled with algorithms that infer frequent traversal patterns , is both effective in indicating popular hot spots and efficient in doing so by using graph-based representations of popular traversals ."
"914","P-tree : a p2p index for resource discovery applications We propose a new distributed , fault-tolerant Peer-to-Peer index structure for resource discovery applications called the P-tree . P-trees efficiently support range queries in addition to equality queries ."
"915","Adaptive faceted browser for navigation in open information spaces Open information spaces have several unique characteristics such as their changeability , large size , complexity and diverse user base . These result in novel challenges during user navigation , information retrieval and data visualization in open information spaces . We propose a method of navigation in open information spaces based on an enhanced faceted browser with support for dynamic facet generation and adaptation based on user characteristics ."
"916","Visibly pushdown automata for streaming XML We propose the study of visibly pushdown automata ( VPA ) for processing XML documents . VPAs are pushdown automata where the input determines the stack operation , and XML documents are naturally visibly pushdown with the VPA pushing onto the stack on open-tags and popping the stack on close-tags . In this paper we demonstrate the power and ease visibly pushdown automata give in the design of streaming algorithms for XML documents . We study the problems of type-checking streaming XML documents against SDTD schemas , and the problem of typing tags in a streaming XML document according to an SDTD schema . For the latter problem , we consider both pre-order typing and post-order typing of a document , which dynamically determines types at open-tags and close-tags respectively as soon as they are met . We also generalize the problems of pre-order and post-order typing to prefix querying . We show that a deterministic VPA yields an algorithm to the problem of answering in one pass the set of all answers to any query that has the property that a node satisfying the query is determined solely by the prefix leading to the node . All the streaming algorithms we develop in this paper are based on the construction of deterministic VPAs , and hence , for any fixed problem , the algorithms process each element of the input in constant time , and use space ( d ) , where d is the depth of the document ."
"917","Mining the search trails of surfing crowds : identifying relevant websites from user activity The paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many Web users . While it has been previously shown that user interactions with search engines can be employed to improve document ranking , browsing behavior that occurs beyond search result pages has been largely overlooked in prior work . The paper demonstrates that users ' post-search browsing activity strongly reflects implicit endorsement of visited pages , which allows estimating topical relevance of Web resources by mining large-scale datasets of search trails . We present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries . Experimental evaluation shows that exploiting complete post-search browsing trails outperforms alternatives in isolation ( e.g. , clickthrough logs ) , and yields accuracy improvements when employed as a feature in learning to rank for Web search ."
"918","Evaluation of edge caching\/offloading for dynamic content delivery As dynamic content becomes increasingly dominant , it becomes an important research topic as how the edge resources such as client-side proxies , which are otherwise underutilized for such content , can be put into use . However , it is unclear what will be the best strategy and the design\/deployment tradeoffs lie therein . In this paper , using one representative e-commerce benchmark , we report our experience of an extensive investigation of different offloading and caching options . Our results point out that , while great benefits can be reached in general , advanced offloading strategies can be overly complex and even counter-productive . In contrast , simple augmentation at proxies to enable fragment caching and page composition achieves most of the benefit without compromising important considerations such as security ."
"919","Extraction and search of chemical formulae in text documents on the web Often scientists seek to search for articles on the Web related to a particular chemical . When a scientist searches for a chemical formula using a search engine today , she gets articles where the exact keyword string expressing the chemical formula is found . Searching for the exact occurrence of keywords during searching results in two problems for this domain : a ) if the author searches for CH4 and the article has H4C , the article is not returned , and b ) ambiguous searches like `` He '' return all documents where Helium is mentioned as well as documents where the pronoun `` he '' occurs . To remedy these deficiencies , we propose a chemical formula search engine . To build a chemical formula search engine , we must solve the following problems : 1 ) extract chemical formulae from text documents , 2 ) index chemical formulae , and 3 ) designranking functions for the chemical formulae . Furthermore , query models are introduced for formula search , and for each a scoring scheme based on features of partial formulae is proposed tomeasure the relevance of chemical formulae and queries . We evaluate algorithms for identifying chemical formulae in documents using classification methods based on Support Vector Machines ( SVM ) , and a probabilistic model based on conditional random fields ( CRF ) . Different methods for SVM and CRF to tune the trade-off between recall and precision forim balanced data are proposed to improve the overall performance . A feature selection method based on frequency and discrimination isused to remove uninformative and redundant features . Experiments show that our approaches to chemical formula extraction work well , especially after trade-off tuning . The results also demonstrate that feature selection can reduce the index size without changing ranked query results much ."
"920","Ranking refinement and its application to information retrieval We consider the problem of ranking refinement , i.e. , to improve the accuracy of an existing ranking function with a small set of labeled instances . We are , particularly , interested in learning a better ranking function using two complementary sources of information , ranking information given by the existing ranking function ( i.e. , a base ranker ) and that obtained from users ' feedbacks . This problem is very important in information retrieval where the feedback is gradually collected . The key challenge in combining the two sources of information arises from the fact that the ranking information presented by the base ranker tends to be imperfect and the ranking information obtained from users ' feedbacks tends to be noisy . We present a novel boosting framework for ranking refinement that can effectively leverage the uses of the two sources of information . Our empirical study shows that the proposed algorithm is effective for ranking refinement , and furthermore significantly outperforms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature ."
"921","Dynamic maintenance of web indexes using landmarks Recent work on incremental crawling has enabled the indexed document collection of a search engine to be more synchronized with the changing World Wide Web . However , this synchronized collection is not immediately searchable , because the keyword index is rebuilt from scratch less frequently than the collection can be refreshed . An inverted index is usually used to index documents crawled from the web . Complete index rebuild at high frequency is expensive . Previous work on incremental inverted index updates have been restricted to adding and removing documents . Updating the inverted index for previously indexed documents that have changed has not been addressed . In this paper , we propose an efficient method to update the inverted index for previously indexed documents whose contents have changed . Our method uses the idea of landmarks together with the diff algorithm to significantly reduce the number of postings in the inverted index that need to be updated . Our experiments verify that our landmark-diff method results in significant savings in the number of update operations on the inverted index ."
"922","Optimal marketing strategies over social networks We discuss the use of social networks in implementing viral marketing strategies . While influence maximization has been studied in this context ( see Chapter 24 of ( 10 ) ) , we study revenue maximization , arguably , a more natural objective . In our model , a buyer 's decision to buy an item is influenced by the set of other buyers that own the item and the price at which the item is offered . We focus on algorithmic question of finding revenue maximizing marketing strategies . When the buyers are completely symmetric , we can find the optimal marketing strategy in polynomial time . In the general case , motivated by hardness results , we investigate approximation algorithms for this problem . We identify a family of strategies called influence-and-exploit strategies that are based on the following idea : Initially influence the population by giving the item for free to carefully a chosen set of buyers . Then extract revenue from the remaining buyers using a ` greedy ' pricing strategy . We first argue why such strategies are reasonable and then show how to use recently developed set-function maximization techniques to find the right set of buyers to influence ."
"923","RDFPeers : a scalable distributed RDF repository based on a structured peer-to-peer network Centralized Resource Description Framework ( RDF ) repositories have limitations both in their failure tolerance and in their scalability . Existing Peer-to-Peer ( P2P ) RDF repositories either can not guarantee to find query results , even if these results exist in the network , or require up-front definition of RDF schemas and designation of super peers . We present a scalable distributed RDF repository ( RDFPeers ) that stores each triple at three places in a multi-attribute addressable network by applying globally known hash functions to its subject predicate and object . Thus all nodes know which node is responsible for storing triple values they are looking for and both exact-match and range queries can be efficiently routed to those nodes . RDFPeers has no single point of failure nor elevated peers and does not require the prior definition of RDF schemas . Queries are guaranteed to find matched triples in the network if the triples exist . In RDFPeers both the number of neighbors per node and the number of routing hops for inserting RDF triples and for resolving most queries are logarithmic to the number of nodes in the network . We further performed experiments that show that the triple-storing load in RDFPeers differs by less than an order of magnitude between the most and the least loaded nodes for real-world RDF data ."
"924","Reactive rules inference from dynamic dependency models Defining dependency models is sometimes an easier , more intuitive way for ontology representation than defining reactive rules directly , as it provides a higher level of abstraction . We will shortly introduce the ADI ( Active Dependency Integration ) model capabilities , emphasizing new developments : 1 . Support of automatic dependencies instantiation from an abstract definition that expresses a general dependency in the ontology , namely a `` template '' . 2 . Inference of rules for dynamic dependency models where dependencies and entities may be inserted deleted and updated . We use the eTrade example in order to exemplify those capabilities ."
"925","Supporting end-users in the creation of dependable web clips Web authoring environments enable end-users to create applications that integrate information from other web sources . Users can create web sites that include built-in components to dynamically incorporate , for example , weather information , stock-quotes , or the latest news from different web sources . Recent surveys conducted among end-users have indicated an increasing interest in creating such applications . Unfortunately , web authoring environments do not provide support beyond a limited set of built-in components . This work addresses this limitation by providing end-user support for `` clipping '' information from a target web site to incorporate it into the end-user site . The support consists of a mechanism to identify the target clipping with multiple markers to increase robustness , and a dynamic assessment of the retrieved information to quantify its reliability . The clipping approach has been integrated as a feature into a popular web authoring tool on which we present the results of two preliminary studies ."
"926","Webanywhere : enabling a screen reading interface for the web on any computer People often use computers other than their own to access web content , but blind users are restricted to using computers equipped with expensive , special-purpose screen reading programs that they use to access the web . WebAnywhere is a web-based , self-voicing web application that enables blind web users to access the web from almost any computer that can produce sound without installing new software . WebAnywhere could serve as a convenient , low-cost solution for blind users on-the-go , for blind users unable to afford another screen reader and for web developers targeting accessible design . This paper describes the implementation of WebAnywhere , overviews an evaluation of it by blind web users , and summarizes a survey of public terminals that shows it can run on most public computers ."
"927","A large-scale study of the evolution of web pages How fast does the web change ? Does most of the content remain unchanged once it has been authored , or are the documents continuously updated ? Do pages change a little or a lot ? Is the extent of change correlated to any other property of the page ? All of these questions are of interest to those who mine the web , including all the popular search engines , but few studies have been performed to date to answer them . One notable exception is a study by Cho and Garcia-Molina , who crawled a set of 720,000 pages on a daily basis over four months , and counted pages as having changed if their MD5 checksum changed . They found that 40 % of all web pages in their set changed within a week , and 23 % of those pages that fell into the . com domain changed daily . This paper expands on Cho and Garcia-Molina 's study , both in terms of coverage and in terms of sensitivity to change . We crawled a set of 150,836,209 HTML pages once every week , over a span of 11 weeks . For each page , we recorded a checksum of the page , and a feature vector of the words on the page , plus various other data such as the page length , the HTTP status code , etc. . Moreover , we pseudo-randomly selected 0.1 % of all of our URLs , and saved the full text of each download of the corresponding pages . After completion of the crawl , we analyzed the degree of change of each page , and investigated which factors are correlated with change intensity . We found that the average degree of change varies widely across top-level domains , and that larger pages change more often and more severely than smaller ones . This paper describes the crawl and the data transformations we performed on the logs , and presents some statistical observations on the degree of change of different classes of pages ."
"928","Monitoring the dynamic web to respond to continuous queries Continuous queries are queries for which responses given to users must be continuously updated , as the sources of interest get updated . Such queries occur , for instance , during on-line decision making , e.g. , traffic flow control , weather monitoring , etc. . The problem of keeping the responses current reduces to the problem of deciding how often to visit a source to determine if and how it has been modified , in order to update earlier responses accordingly . On the surface , this seems to be similar to the crawling problem since crawlers attempt to keep indexes up-to-date as pages change and users pose search queries . We show that this is not the case , both due to the inherent differences between the nature of the two problems as well as the performance metric . We propose , develop and evaluate a novel multi-phase ( Continuous Adaptive Monitoring ) ( CAM ) solution to the problem of maintaining the currency of query results . Some of the important phases are : The tracking phase , in which changes , to an initially identified set of relevant pages , are tracked . From the observed change characteristics of these pages , a probabilistic model of their change behavior is formulated and weights are assigned to pages to denote their importance for the current queries . During the next phase , the resource allocation phase , based on these statistics , resources , needed to continuously monitor these pages for changes , are allocated . Given these resource allocations , the scheduling phase produces an optimal achievable schedule for the monitoring tasks . An experimental evaluation of our approach compared to prior approaches for crawling dynamic web pages shows the effectiveness of CAM for monitoring dynamic changes . For example , by monitoring just 5 % of the page changes , CAM is able to return 90 % of the changed information to the users . The experiments also produce some interesting observations pertaining to the differences between the two problems of crawling -- to build an index -- and the problem of change tracking -- to respond to continuous queries ."
"929","Planetary-scale views on a large instant-messaging network We present a study of anonymized data capturing a month of high-level communication activities within the whole of the Microsoft Messenger instant-messaging system . We examine characteristics and patterns that emerge from the collective dynamics of large numbers of people , rather than the actions and characteristics of individuals . The dataset contains summary properties of 30 billion conversations among 240 million people . From the data , we construct a communication graph with 180 million nodes and 1.3 billion undirected edges , creating the largest social network constructed and analyzed to date . We report on multiple aspects of the dataset and synthesized graph . We find that the graph is well-connected and robust to node removal . We investigate on a planetary-scale the oft-cited report that people are separated by `` six degrees of separation '' and find that the average path length among Messenger users is 6.6 . We find that people tend to communicate more with each other when they have similar age , language , and location , and that cross-gender conversations are both more frequent and of longer duration than conversations with the same gender ."
"930","Generating diverse and representative image search results for landmarks Can we leverage the community-contributed collections of rich media on the web to automatically generate representative and diverse views of the world 's landmarks ? We use a combination of context - and content-based tools to generate representative sets of images for location-driven features and landmarks , a common search task . To do that , we using location and other metadata , as well as tags associated with images , and the images ' visual features . We present an approach to extracting tags that represent landmarks . We show how to use unsupervised methods to extract representative views and images for each landmark . This approach can potentially scale to provide better search and representation for landmarks , worldwide . We evaluate the system in the context of image search using a real-life dataset of 110,000 images from the San Francisco area ."
"931","Exception handling in workflow-driven Web applications As the Web becomes a platform for implementing B2B applications , the need arises of Web conceptual models for describing Web oriented workflow applications implementing business processes . In this context , new problems about process correctness arise , due to the loose control of Web applications upon the behavior of their Web clients . Indeed , incoherent user 's behavior can lead to inconsistent processes . This paper presents a high level approach to the management of exceptions that occur during the execution of processes on the Web . We present a classification of exceptions that can be raised inside workflow-driven Web applications , and recovery policies to retrieve coherent status and data after an exception . We devise these concepts at high level and then we exploit them using a Web modeling language ( WebML ) that in turn provides development facilities like automatic code generation , validation of hypertext models , and so on . An industrial implementation experience is briefly presented too ."
"932","OWL FA : a metamodeling extension of OWL D This paper proposes OWL FA , a decidable extension of OWL DL with the metamodeling architecture of RDFS ( FA ) . It shows that the knowledge base satisfiability problem of OWL FA can be reduced to that of OWL DL , and compares the FA semantics with the recently proposed contextual semantics and Hilog semantics for OWL ."
"933","Supporting anonymous location queries in mobile environments with privacygrid This paper presents PrivacyGrid - a framework for supporting anonymous location-based queries in mobile information delivery systems . The PrivacyGrid framework offers three unique capabilities . First , it provides a location privacy protection preference profile model , called location P3P , which allows mobile users to explicitly define their preferred location privacy requirements in terms of both location hiding measures ( e.g. , location k-anonymity and location l-diversity ) and location service quality measures ( e.g. , maximum spatial resolution and maximum temporal resolution ) . Second , it provides fast and effective location cloaking algorithms for location k-anonymity and location l-diversity in a mobile environment . We develop dynamic bottom-up and top-down grid cloaking algorithms with the goal of achieving high anonymization success rate and efficiency in terms of both time complexity and maintenance cost . A hybrid approach that carefully combines the strengths of both bottom-up and top-down cloaking approaches to further reduce the average anonymization time is also developed . Last but not the least , PrivacyGrid incorporates temporal cloaking into the location cloaking process to further increase the success rate of location anonymization . We also discuss PrivacyGrid mechanisms for supporting anonymous location queries . Experimental evaluation shows that the PrivacyGrid approach can provide close to optimal location k-anonymity as defined by per user location P3P without introducing significant performance penalties ."
"934","Recrawl scheduling based on information longevity It is crucial for a web crawler to distinguish between ephemeral and persistent content . Ephemeral content ( e.g. , quote of the day ) is usually not worth crawling , because by the time it reaches the index it is no longer representative of the web page from which it was acquired . On the other hand , content that persists across multiple page updates ( e.g. , recent blog postings ) may be worth acquiring , because it matches the page 's true content for a sustained period of time . In this paper we characterize the longevity of information found on the web , via both empirical measurements and a generative model that coincides with these measurements . We then develop new recrawl scheduling policies that take longevity into account . As we show via experiments over real web data , our policies obtain better freshness at lower cost , compared with previous approaches ."
"935","Multimedia meets computer graphics in SMIL2 .0 : a time model for the web Multimedia scheduling models provide a rich variety of tools for managing the synchronization of media like video and audio , but generally have an inflexible model for time itself . In contrast , modern animation models in the computer graphics community generally lack tools for synchronization and structural time , but allow for a flexible concept of time , including variable pacing , acceleration and deceleration and other tools useful for controlling and adapting animation behaviors . Multimedia authors have been forced to choose one set of features over the others , limiting the range of presentations they can create . Some programming models addressed some of these problems , but provided no declarative means for authors and authoring tools to leverage the functionality . This paper describes a new model incorporated into SMIL 2.0 that combines the strengths of scheduling models with the flexible time manipulations of animation models . The implications of this integration are discussed with respect to scheduling and structured time , drawing upon experience with SMIL 2.0 timing and synchronization , and the integration with XHTML ."
"936","Cat and mouse : content delivery tradeoffs in web access Web pages include extraneous material that may be viewed as undesirable by a user . Increasingly many Web sites also require users to register to access either all or portions of the site . Such tension between content owners and users has resulted in a `` cat and mouse '' game between content provided and how users access it . We carried out a measurement-based study to understand the nature of extraneous content and its impact on performance as perceived by users . We characterize how this content is distributed and the effectiveness of blocking mechanisms to stop it as well as countermeasures taken by content owners to negate such mechanisms . We also examine sites that require some form of registration to control access and the attempts made to circumvent it . Results from our study show that extraneous content exists on a majority of popular pages and that a 25-30 % reduction in downloaded objects and bytes with corresponding latency reduction can be attained by blocking such content . The top ten advertisement delivering companies delivered 40 % of all URLs matched as ads in our study . Both the server name and the remainder of the URL are important in matching a URL as an ad . A majority of popular sites require some form of registration and for such sites users can obtain an account from a shared public database . We discuss future measures and countermeasures on the part of each side ."
"937","Delay tolerant applications for low bandwidth and intermittently connected users : the aAQUA experience With the explosive growth and spread of Internet , web access from mobile and rural users has become significant . But these users face problems of low bandwidth and intermittent Internet connectivity . To make the benefits of the Internet reach the common man in developing countries , accessibility and availability of the information has to be improved . aAQUA is an online multilingual , multimedia agricultural portal for disseminating information from and to rural communities . Considering resource constrained rural environments , we have designed and implemented an offline solution which provides an online experience to users in disconnected mode . Our solution is based on heterogeneous database synchronization which involves only a small synchronization payload ensuring an efficient use of available bandwidth . Offline aAQUA has been deployed in the field and systematic studies of our solution show that user experience has improved tremendously not only in disconnected mode but also in connected mode ."
"938","Analysis of topological characteristics of huge online social networking services Social networking services are a fast-growing business in the Internet . However , it is unknown if online relationships and their growth patterns are the same as in real-life social networks . In this paper , we compare the structures of three online social networking services : Cyworld , MySpace , and orkut , each with more than 10 million users , respectively . We have access to complete data of Cyworld 's ilchon ( friend ) relationships and analyze its degree distribution , clustering property , degree correlation , and evolution over time . We also use Cyworld data to evaluate the validity of snowball sampling method , which we use to crawl and obtain partial network topologies of MySpace and orkut . Cyworld , the oldest of the three , demonstrates a changing scaling behavior over time in degree distribution . The latest Cyworld data 's degree distribution exhibits a multi-scaling behavior , while those of MySpace and orkut have simple scaling behaviors with different exponents . Very interestingly , each of the two e ponents corresponds to the different segments in Cyworld 's degree distribution . Certain online social networking services encourage online activities that can not be easily copied in real life ; we show that they deviate from close-knit online social networks which show a similar degree correlation pattern to real-life social networks ."
"939","Subspace : secure cross-domain communication for web mashups Combining data and code from third-party sources has enabled a new wave of web mashups that add creativity and functionality to web applications . However , browsers are poorly designed to pass data between domains , often forcing web developers to abandon security in the name of functionality . To address this deficiency , we developed Subspace , a cross-domain communication mechanism that allows efficient communication across domains without sacrificing security . Our prototype requires only a small JavaScript library , and works across all major browsers . We believe Subspace can serve as a new secure communication primitive for web mashups ."
"940","Automatic identification of user goals in Web search There has been recent interests in studying the `` goal '' behind a user 's Web query , so that this goal can be used to improve the quality of a search engine 's results . Previous studies have mainly focused on using manual query-log investigation to identify Web query goals . In this paper we study whether and how we can automate this goal-identification process . We first present our results from a human subject study that strongly indicate the feasibility of automatic query-goal identification . We then propose two types of features for the goal-identification task : user-click behavior and anchor-link distribution . Our experimental evaluation shows that by combining these features we can correctly identify the goals for 90 % of the queries studied ."
"941","The web around the corner : augmenting the browser with gps As programmable mobile devices ( such as high-end cellular phones and Personal Digital Assistants ) became widely adopted , users ask for Internet access on-the-road . While upcoming technologies like UMTS and Wi-Fi provide broadband wireless communication , Web services and Web browsers do not provide any sort of location-awareness yet . As GPS receivers get cheaper , positioning devices will be embedded into commercial mobile devices . Thus , the position of the user can be used to filter and tailor the information presented to the user as already done for language preferences and user-agent . This paper describes early results of an ongoing project called GPSWeb , which aims to provide GPS support for Web browsers and an application model for Location-Based Services . It introduces the Location-Based Browsing concept that enhances the classic Webuser-Website interaction ."
"942","A fast XPATH evaluation technique with the facility of updates This paper addresses the problem of fast retrieval of data from XML documents by providing a labeling schema that can easily handle simple as well as complex XPATH queries and also provide for updates without the need for the entire document being re-indexed in the RDBMS . We introduce a new labeling schema called the `` Z-Label '' for efficiently processing XPATH queries involving child and descendant axes . The use of `` Z-Label '' coupled with the indexing schema provides for smooth updates in the XML document ."
"943","Semantic resource management for the web : an e-learning application Topics in education are changing with an ever faster pace . ELearning resources tend to be more and more decentralized . Users increasingly need to be able to use the resources of the web . For this , they should have tools for finding and organizing information in a decentralized way . In this paper , we show how an ontologybased tool suite allows to make the most of the resources available on the web ."
"944","Web browsing performance of wireless thin-client computing Web applications are becoming increasingly popular for mobile wireless systems . However , wireless networks can have high packet loss rates , which can degrade web browsing performance on wireless systems . An alternative approach is wireless thin-client computing , in which the web browser runs on a remote thin server with a more reliable wired connection to the Internet . A mobile client then maintains a connection to the thin server to receive display updates over the lossy wireless network . To assess the viability of this thin-client approach , we compare the web browsing performance of thin clients against fat clients that run the web browser locally in lossy wireless networks . Our results show that thin clients can operate quite effectively over lossy networks . Compared to fat clients running web browsers locally , our results show surprisingly that thin clients can be faster and more resilient on web applications over lossy wireless LANs despite having to send more data over the network . We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance in lossy wireless networks ."
"945","A method for transparent admission control and request scheduling in e-commerce web sites This paper presents a method for admission control and request scheduling for multiply-tiered e-commerce Web sites , achieving both stable behavior during overload and improved response times . Our method externally observes execution costs of requests online , distinguishing different request types , and performs overload protection and preferential scheduling using relatively simple measurements and a straight forward control mechanism . Unlike previous proposals , which require extensive changes to the server or operating system , our method requires no modifications to the host O.S. , Web server , application server or database . Since our method is external , it can be implemented in a proxy . We present such an implementation , called Gatekeeper , using it with standard software components on the Linux operating system . We evaluate the proxy using the industry standard TPC-W workload generator in a typical three-tiered e-commerce environment . We show consistent performance during overload and throughput increases of up to 10 percent . Response time improves by up to a factor of 14 , with only a 15 percent penalty to large jobs ."
"946","Dynamic personalized pagerank in entity-relation graphs Extractors and taggers turn unstructured text into entity-relation ( ER ) graphs where nodes are entities ( email , paper , person , conference , company ) and edges are relations ( wrote , cited,works-for ) . Typed proximity search of the form ( B ) type = personNEAR company ~ `` IBM '' , paper ~ `` XML '' ( \/ B ) is an increasingly usefulsearch paradigm in ER graphs . Proximity search implementations either perform a Pagerank-like computation at query time , which is slow , or precompute , store and combine per-word Pageranks , which can be very expensive in terms of preprocessing time and space . We present HubRank , a new system for fast , dynamic , space-efficient proximity searches in ER graphs . During preprocessing , HubRank computesand indexes certain `` sketchy '' random walk fingerprints for a small fraction of nodes , carefully chosen using query log statistics . At query time , a small `` active '' subgraph is identified , bordered bynodes with indexed fingerprints . These fingerprints are adaptively loaded to various resolutions to form approximate personalized Pagerank vectors ( PPVs ) . PPVs at remaining active nodes are now computed iteratively . We report on experiments with CiteSeer 's ER graph and millions of real Cite Seer queries . Some representative numbers follow . On our testbed , HubRank preprocesses and indexes 52 times faster than whole-vocabulary PPV computation . A text index occupies 56 MB . Whole-vocabulary PPVs would consume 102GB . If PPVs are truncated to 56 MB , precision compared to true Pagerank drops to 0.55 ; incontrast , HubRank has precision 0.91 at 63MB . HubRank 's average querytime is 200-300 milliseconds ; query-time Pagerank computation takes 11 seconds on average ."
"947","On incremental maintenance of 2-hop labeling of graphs Recent interests on XML , Semantic Web , and Web ontology , among other topics , have sparked a renewed interest on graph-structured databases . A fundamental query on graphs is the reachability test of nodes . Recently , 2-hop labeling has been proposed to index large collections of XML and\/or graphs for efficient reachability tests . However , there has been few work on updates of 2-hop labeling . This is compounded by the fact that Web data changes over time . In response to these , this paper studies the incremental maintenance of 2-hop labeling . We identify the main reason for the inefficiency of updates of existing 2-hop labels . We propose two updatable 2-hop labelings , hybrids of 2-hop labeling , and their incremental maintenance algorithms . The proposed 2-hop labeling is derived from graph connectivities , as opposed to set cover which is used by all previous work . Our experimental evaluation illustrates the space efficiency and update performance of various kinds of 2-hop labeling . The main conclusion is that there is a natural way to spare some index size for update performance in 2-hop labeling ."
"948","Knowledge sharing and yahoo answers : everyone knows something Yahoo Answers ( YA ) is a large and diverse question-answer forum , acting not only as a medium for sharing technical knowledge , but as a place where one can seek advice , gather opinions , and satisfy one 's curiosity about a countless number of things . In this paper , we seek to understand YA 's knowledge sharing and activity . We analyze the forum categories and cluster them according to content characteristics and patterns of interaction among the users . While interactions in some categories resemble expertise sharing forums , others incorporate discussion , everyday advice , and support . With such a diversity of categories in which one can participate , we find that some users focus narrowly on specific topics , while others participate across categories . This not only allows us to map related categories , but to characterize the entropy of the users ' interests . We find that lower entropy correlates with receiving higher answer ratings , but only for categories where factual expertise is primarily sought after . We combine both user attributes and answer characteristics to predict , within a given category , whether a particular answer will be chosen as the best answer by the asker ."
"949","Constructing virtual documents for ontology matching On the investigation of linguistic techniques used in ontology matching , we propose a new idea of virtual documents to pursue a cost-effective approach to linguistic matching in this paper . Basically , as a collection of weighted words , the virtual document of a URIref declared in an ontology contains not only the local descriptions but also the neighboring information to reflect the intended meaning of the URIref . Document similarity can be computed by traditional vector space techniques , and then be used in the similarity-based approaches to ontology matching . In particular , the RDF graph structure is exploited to define the description formulations and the neighboring operations . Experimental results show that linguistic matching based on the virtual documents is dominant in average F-Measure as compared to other three approaches . It is also demonstrated by our experiments that the virtual documents approach is cost-effective as compared to other linguistic matching approaches ."
"950","Summarizing email conversations with clue words Accessing an ever increasing number of emails , possibly on small mobile devices , has become a major problem for many users . Email summarization is a promising way to solve this problem . In this paper , we propose a new framework for email summarization . One novelty is to use a fragment quotation graph to try to capture an email conversation . The second novelty is to use clue words to measure the importance of sentences in conversation summarization . Based on clue words and their scores , we propose a method called CWS , which is capable of producing a summary of any length as requested by the user . We provide a comprehensive comparison of CWS with various existing methods on the Enron data set . Preliminary results suggest that CWS provides better summaries than existing methods ."
"951","Fine grained content-based adaptation mechanism for providing high end-user quality of experience with adaptive hypermedia systems New communication technologies can enable Web users to access personalised information `` anytime , anywhereö . However , the network environments allowing this `` anytime , anywhereö access may have widely varying performance characteristics such as bandwidth , level of congestion , mobility support , and cost of transmission . It is unrealistic to expect that the quality of delivery of the same content can be maintained in this variable environment , but rather an effort must be made to fit the content served to the current delivery conditions , thus ensuring high Quality of Experience ( QoE ) to the users . This paper introduces an end-user QoE-aware adaptive hypermedia framework that extends the adaptation functionality of adaptive hypermedia systems with a fine-grained content-based adaptation mechanism . The proposed mechanism attempts to take into account multiple factors affecting QoE in relation to the delivery of Web content . Various simulation tests investigate the performance improvements provided by this mechanism , in a home-like , low bit rate operational environment , in terms of access time per page , aggregate access time per browsing session and quantity of transmitted information ."
"952","Scaling link-based similarity search To exploit the similarity information hidden in the hyperlink structure of the web , this paper introduces algorithms scalable to graphs with billions of vertices on a distributed architecture . The similarity of multi-step neighborhoods of vertices are numerically evaluated by similarity functions including SimRank ( 20 ) , a recursive refinement of cocitation ; PSimRank , a novel variant with better theoretical characteristics ; and the Jaccard coefficient , extended to multi-step neighborhoods . Our methods are presented in a general framework of Monte Carlo similarity search algorithms that precompute an index database of random fingerprints , and at query time , similarities are estimated from the fingerprints . The performance and quality of the methods were tested on the Stanford Webbase ( 19 ) graph of 80M pages by comparing our scores to similarities extracted from the ODP directory ( 26 ) . Our experimental results suggest that the hyperlink structure of vertices within four to five steps provide more adequate information for similarity search than single-step neighborhoods ."
"953","A uniform approach to accelerated PageRank computation In this note we consider a simple reformulation of the traditional power iteration algorithm for computing the stationary distribution of a Markov chain . Rather than communicate their current probability values to their neighbors at each step , nodes instead communicate only changes in probability value . This reformulation enables a large degree of flexibility in the manner in which nodes update their values , leading to an array of optimizations and features , including faster convergence , efficient incremental updating , and a robust distributed implementation . While the spirit of many of these optimizations appear in previous literature , we observe several cases where this unification simplifies previous work , removing technical complications and extending their range of applicability . We implement and measure the performance of several optimizations on a sizable ( 34M node ) web subgraph , seeing significant composite performance gains , especially for the case of incremental recomputation after changes to the web graph ."
"954","Analysis of interacting BPEL web services This paper presents a set of tools and techniques for analyzing interactions of composite web services which are specified in BPEL and communicate through asynchronous XML messages . We model the interactions of composite web services as conversations , the global sequence of messages exchanged by the web services . As opposed to earlier work , our tool-set handles rich data manipulation via XPath expressions . This allows us to verify designs at a more detailed level and check properties about message content . We present a framework where BPEL specifications of web services are translated to an intermediate representation , followed by the translation of the intermediate representation to a verification language . As an intermediate representation we use guarded automata augmented with unbounded queues for incoming messages , where the guards are expressed as XPath expressions . As the target verification language we use Promela , input language of the model checker SPIN . Since SPIN model checker is a finite-state verification tool we can only achieve partial verification by fixing the sizes of the input queues in the translation . We propose the concept of synchronizability to address this problem . We show that if a composite web service is synchronizable , then its conversation set remains same when asynchronous communication is replaced with synchronous communication . We give a set of sufficient conditions that guarantee synchronizability and that can be checked statically . Based on our synchronizability results , we show that a large class of composite web services with unbounded input queues can be completely verified using a finite state model checker such as SPIN ."
"955","Personalized interactive faceted search Faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces . A faceted search system presents users with key-value metadata that is used for query refinement . While popular in e-commerce and digital libraries , not much research has been conducted on which metadata to present to a user in order to improve the search experience . Nor are there repeatable benchmarks for evaluating a faceted search engine . This paper proposes the use of collaborative filtering and personalization to customize the search interface to each user 's behavior . This paper also proposes a utility based framework to evaluate the faceted interface . In order to demonstrate these ideas and better understand personalized faceted search , several faceted search algorithms are proposed and evaluated using the novel evaluation methodology ."
"956","Genealogical trees on the web : a search engine user perspective This paper presents an extensive study about the evolution of textual content on the Web , which shows how some new pages are created from scratch while others are created using already existing content . We show that a significant fraction of the Web is a byproduct of the latter case . We introduce the concept of Web genealogical tree , in which every page in a Web snapshot is classified into a component . We study in detail these components , characterizing the copies and identifying the relation between a source of content and a search engine , by comparing page relevance measures , documents returned by real queries performed in the past , and click-through data . We observe that sources of copies are more frequently returned by queries and more clicked than other documents ."
"957","User-centric Web crawling Search engines are the primary gateways of information access on the Web today . Behind the scenes , search engines crawl the Web to populate a local indexed repository of Web pages , used to answer user search queries . In an aggregate sense , the Web is very dynamic , causing any repository of Web pages to become out of date over time , which in turn causes query answer quality to degrade . Given the considerable size , dynamicity , and degree of autonomy of the Web as a whole , it is not feasible for a search engine to maintain its repository exactly synchronized with the Web . In this paper we study how to schedule Web pages for selective ( re ) downloading into a search engine repository . The scheduling objective is to maximize the quality of the user experience for those who query the search engine . We begin with a quantitative characterization of the way in which the discrepancy between the content of the repository and the current content of the live Web impacts the quality of the user experience . This characterization leads to a user-centric metric of the quality of a search engine 's local repository . We use this metric to derive a policy for scheduling Web page ( re ) downloading that is driven by search engine usage and free of exterior tuning parameters . We then focus on the important subproblem of scheduling refreshing of Web pages already present in the repository , and show how to compute the priorities efficiently . We provide extensive empirical comparisons of our user-centric method against prior Web page refresh strategies , using real Web data . Our results demonstrate that our method requires far fewer resources to maintain same search engine quality level for users , leaving substantially more resources available for incorporating new Web pages into the search repository ."
"958","Finding the right facts in the crowd : factoid question answering over social media Community Question Answering has emerged as a popular and effective paradigm for a wide range of information needs . For example , to find out an obscure piece of trivia , it is now possible and even very effective to post a question on a popular community QA site such as Yahoo ! Answers , and to rely on other users to provide answers , often within minutes . The importance of such community QA sites is magnified as they create archives of millions of questions and hundreds of millions of answers , many of which are invaluable for the information needs of other searchers . However , to make this immense body of knowledge accessible , effective answer retrieval is required . In particular , as any user can contribute an answer to a question , the majority of the content reflects personal , often unsubstantiated opinions . A ranking that combines both relevance and quality is required to make such archives usable for factual information retrieval . This task is challenging , as the structure and the contents of community QA archives differ significantly from the web setting . To address this problem we present a general ranking framework for factual information retrieval from social media . Results of a large scale evaluation demonstrate that our method is highly effective at retrieving well-formed , factual answers to questions , as evaluated on a standard factoid QA benchmark . We also show that our learning framework can be tuned with the minimum of manual labeling . Finally , we provide result analysis to gain deeper understanding of which features are significant for social media search and retrieval . Our system can be used as a crucial building block for combining results from a variety of social media content with general web search results , and to better integrate social media content for effective information access ."
"959","Retrieving multimedia web objects based on PageRank algorithm Hyperlink analysis has been widely investigated to support the retrieval of Web documents in Internet search engines . It has been proven that the hyperlink analysis significantly improves the relevance of the search results and these techniques have been adopted in many commercial search engines , e.g. Google . However , hyperlink analysis is mostly utilized in the ranking mechanism of Web pages only but not including other multimedia objects , such as images and video . In this project , we propose a modified Multimedia PageRank algorithm to support the searching of multimedia objects in the Web ."
"960","Learning to map between ontologies on the semantic web Ontologies play a prominent role on the Semantic Web . They make possible the widespread publication of machine understandable data , opening myriad opportunities for automated information processing . However , because of the Semantic Web 's distributed nature , data on it will inevitably come from many different ontologies . Information processing across ontologies is not possible without knowing the semantic mappings between their elements . Manually finding such mappings is tedious , error-prone , and clearly not possible at the Web scale . Hence , the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web . We describe glue , a system that employs machine learning techniques to find such mappings . Given two ontologies , for each concept in one ontology glue finds the most similar concept in the other ontology . We give well-founded probabilistic definitions to several practical similarity measures , and show that glue can work with all of them . This is in contrast to most existing approaches , which deal with a single similarity measure . Another key feature of glue is that it uses multiple learning strategies , each of which exploits a different type of information either in the data instances or in the taxonomic structure of the ontologies . To further improve matching accuracy , we extend glue to incorporate commonsense knowledge and domain constraints into the matching process . For this purpose , we show that relaxation labeling , a well-known constraint optimization technique used in computer vision and other fields , can be adapted to work efficiently in our context . Our approach is thus distinguished in that it works with a variety of well-defined similarity notions and that it efficiently incorporates multiple types of knowledge . We describe a set of experiments on several real-world domains , and show that glue proposes highly accurate semantic mappings ."
"961","On the bursty evolution of blogspace We propose two new tools to address the evolution of hyperlinked corpora . First , we define time graphs to extend the traditional notion of an evolving directed graph , capturing link creation as a point phenomenon in time . Second , we develop definitions and algorithms for time-dense community tracking , to crystallize the notion of community evolution . We develop these tools in the context of Blogspace , the space of weblogs ( or blogs ) . Our study involves approximately 750K links among 25K blogs . We create a time graph on these blogs by an automatic analysis of their internal time stamps . We then study the evolution of connected component structure and microscopic community structure in this time graph . We show that Blogspace underwent a transition behavior around the end of 2001 , and has been rapidly expanding over the past year , not just in metrics of scale , but also in metrics of community structure and connectedness . This expansion shows no sign of abating , although measures of connectedness must plateau within two years . By randomizing link destinations in Blogspace , but retaining sources and timestamps , we introduce a concept of randomized Blogspace . Herein , we observe similar evolution of a giant component , but no corresponding increase in community structure . Having demonstrated the formation of micro-communities over time , we then turn to the ongoing activity within active communities . We extend recent work of Kleinberg ( 11 ) to discover dense periods of `` bursty '' intra-community link creation ."
"962","Semantic Wikipedia Wikipedia is the world 's largest collaboratively edited source of encyclopaedic knowledge . But in spite of its utility , its contents are barely machine-interpretable . Structural knowledge , e. , g. about how concepts are interrelated , can neither be formally stated nor automatically processed . Also the wealth of numerical data is only available as plain text and thus can not be processed by its actual meaning . We provide an extension to be integrated in Wikipedia , that allows the typing of links between articles and the specification of typed data inside the articles in an easy-to-use manner . Enabling even casual users to participate in the creation of an open semantic knowledge base , Wikipedia has the chance to become a resource of semantic statements , hitherto unknown regarding size , scope , openness , and internationalisation . These semantic enhancements bring to Wikipedia benefits of today 's semantic technologies : more specific ways of searching and browsing . Also , the RDF export , that gives direct access to the formalised knowledge , opens Wikipedia up to a wide range of external applications , that will be able to use it as a background knowledge base . In this paper , we present the design , implementation , and possible uses of this extension ."
"963","Compositional knowledge management for medical services on semantic web The vision of the Semantic Web is to reduce manual discovery and usage of Web resources ( documents and services ) and to allow software agents to automatically identify these Web resources , integrate them and execute them for achieving the intended goals of the user . Such a composed Web service may be represented as a workflow , called service flow . Current Web service standards are not sufficient for automatic composition . This paper presents different types of compositional knowledge required for Web service discovery and composition . As a proof of concept , we have implemented our framework in a cardiovascular domain which requires advanced service discovery and composition across heterogeneous platforms of multiple organizations ."
"964","Analyzing search engine advertising : firm behavior and cross-selling in electronic markets The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines . Firms across different industries have are beginning to adopt this as the primary form of online advertising . This process works on an auction mechanism in which advertisers bid for different keywords , and final rank for a given keyword is allocated by the search engine . But how different are firm 's actual bids from their optimal bids ? Moreover , what are other ways in which firms can potentially benefit from sponsored search advertising ? Based on the model and estimates from prior work ( 10 ) , we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords . Further , we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories , and estimate the model using Markov Chain Monte Carlo ( MCMC ) methods . Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits . We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement . We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for . Latency ( the time it takes for consumer to place a purchase order after clicking on the advertisement ) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet ."
"965","eBag : a ubiquitous Web infrastructure for nomadic learning This paper describes the eBag infrastructure , which is a generic infrastructure inspired from work with school children who could benefit from a electronic schoolbag for collaborative handling of their digital material . The eBag infrastructure is utilizing the Context-aware HyCon framework and collaborative web services based on WebDAV . A ubiquitous login and logout mechanism has been built based on BlueTooth sensor networks . The eBag infrastructure has been tried out in field tests with school kids . In this paper we discuss experiences and design issues for ubiquitous Web integration in interactive school environments with multiple interactive whiteboards and workstations . This includes proposals for specialized and adaptive XLink structures for organizing school materials as well as issues in login\/logout based on proximity of different display surfaces ."
"966","Hierarchical substring caching for efficient content distribution to low-bandwidth clients While overall bandwidth in the internet has grown rapidly over the last few years , and an increasing number of clients enjoy broadband connectivity , many others still access the internet over much slower dialup or wireless links . To address this issue , a number of techniques for optimized delivery of web and multimedia content over slow links have been proposed , including protocol optimizations , caching , compression , and multimedia transcoding , and several large ISPs have recently begun to widely promote dialup acceleration services based on such techniques . A recent paper by Rhea , Liang , and Brewer proposed an elegant technique called value-based caching that caches substrings of files , rather than entire files , and thus avoids repeated transmission of substrings common to several pages or page versions . We propose and study a hierarchical substring caching technique that provides significant savings over this basic approach . We describe several additional techniques for minimizing overheads and perform an evaluation on a large set of real web access traces that we collected . In the second part of our work , we compare our approach to a widely studied alternative approach based on delta compression , and show how to integrate the two for best overall performance . The studied techniques are typically employed in a client-proxy environment , with each proxy serving a large number of clients , and an important aspect is how to conserve resources on the proxy while exploiting the significant memory and CPU power available on current clients ."
"967","Contextual advertising by combining relevance with click feedback Contextual advertising supports much of the Web 's ecosystem today . User experience and revenue ( shared by the site publisher and the ad network ) depend on the relevance of the displayed ads to the page content . As with other document retrieval systems , relevance is provided by scoring the match between individual ads ( documents ) and the content of the page where the ads are shown ( query ) . In this paper we show how this match can be improved significantly by augmenting the ad-page scoring function with extra parameters from a logistic regression model on the words in the pages and ads . A key property of the proposed model is that it can be mapped to standard cosine similarity matching and is suitable for efficient and scalable implementation over inverted indexes . The model parameter values are learned from logs containing ad impressions and clicks , with shrinkage estimators being used to combat sparsity . To scale our computations to train on an extremely large training corpus consisting of several gigabytes of data , we parallelize our fitting algorithm in a Hadoop framework ( 10 ) . Experimental evaluation is provided showing improved click prediction over a holdout set of impression and click events from a large scale real-world ad placement engine . Our best model achieves a 25 % lift in precision relative to a traditional information retrieval model which is based on cosine similarity , for recalling 10 % of the clicks in our test data ."
"968","Distribution of relevant documents in domain-level aggregates for topic distillation In this paper , we study the distribution of relevant documents in aggregates , formed by grouping the retrieved documents according to their domain . For each aggregate , we take into account its size , and a measure of the correlation between its incoming and outgoing hyperlinks . We report on a preliminary experiment with two TREC topic distillation tasks , where we find that larger aggregates , or those aggregates with correlated hyperlinks , are more likely to contain relevant documents . This result shows that the distribution of domain-level aggregates is potentially useful for finding relevant documents ."
"969","Mining RDF metadata for generalized association rules : knowledge discovery in the semantic web era In this paper , we present a novel frequent generalized pattern mining algorithm , called GP-Close , for mining generalized associations from RDF metadata . To solve the over-generalization problem encountered by existing methods , GP-Close employs the notion of emphgeneralization closure for systematic over-generalization reduction ."
"970","Predictive caching and prefetching of query results in search engines We study the caching of query result pages in Web search engines . Popular search engines receive millions of queries per day , and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements . We present PDC ( probability driven cache ) , a novel scheme tailored for caching search results , that is based on a probabilistic model of search engine users . We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC , as well as traditional LRU and SLRU based caching schemes . The trace driven simulations show that PDC outperforms the other policies . We also examine the prefetching of search results , and demonstrate that prefetching can increase cache hit ratios by 50 % for large caches , and can double the hit ratios of small caches . When integrating prefetching into PDC , we attain hit ratios of over 0.53 ."
"971","Delivering web service coordination capability to users As web service technology matures there is growing interest in exploiting workflow techniques to coordinate web services . Bioinformaticians are a user community who combine web resources to perform in silico experiments . These users are scientists and not information technology experts they require workflow solutions that have a low cost of entry for service users and providers . Problems satisfying these requirements with current techniques led to the development of the Simple conceptual unified flow language ( Scufl ) . Scufl is supported by the Freefluo enactment engine ( 1 ) , and the Taverna editing workbench ( 3 ) . The extensibility of Scufl , supported by these tools , means that workflows coordinating web services can be matched to how users view their problems . The Taverna workbench exploits the web to keep Scufl simple by retrieving detail from URIs when required , and by scavenging the web for services . Scufl and its tools are not bioinformatics specific . They can be exploited by other communities who require user-driven composition and execution of workflows coordinating web resources ."
"972","Agent-based semantic web services The Web Services world consists of loosely-coupled distributed systems which adapt to ad-hoc changes by the use of service descriptions that enable opportunistic service discovery . At present , these service descriptions are semantically impoverished , being concerned with describing the functional signature of the services rather than characterising their meaning . In the Semantic Web community , the DAML Services effort attempts to rectify this by providing a more expressive way of describing Web services using ontologies . However , this approach does not separate the domain-neutral communicative intent of a message ( considered in terms of speech acts ) from its domain-specific content , unlike similar developments from the multi-agent systems community . In this paper , we describe our experiences of designing and building an ontologically motivated Web Services system for situational awareness and information triage in a simulated humanitarian aid scenario . In particular , we discuss the merits of using techniques from the multi-agent systems community for separating the intentional force of messages from their content , and the implementation of these techniques within the DAML Services model ."
"973","Automatically refining the wikipedia infobox ontology The combined efforts of human volunteers have recently extracted numerous facts from Wikipedia , storing them as machine-harvestable object-attribute-value triples in Wikipedia infoboxes . Machine learning systems , such as Kylin , use these infoboxes as training data , accurately extracting even more semantic knowledge from natural language text . But in order to realize the full power of this information , it must be situated in a cleanly-structured ontology . This paper introduces KOG , an autonomous system for refining Wikipedia 's infobox-class ontology towards this end . We cast the problem of ontology refinement as a machine learning problem and solve it using both SVMs and a more powerful joint-inference approach expressed in Markov Logic Networks . We present experiments demonstrating the superiority of the joint-inference approach and evaluating other aspects of our system . Using these techniques , we build a rich ontology , integrating Wikipedia 's infobox-class schemata with WordNet . We demonstrate how the resulting ontology may be used to enhance Wikipedia with improved query processing and other features ."
"974","A flexible learning system for wrapping tables and lists in HTML documents A program that makes an existing website look like a database is called a wrapper . Wrapper learning is the problem of learning website wrappers from examples . We present a wrapper-learning system called WL2 that can exploit several different representations of a document . Examples of such different representations include DOM-level and token-level representations , as well as two-dimensional geometric views of the rendered page ( for tabular data ) and representations of the visual appearance of text asm it will be rendered . Additionally , the learning system is modular , and can be easily adapted to new domains and tasks . The learning system described is part of an `` industrial-strength '' wrapper management system that is in active use at WhizBang Labs . Controlled experiments show that the learner has broader coverage and a faster learning rate than earlier wrapper-learning systems ."
"975","Ensuring required failure atomicity of composite Web services The recent evolution of Internet , driven by the Web services technology , is extending the role of the Web from a support of information interaction to a middleware for B2B interactions . Indeed , the Web services technology allows enterprises to outsource parts of their business processes using Web services . And it also provides the opportunity to dynamically offer new value-added services through the composition of pre-existing Web services . In spite of the growing interest in Web services , current technologies are found lacking efficient transactional support for composite Web services ( CSs ) . In this paper , we propose a transactional approach to ensure the failure atomicity , of a CS , required by partners . We use the Accepted Termination States ( ATS ) property as a mean to express the required failure atomicity . Partners specify their CS , mainly its control flow , and the required ATS . Then , we use a set of transactional rules to assist designers to compose a valid CS with regards to the specified ATS ."
"976","SPath : a path language for XML schema XML is increasingly being used as a typed data format , and therefore it becomes more important to gain access to the type system ; very often this is an XML Schema . The XML Schema Path Language ( SPath ) presented in this paper provides access to XML Schema components by extending the well-known XPath language to also include the domain of XML Schemas . Using SPath , XML developers gain access to XML Schemas and thus can more easily develop software which is type - or schema-aware , and thus more robust ."
"977","Learning information intent via observation Users in an organization frequently request help by sending request messages to assistants that express information intent : an intention to update data in an information system . Human assistants spend a significant amount of time and effort processing these requests . For example , human resource assistants process requests to update personnel records , and executive assistants process requests to schedule conference rooms or to make travel reservations . To process the intent of a request , assistants read the request and then locate , complete , and submit a form that corresponds to the expressed intent . Automatically or semi-automatically processing the intent expressed in a request on behalf of an assistant would ease the mundane and repetitive nature of this kind of work . For a well-understood domain , a straightforward application of natural language processing techniques can be used to build an intelligent form interface to semi-automatically process information intent request messages . However , high performance parsers are based on machine learning algorithms that require a large corpus of examples that have been labeled by an expert . The generation of a labeled corpus of requests is a major barrier to the construction of a parser . In this paper , we investigate the construction of a natural language processing system and an intelligent form system that observes an assistant processing requests . The intelligent form system then generates a labeled training corpus by interpreting the observations . This paper reports on the measurement of the performance of the machine learning algorithms based on real data . The combination of observations , machine learning and interaction design produces an effective intelligent form interface based on natural language processing ."
"978","A larger scale study of robots . txt A website can regulate search engine crawler access to its content using the robots exclusion protocol , specified in its robots . txt file . The rules in the protocol enable the site to allow or disallow part or all of its content to certain crawlers , resulting in a favorable or unfavorable bias towards some of them . A 2007 survey on the robots . txt usage of about 7,593 sites found some evidence of such biases , the news of which led to widespread discussions on the web . In this paper , we report on our survey of about 6 million sites . Our survey tries to correct the shortcomings of the previous survey and shows the lack of any significant preferences towards any particular search engine ."
"979","An automatic semantic relationships discovery approach An important obstacle to the success of the Semantic Web is that the establishment of the semantic relationship is labor-intensive . This paper proposes an automatic semantic relationship discovering approach for constructing the semantic link network . The basic premise of this work is that the semantics of a web page can be reflected by a set of keywords , and the semantic relationship between two web pages can be determined by the semantic relationship between their keyword sets . The approach adopts the data mining algorithms to discover the semantic relationships between keyword sets , and then uses deductive and analogical reasoning to enrich the semantic relationships . The proposed algorithms have been implemented . Experiment shows that the approach is feasible ."
"980","PageRank as a function of the damping factor PageRank is defined as the stationary state of a Markov chain . The chain is obtained by perturbing the transition matrix induced by a web graph with a damping factor α that spreads uniformly part of the rank . The choice of α is eminently empirical , and in most cases the original suggestion α = 0.85 by Brin and Page is still used . Recently , however , the behavior of PageRank with respect to changes in α was discovered to be useful in link-spam detection ( 21 ) . Moreover , an analytical justification of the value chosen for α is still missing . In this paper , we give the first mathematical analysis of PageRank when α changes . In particular , we show that , contrarily to popular belief , for real-world graphs values of α close to 1 do not give a more meaningful ranking . Then , we give closed-form formulae for PageRank derivatives of any order , and an extension of the Power Method that approximates them with convergence O ( tk αt ) for the k-th derivative . Finally , we show a tight connection between iterated computation and analytical behavior by proving that the k-th iteration of the Power Method gives exactly the PageRank value obtained using a Maclaurin polynomial of degree k. The latter result paves the way towards the application of analytical methods to the study of PageRank ."
"981","Model-based version and configuration management for a web engineering lifecycle During a lifecycle of a large-scale Web application , Web developers produce a wide variety of inter-related Web objects . Following good Web engineering practice , developers often create them based on a Web application development method , which requires certain logical models for the development and maintenance process . Web development is dynamic , thus , those logical models as well as Web artifacts evolve over time . However , the task of managing their evolution is still very inefficient because design decisions in models are not directly accessible in existing file-based software configuration management repositories . Key limitations of existing Web version control tools include their inadequacy in representing semantics of design models and inability to manage the evolution of model-based objects and their logical connections to Web documents . This paper presents a framework that allows developers to manage versions and configurations of models and to capture changes to model-to-model relations among Web objects . Model-based objects , Web documents , and relations are directly represented and versioned in a structure-oriented manner ."
"982","WEBCAP : a capacity planning tool for web resource management A staggering number of multimedia applications are being introduced every day . Yet , the inordinate delays encountered in retrieving multimedia documents make it difficult to use the Web for real-time applications such as educational broadcasting , video conferencing , and multimedia streaming . The problem of delivering multimedia documents in time while placing the least demands on the client , network and server resources is a challenging optimization problem . The WEBCAP is ongoing project that explores applying capacity planning techniques to manage or tune the Web resources ( client , network , server ) for optimal or near optimal performance , subject to minimizing the retrieval cost while satisfying the real-time constraints and available resources . The WEBCAP project consists of four software modules : object extractor , object representer , object scheduler , and system tuner . The four modules are connected serially with 3 feedback-loops . In this paper , we focus on how to extract objects from multimedia document and how to represent them as object and operation flow graphs while maintaining precedence relations among the objects ."
"983","G-ToPSS : fast filtering of graph-based metadata RDF is increasingly being used to represent metadata . RDF Site Summary ( RSS ) is an application of RDF on the Web that has considerably grown in popularity . However , the way RSS systems operate today does not scale well . In this paper we introduce G-ToPSS , a scalable publish\/subscribe system for selective information dissemination . G-ToPSS is particularly well suited for applications that deal with large-volume content distribution from diverse sources . RSS is an instance of the content distribution problem . G-ToPSS allows use of ontology as a way to provide additional information about the data . Furthermore , in this paper we show how G-ToPSS can support RDFS class taxonomies . We have implemented and experimentally evaluated G-ToPSS and we provide results in the paper demonstrating its scalability compared to alternatives ."
"984","Better abstractions for secure server-side scripting It is notoriously difficult to program a solid web application . Besides addressing web interactions , state maintenance , and whimsical user navigation behaviors , programmers must also avoid a minefield of security vulnerabilities . The problem is twofold . First , we lack a clear understanding of the new computation model underlying web applications . Second , we lack proper abstractions for hiding common and subtle coding details that are orthogonal to the business functionalities of specific web applications . This paper addresses both issues . First , we present a language BASS for declarative server-side scripting . BASS allows programmers to work in an ideal world , using new abstractions to tackle common but problematic aspects of web programming . The meta properties of BASS provide useful security guarantees . Second , we present a language MOSS reflecting realistic web programming concepts and scenarios , thus articulating the computation model behind web programming . Finally , we present a translation from BASS to MOSS , demonstrating how the ideal programming model and security guarantees of BASS can be implemented in practice ."
"985","On optimal service selection While many works have been devoted to service matchmaking and modeling nonfunctional properties , the problem of matching service requests to offers in an optimal way has not yet been extensively studied . In this paper we formalize three kinds of optimal service selection problems , based on different criteria . Then we study their complexity and implement solutions . We prove that one-time costs make the optimal selection problem computationally hard ; in the absence of these costs the problem can be solved in polynomial time . We designed and implemented both exact and heuristic ( suboptimal ) algorithms for the hard case , and carried out a preliminary experimental evaluation with interesting results ."
"986","Cataclysm : policing extreme overloads in internet applications In this paper we present the Cataclysm server platform for handling extreme overloads in hosted Internet applications . The primary contribution of our work is to develop a low overhead , highly scalable admission control technique for Internet applications . Cataclysm provides several desirable features , such as guarantees on response time by conducting accurate size-based admission control , revenue maximization at multiple time-scales via preferential admission of important requests and dynamic capacity provisioning , and the ability to be operational even under extreme overloads . Cataclysm can transparently trade-off the accuracy of its decision making with the intensity of the workload allowing it to handle incoming rates of several tens of thousands of requests\/second . We implement a prototype Cataclysm hosting platform on a Linux cluster and demonstrate the benefits of our integrated approach using a variety of workloads ."
"987","Off the beaten tracks : exploring three aspects of web navigation This paper presents results of a long-term client-side Web usage study , updating previous studies that range in age from five to ten years . We focus on three aspects of Web navigation : changes in the distribution of navigation actions , speed of navigation and within-page navigation . `` Navigation actionsö corresponding to users ' individual page requests are discussed by type . We reconfirm links to be the most important navigation element , while backtracking has lost more than half of its previously reported share and form submission has become far more common . Changes of the Web and the browser interfaces are candidates for causing these changes . Analyzing the time users stayed on pages , we confirm Web navigation to be a rapidly interactive activity . A breakdown of page characteristics shows that users often do not take the time to read the available text or consider all links . The performance of the Web is analyzed and reassessed against the resulting requirements . Finally , habits of within-page navigation are presented . Although most selected hyperlinks are located in the top left corner of the = screen , in nearly a quarter of all cases people choose links that require scrolling . We analyzed the available browser real estate to gain insights for the design of non-scrolling Web pages ."
"988","Towards context-aware adaptable web services In this paper , we present a context framework that facilitates the development and deployment of context-aware adaptable Web services . Web services are provided with context information about clients that may be utilized to provide a personalized behavior . Context is extensible with new types of information at any time without any changes to the underlying infrastructure . Context processing is done by Web services , context plugins , or context services . Context plugins and context services pre - and post-process Web service messages based on the available contextinformation . Both are essential for automatic context processing and automatic adaption of Web services to new context types without the necessity to adjust the Web services themselves . We implemented the context framework within the ServiceGlobe system , our open and distributed Web service platform ."
"989","IRLbot : scaling to 6 billion pages and beyond This paper shares our experience in designing a web crawler that can download billions of pages using a single-server implementation and models its performance . We show that with the quadratically increasing complexity of verifying URL uniqueness , BFS crawl order , and fixed per-host rate-limiting , current crawling algorithms can not effectively cope with the sheer volume of URLs generated in large crawls , highly-branching spam , legitimate multi-million-page blog sites , and infinite loops created by server-side scripts . We offer a set of techniques for dealing with these issues and test their performance in an implementation we call IRLbot . In our recent experiment that lasted 41 days , IRLbot running on a single server successfully crawled 6.3 billion valid HTML pages ( $ 7.6 $ billion connection requests ) and sustained an average download rate of 319 mb\/s ( 1,789 pages\/s ) . Unlike our prior experiments with algorithms proposed in related work , this version of IRLbot did not experience any bottlenecks and successfully handled content from over 117 million hosts , parsed out 394 billion links , and discovered a subset of the web graph with 41 billion unique nodes ."
"990","Modeling online reviews with multi-grain topic models In this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews . Extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews ( 18 , 19 , 7 , 12 , 27 , 36 , 21 ) . Our models are based on extensions to standard topic modeling methods such as LDA and PLSA to induce multi-grain topics . We argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects ( e.g. , the brand of a product type ) rather than the aspects of an object that tend to be rated by a user . The models we present not only extract ratable aspects , but also cluster them into coherent topics , e.g. , ` waitress ' and ` bartender ' are part of the same topic ` staff ' for restaurants . This differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering . We evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models ."
"991","Invasive browser sniffing and countermeasures We describe the detrimental effects of browser cache\/history sniffing in the context of phishing attacks , and detail an approach that neutralizes the threat by means of URL personalization ; we report on an implementation performing such personalization on the fly , and analyze the costs of and security properties of our proposed solution ."
"992","Keynote talk Richard Granger will be providing an update on the deployment of information technology at a national scale in the NHS in England . Particular topics that will be covered include variability of performance and user organizations and suppliers . Access\/channel strategies for NHS users and members of the public . Take-up rates for new technologies including internet adoption . Data on number of users and transactions to date will also be provided ."
"993","Forcehttps : protecting high-security web sites from network attacks As wireless networks proliferate , web browsers operate in an increasingly hostile network environment . The HTTPS protocol has the potential to protect web users from network attackers , but real-world deployments must cope with misconfigured servers , causing imperfect web sites and users to compromise browsing sessions inadvertently . ForceHTTPS is a simple browser security mechanism that web sites or users can use to opt in to stricter error processing , improving the security of HTTPS by preventing network attacks that leverage the browser 's lax error processing . By augmenting the browser with a database of custom URL rewrite rules , ForceHTTPS allows sophisticated users to transparently retrofit security onto some insecure sites that support HTTPS . We provide a prototype implementation of ForceHTTPS as a Firefox browser extension ."
"994","Improving recommendation lists through topic diversification In this work we present topic diversification , a novel method designed to balance and diversify personalized recommendation lists in order to reflect the user 's complete spectrum of interests . Though being detrimental to average accuracy , we show that our method improves user satisfaction with recommendation lists , in particular for lists generated using the common item-based collaborative filtering algorithm . Our work builds upon prior research on recommender systems , looking at properties of recommendation lists as entities in their own right rather than specifically focusing on the accuracy of individual recommendations . We introduce the intra-list similarity metric to assess the topical diversity of recommendation lists and the topic diversification approach for decreasing the intra-list similarity . We evaluate our method using book recommendation data , including offline analysis on 361 , ! , 349 ratings and an online study involving more than 2 , ! , 100 subjects ."
"995","Translating XSLT programs to Efficient SQL queries We present an algorithm for translating XSLT programs into SQL . Our context is that of virtual XML publishing , in which a single XML view is defined from a relational database , and subsequently queried with XSLT programs . Each XSLT program is translated into a single SQL query and run entirely in the database engine . Our translation works for a large fragment of XSLT , which we define , that includes descendant\/ancestor axis , recursive templates , modes , parameters , and aggregates . We put considerable effort in generating correct and efficient SQL queries and describe several optimization techniques to achieve this efficiency . We have tested our system on all 22 SQL queries of the TPC-H database benchmark which we represented in XSLT and then translated back to SQL using our translator ."
"996","A quality model for multichannel adaptive information The ongoing diffusion of novel and mobile devices offers new ways to provide services across a growing set of network technologies . As a consequence , traditional information systems evolve to multichannel systems in which services are provided through different channels , being a channel the abstraction of a device and a network . This work proposes a quality model suitable for capturing and reasoning about quality aspects of multichannel information systems . In particular , the model enables a clear separation of modeling aspects of services , networks , and devices . Further , it embeds rules enabling the evaluation of end-to-end quality , which can be used to select services according to the actual quality perceived by users ."
"997","An evaluation of binary xml encoding optimizations for fast stream based xml processing This paper provides an objective evaluation of the performance impacts of binary XML encodings , using a fast stream-based XQuery processor as our representative application . Instead of proposing one binary format and comparing it against standard XML parsers , we investigate the individual effects of several binary encoding techniques that are shared by many proposals . Our goal is to provide a deeper understanding of the performance impacts of binary XML encodings in order to clarify the ongoing and often contentious debate over their merits , particularly in the domain of high performance XML stream processing ."
"998","Service-oriented data denormalization for scalable web applications Many techniques have been proposed to scale web applications . However , the data interdependencies between the database queries and transactions issued by the applications limit their efficiency . We claim that major scalability improvements can be gained by restructuring the web application data into multiple independent data services with exclusive access to their private data store . While this restructuring does not provide performance gains by itself , the implied simplification of each database workload allows a much more efficient use of classical techniques . We illustrate the data denormalization process on three benchmark applications : TPC-W , RUBiS and RUBBoS . We deploy the resulting service-oriented implementation of TPC-W across an 85-node cluster and show that restructuring its data can provide at least an order of magnitude improvement in the maximum sustainable throughput compared to master-slave database replication , while preserving strong consistency and transactional properties ."
"999","Exploring social annotations for information retrieval Social annotation has gained increasing popularity in many Web-based applications , leading to an emerging research area in text analysis and information retrieval . This paper is concerned with developing probabilistic models and computational algorithms for social annotations . We propose a unified framework to combine the modeling of social annotations with the language modeling-based methods for information retrieval . The proposed approach consists of two steps : ( 1 ) discovering topics in the contents and annotations of documents while categorizing the users by domains ; and ( 2 ) enhancing document and query language models by incorporating user domain interests as well as topical background models . In particular , we propose a new general generative model for social annotations , which is then simplified to a computationally tractable hierarchical Bayesian network . Then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models . Experiments are carried out on a real-world annotation data set sampled from del. icio . us . Our results demonstrate significant improvements over the traditional approaches ."
"1000","Summarization of online image collections via implicit feedback The availability of map interfaces and location-aware devices makes a growing amount of unstructured , geo-referenced information available on the Web . In particular , over twelve million geo-referenced photos are now available on Flickr , a popular photo-sharing website . We show a method to analyze the Flickr data and generate aggregate knowledge in the form of `` representative tags '' for arbitrary areas in the world . We display these tags on a map interface in an interactive web application along with images associated with each tag . We then use the implicit feedback of the aggregate user interactions with the tags and images to learn which images best describe the area shown on the map ."
"1001","Shilling recommender systems for fun and profit Recommender systems have emerged in the past several years as an effective way to help people cope with the problem of information overload . One application in which they have become particularly common is in e-commerce , where recommendation of items can often help a customer find what she is interested in and , therefore can help drive sales . Unscrupulous producers in the never-ending quest for market penetration may find it profitable to shill recommender systems by lying to the systems in order to have their products recommended more often than those of their competitors . This paper explores four open questions that may affect the effectiveness of such shilling attacks : which recommender algorithm is being used , whether the application is producing recommendations or predictions , how detectable the attacks are by the operator of the system , and what the properties are of the items being attacked . The questions are explored experimentally on a large data set of movie ratings . Taken together , the results of the paper suggest that new ways must be used to evaluate and detect shilling attacks on recommender systems ."
"1002","Crawling multiple UDDI business registries As Web services proliferate , size and magnitude of UDDI Business Registries ( UBRs ) are likely to increase . The ability to discover Web services of interest then across multiple UBRs becomes a major challenge specially when using primitive search methods provided by existing UDDI APIs . Clients do not have the time to endlessly search accessible UBRs for finding appropriate services particularly when operating via mobile devices . Finding services of interest should be time effective and highly productive . This paper addresses issues relating to the efficient access and discovery of Web services across multiple UBRs and introduces a novel exploration engine , the Web Service Crawler Engine ( WSCE ) . WSCE is capable of crawling multiple UBRs , and enables for the establishment of a centralized Web services repository that can be used for discovering Web services much more efficiently . The paper presents experimental validation , results , and analysis of the proposed ideas ."
"1003","OREL : an ontology-based rights expression language This paper proposes an Ontology-based Rights Expression Language , called OREL . Based on OWL Web Ontology Language , OREL allows not only users but also machines to handle digital rights at semantics level . The ontology-based rights model of OREL is also presented . The usage of OREL and its advantages against existing RELs are discussed ."
"1004","A comparison of case-based reasoning approaches Over the years software engineering researchers have suggested numerous techniques for estimating development effort . These techniques have been classified mainly as algorithmic , machine learning and expert judgement . Several studies have compared the prediction accuracy of those techniques , with emphasis placed on linear regression , stepwise regression , and Case-based Reasoning ( CBR ) . To date no converging results have been obtained and we believe they may be influenced by the use of the same CBR configuration . The objective of this paper is twofold . First , to describe the application of case-based reasoning for estimating the effort for developing Web hypermedia applications . Second , comparing the prediction accuracy of different CBR configurations , using two Web hypermedia datasets . Results show that for both datasets the best estimations were obtained with weighted Euclidean distance , using either one analogy ( dataset 1 ) or 3 analogies ( dataset 2 ) . We suggest therefore that case-based reasoning is a candidate technique for effort estimation and , with the aid of an automated environment , can be applied to Web hypermedia development effort prediction ."
"1005","Flickr tag recommendation based on collective knowledge Online photo services such as Flickr and Zooomr allow users to share their photos with family , friends , and the online community at large . An important facet of these services is that users manually annotate their photos using so called tags , which describe the contents of the photo or provide additional contextual and semantical information . In this paper we investigate how we can assist users in the tagging phase . The contribution of our research is twofold . We analyze a representative snapshot of Flickr and present the results by means of a tag characterisation focussing on how users tags photos and what information is contained in the tagging . Based on this analysis , we present and evaluate tag recommendation strategies to support the user in the photo annotation task by recommending a set of tags that can be added to the photo . The results of the empirical evaluation show that we can effectively recommend relevant tags for a variety of photos with different levels of exhaustiveness of original tagging ."
"1006","Networked graphs : a declarative mechanism for SPARQL rules , SPARQL views and RDF data integration on the web Easy reuse and integration of declaratively described information in a distributed setting is one of the main motivations for building the Semantic Web . Despite of this claim , reuse and recombination of RDF data today is mostly done using data replication and procedural code . A simple declarative mechanism for reusing and combining RDF data would help users to generate content for the semantic web . Having such a mechanism , the Semantic Web could better benefit from user generated content , as it is broadly present in the so called Web 2.0 , but also from better linkage of existing content . We propose Networked Graphs , which allow users to define RDF graphs both , by extensionally listing content , but also by using views on other graphs . These views can be used to include parts of other graphs , to transform data before including it and to denote rules . The relationships between graphs are described declaratively using SPARQL queries and an extension of the SPARQL semantics . Networked Graphs are easily exchangeable between and interpretable on different computers . Using existing protocols , Networked Graphss can be evaluated in a distributed setting ."
"1007","Video suggestion and discovery for youtube : taking random walks through the view graph The rapid growth of the number of videos in YouTube provides enormous potential for users to find content of interest to them . Unfortunately , given the difficulty of searching videos , the size of the video repository also makes the discovery of new content a daunting task . In this paper , we present a novel method based upon the analysis of the entire user-video graph to provide personalized video suggestions for users . The resulting algorithm , termed Adsorption , provides a simple method to efficiently propagate preference information through a variety of graphs . We extensively test the results of the recommendations on a three month snapshot of live data from YouTube ."
"1008","Newsjunkie : providing personalized newsfeeds via analysis of information novelty We present a principled methodology for filtering news stories by formal measures of information novelty , and show how the techniques can be usedto custom-tailor news feeds based on information that a user has already reviewed . We review methods for analyzing novelty and then describe Newsjunkie , a system that personalizes news for users by identifying the novelty of stories in the context of stories they have already reviewed . Newsjunkie employs novelty-analysis algorithms that represent articles as words and named entities . The algorithms analyze inter-andintra-document dynamics by considering how information evolves over timefrom article to article , as well as within individual articles . We review the results of a user study undertaken to gauge the value of the approachover legacy time-based review of newsfeeds , and also to compare the performance of alternate distance metrics that are used to estimate the dissimilarity between candidate new articles and sets of previously reviewed articles ."
"1009","Anycast CDNS revisited Because it is an integral part of the Internet routing apparatus , and because it allows multiple instances of the same service to be `` naturally '' discovered , IP Anycast has many attractive features for any service that involve the replication of multiple instances across the Internet . While briefly considered as an enabler when content distribution networks ( CDNs ) first emerged , the use of IP Anycast was deemed infeasible in that environment . The main reasons for this decision were the lack of load awareness of IP Anycast and unwanted side effects of Internet routing changes on the IP Anycast mechanism . Prompted by recent developments in route control technology , as well as a better understanding of the behavior of IP Anycast in operational settings , we revisit this decision and propose a load-aware IP Anycast CDN architecture that addresses these concerns while benefiting from inherent IP Anycast features . Our architecture makes use of route control mechanisms to take server and network load into account to realize load-aware Anycast . We show that the resulting redirection requirements can be formulated as a Generalized Assignment Problem and present practical algorithms that address these requirements while at the same time limiting session disruptions that plague regular IP Anycast . We evaluate our algorithms through trace based simulation using traces obtained from an operation CDN network ."
"1010","Towards extracting flickr tag semantics We address the problem of extracting semantics of tags -- short , unstructured text-labels assigned to resources on the Web -- based on each tag 's metadata patterns . In particular , we describe an approach for extracting place and event semantics for tags that are assigned to photos on Flickr , a popular photo sharing website supporting time and location ( latitude\/longitude ) metadata . The approach can be generalized to other domains where text terms can be extracted and associated with metadata patterns , such as geo-annotated web pages ."
"1011","Non-intrusive monitoring and service adaptation for WS-BPEL Web service processes currently lack monitoring and dynamic ( runtime ) adaptation mechanisms . In highly dynamic processes , services frequently need to be exchanged due to a variety of reasons . In this paper we present VieDAME , a system which allows monitoring of BPEL processes according to Quality of Service ( QoS ) attributes and replacement of existing partner services based on various ( pluggable ) replacement strategies . The chosen replacement services can be syntactically or semantically equivalent to the BPEL interface . Services can be automatically replaced during runtime without any downtime of the overall system . We implemented our solution with an aspect-oriented approach by intercepting SOAP messages and allow services to be exchanged during runtime with little performance penalty costs , as shown in our experiments , thereby making our approach suitable for high-availability BPEL environments ."
"1012","Copyright protection on the web : a hybrid digital video watermarking scheme Video is one of the most popular data shared in the Web , and the protection of video copyright is of vast interest . In this paper , we present a comprehensive approach for protecting and managing video copyrights in the Internet with watermarking techniques . We propose a novel hybrid digital video watermarking scheme with scrambled watermarks and error correction codes . The effectiveness of this scheme is verified through a series of experiments , and the robustness of our approach is demonstrated using the criteria of the latest StirMark test ."
"1013","ATMEN : a triggered network measurement infrastructure Web performance measurements and availability tests have been carried out using a variety of infrastructures over the last several years . Disruptions in the Internet can lead to Web sites being unavailable or increase user-perceived latency . The unavailability could be due to DNS , failures in segments of the physical network cutting off thousands of users , or attacks . Prompt reactions to network-wide events can be facilitated by local or remote measurement and monitoring . Better yet , a distributed set of intercommunicating measurement and monitoring entities that react to events dynamically could go a long way to handle disruptions . We have designed and built ATMEN , a triggered measurement infrastructure to communicate and coordinate across various administrative entities . ATMEN nodes can trigger new measurements , query ongoing passive measurements or historical measurements stored on remote nodes , and coordinate the responses to make local decisions . ATMEN reduces wasted measurements by judiciously reusing measurements along three axes : spatial , temporal , and application . We describe the use of ATMEN for key Web applications such as performance based ranking of popular Web sites and availability of DNS servers on which most Web transactions are dependent . The evaluation of ATMEN is done using multiple network monitoring entities called Gigascopes installed across the USA , measurement data of a popular network application involving millions of users distributed across the Internet , and scores of clients to aid in gathering measurement information upon demand . Our results show that such a system can be built in a scalable fashion ."
"1014","A web services architecture for learning object discovery and assembly Courseware systems are often based on an assembly of different components , addressing the different needs of storage and delivery functionality . The Learning Technology Standard Architecture LTSA provides a generic architectural framework for these systems . Recent developments in Web technology -- e.g. the Web services framework -- have greatly enhanced the flexible and interoperable implementation of courseware architectures . We argue that in order to make the Web services philosophy work , two enhancements to the LTSA approach are required . Firstly , a combination with metadata annotation is needed to support the discovery of educational Web services . Secondly , if these components are to be provided in form of services , more support is needed for their assembly . Architectural patterns of a finer degree of granularity shall satisfy this need ."
"1015","Session level techniques for improving web browsing performance on wireless links Recent observations through experiments that we have performed incurrent third generation wireless networks have revealed that the achieved throughput over wireless links varies widely depending on the application . In particular , the throughput achieved by file transfer application ( FTP ) and web browsing application ( HTTP ) are quite different . The throughput achieved over a HTTP session is much lower than that achieved over an FTP session . The reason for the lower HTTP throughput is that the HTTP protocol is affected by the large Round-Trip Time ( RTT ) across Wireless links . HTTP transfers require multiple TCP connections and DNS lookups before a HTTP page can be displayed . Each TCP connection requires several RTTs to fully open the TCP send window and each DNS lookup requires several RTTs before resolving the domain name to IP mapping . These TCP\/DNS RTTs significantly degrade the performance of HTTP over wireless links . To overcome these problems , we have developed session level optimization techniques to enhance HTTP download mechanisms . These techniques ( a ) minimize the number of DNS lookups over the wireless link and ( b ) minimize the number of TCP connections opened by the browser . These optimizations bridge the mismatch caused by wireless links between application-level protocols ( such as HTTP ) and transport-level protocols ( such asTCP ) . Our solutions do not require any client-side software and can be deployed transparently on a service provider network toprovide 30-50 % decrease in end-to-end user perceived latency and 50-100 % increase in data throughput across wireless links for HTTP sessions ."
"1016","Partitioning of Web graphs by community topology We introduce a stricter Web community definition to overcome boundary ambiguity of a Web community defined by Flake , Lawrence and Giles ( 2 ) , and consider the problem of finding communities that satisfy our definition . We discuss how to find such communities and hardness of this problem . We also propose Web page partitioning by equivalence relation defined using the class of communities of our definition . Though the problem of efficiently finding all communities of our definition is NP-complete , we propose an efficient method of finding a subclass of communities among the sets partitioned by each of n-1 cuts represented by a Gomory-Hu tree ( 10 ) , and partitioning a Web graph by equivalence relation defined using the subclass . According to our preliminary experiments , partitioning by our method divided the pages retrieved by keyword search into several different categories to some extent ."
"1017","Web data integration using approximate string join Web data integration is an important preprocessing step for web mining . It is highly likely that several records on the web whose textual representations differ may represent the same real world entity . These records are called approximate duplicates . Data integration seeks to identify such approximate duplicates and merge them into integrated records . Many existing data integration algorithms make use of approximate string join , which seeks to ( approximately ) find all pairs of strings whose distances are less than a certain threshold . In this paper , we propose a new mapping method to detect pairs of strings with similarity above a certain threshold . In our method , each string is first mapped to a point in a high dimensional grid space , then pairs of points whose distances are 1 are identified . We implement it using Oracle SQL and PL\/SQL . Finally , we evaluate this method using real data sets . Experimental results suggest that our method is both accurate and efficient ."
"1018","Matching web site structure and content To keep an overview of a complex corporate web sites , it is crucial to understand the relationship of contents , structure and the user 's behavior . In this paper , we describe an approach which is allowing us to compare web page content with the information implictly defined by the structure of the web site . We start by describing each web page with a set of key words . We combine this information with the link structure in an algorithm generating a context based description . By comparing both descriptions , we draw conclusions about the semantic relationship of a web page and its neighborhood . In this way , we indicate whether a page fits in the content of its neighborhood . Doing this , we implicitly identify topics which span over several connected web pages . With our approach we support redesign processes by assessing the actual structure and content of a web site with designer 's concepts ."
"1019","CiteSeerx : an architecture and web service design for an academic document search engine CiteSeer is a scientific literature digital library and search engine which automatically crawls and indexes scientific documents in the field of computer and information science . After serving as a public search engine for nearly ten years , CiteSeer is starting to have scaling problems for handling of more documents , adding new feature and more users . Its monolithic architecture design prevents it from effectively making use of new web technologies and providing new services . After analyzing the current system problems , we propose a new architecture and data model , CiteSeerx . CiteSeerx that will overcome the existing problems as well as provide scalability and better performance plus new services and system features ."
"1020","Generative models for name disambiguation Name ambiguity is a special case of identity uncertainty where one person can be referenced by multiple name variations in different situations or evenshare the same name with other people . In this paper , we present an efficient framework by using two novel topic-based models , extended from Probabilistic Latent Semantic Analysis ( PLSA ) and Latent Dirichlet Allocation ( LDA ) . Our models explicitly introduce a new variable for persons and learn the distribution of topics with regard to persons and words . Experiments indicate that our approach consistently outperforms other unsupervised methods including spectral and DBSCAN clustering . Scalability is addressed by disambiguating authors in over 750,000 papers from the entire CiteSeer dataset ."
"1021","XJ : integration of XML processing into java The increased importance of XML as a universal data representation format has led to several proposals for enabling the development of applications that operate on XML data . These proposals range from runtime API-based interfaces to XML-based programming languages . The subject of this paper is XJ , a research language that proposes novel mechanisms for the integration of XML as a first-class construct into JavaTM . The design goals of XJ distinguish it from pastwork on integrating XML support into programming languages -- specifically , the XJ design adheres to the XML Schema and XPathstandards , and supports in-place updates of XML data thereby keeping with the imperative nature of Java . We have also built a prototype compiler for XJ , and our preliminary experimental results demonstrate that the performance of XJ programs can approach that of tradition allow level API-based interfaces , while providing a higher level of abstraction ."
"1022","Detecting spam web pages through content analysis In this paper , we continue our investigations of `` web spam '' : the injection of artificially-created pages into the web in order to influence the results from search engines , to drive traffic to certain pages for fun or profit . This paper considers some previously-undescribed techniques for automatically detecting spam pages , examines the effectiveness of these techniques in isolation and when aggregated using classification algorithms . When combined , our heuristics correctly identify 2,037 ( 86.2 % ) of the 2,364 spam pages ( 13.8 % ) in our judged collection of 17,168 pages , while misidentifying 526 spam and non-spam pages ( 3.1 % ) ."
"1023","Topical TrustRank : using topicality to combat web spam Web spam is behavior that attempts to deceive search engine ranking algorithms . TrustRank is a recent algorithm that can combat web spam . However , TrustRank is vulnerable in the sense that the seed set used by TrustRank may not be sufficiently representative to cover well the different topics on the Web . Also , for a given seed set , TrustRank has a bias towards larger communities . We propose the use of topical information to partition the seed set and calculate trust scores for each topic separately to address the above issues . A combination of these trust scores for a page is used to determine its ranking . Experimental results on two large datasets show that our Topical TrustRank has a better performance than TrustRank in demoting spam sites or pages . Compared to TrustRank , our best technique can decrease spam from the top ranked sites by as much as 43.1 % ."
"1024","Deriving knowledge from figures for digital libraries Figures in digital documents contain important information . Current digital libraries do not summarize and index information available within figures for document retrieval . We present our system on automatic categorization of figures and extraction of data from 2-D plots . A machine-learning based method is used to categorize figures into a set of predefined types based on image features . An automated algorithm is designed to extract data values from solid line curves in 2-D plots . The semantic type of figures and extracted data values from 2-D plots can be integrated with textual information within documents to provide more effective document retrieval services for digital library users . Experimental evaluation has demonstrated that our system can produce results suitable for real-world use ."
"1025","An efficient and systematic method to generate xslt stylesheets for different wireless pervasive devices It is a tedious and cumbersome process to update directly a WML document for the wireless Web because its content composes of both data and presentation . Thus , XML is used to handle the data while its XSLT stylesheet is used to extract and format the data for presentation . However , different stylesheets have to be used for different devices . An efficient and systematic method based on the idea of generating two separate sets of rules corresponding to content extracting and formatting parts of the stylesheet is described in this paper . The data extraction part is constructed from content rules while the formatting part is constructed from presentation rules . They are then combined together to form a stylesheet by an XSLT generator . A large number of stylesheets corresponding to different devices and a number of standard DTD documents or XML schemas can be generated in this way and stored in the pool during application setup stage . They will be individually selected from the pool by an XSLT engine to produce different WML documents for different devices during run time ."
"1026","Towards content trust of web resources Trust is an integral part of the Semantic Web architecture . While most prior work focuses on entity-centered issues such as authentication and reputation , it does not model the content , i.e. the nature and use of the information being exchanged . This paper discusses content trust as an aggregate of other trust measures that have been previously studied . The paper introduces several factors that users consider in deciding whether to trust the content provided by a Web resource . Many of these factors are hard to capture in practice , since they would require a large amount of user input . Our goal is to discern which of these factors could be captured in practice with minimal user interaction in order to maximize the system 's trust estimates . The paper also describes a simulation environment that we have designed to study alternative models of content trust ."
"1027","A clustering method for web data with multi-type interrelated components Traditional clustering algorithms work on `` flat '' data , making the assumption that the data instances can only be represented by a set of homogeneous and uniform features . Many real world data , however , is heterogeneous in nature , comprising of multiple types of interrelated components . We present a clustering algorithm , K-SVMeans , that integrates the well known K-Means clustering with the highly popular Support Vector Machines ( SVM ) in order to utilize the richness of data . Our experimental results on authorship analysis of scientific publications show that K-SVMeans achieves better clustering performance than homogeneous data clustering ."
"1028","A possible simplification of the semantic web architecture In the semantic Web architecture , Web ontology languages arebuilt on top of RDF ( S ) . However , serious difficulties have arisen when trying to layer expressive ontology languages , like OWL , on top of RDF-Schema . Although these problems can be avoided , OWL ( andthe whole semantic Web architecture ) becomes much more complex than it should be . In this paper , a possible simplification of thesemantic Web architecture is suggested , which has several import antadvantages with respect to the layering currently accepted by the W3C Ontology Working Group ."
"1029","A combined approach to checking web ontologies The understanding of Semantic Web documents is built upon ontologies that define concepts and relationships of data . Hence , the correctness of ontologies is vital . Ontology reasoners such as RACER and FaCT have been developed to reason ontologies with a high degree of automation . However , complex ontology-related properties may not be expressible within the current web ontology languages , consequently they may not be checkable by RACER and FaCT . We propose to use the software engineering techniques and tools , i.e. , Z\/EVES and Alloy Analyzer , to complement the ontology tools for checking Semantic Web documents . In this approach , Z\/EVES is first applied to remove trivial syntax and type errors of the ontologies . Next , RACER is used to identify any ontological inconsistencies , whose origins can be traced by Alloy Analyzer . Finally Z\/EVES is used again to express complex ontology-related properties and reveal errors beyond the modeling capabilities of the current web ontology languages . We have successfully applied this approach to checking a set of military plan ontologies ."
"1030","GIO : a semantic web application using the information grid framework It is well understood that the key for successful Semantic Web applications depends on the availability of machine understandable meta-data . We describe the Information Grid , a practical approach to the Semantic Web , and show a prototype implementation . Information grid resources span all the data in the organization and all the metadata required to make it meaningful . The final goal is to let organizations view their assets in a smooth continuum from the Internet to the Intranet , with uniform semantically rich access ."
"1031","Knowledge modeling and its application in life sciences : a tale of two ontologies High throughput glycoproteomics , similar to genomics and proteomics , involves extremely large volumes of distributed , heterogeneous data as a basis for identification and quantification of a structurally diverse collection of biomolecules . The ability to share , compare , query for and most critically correlate datasets using the native biological relationships are some of the challenges being faced by glycobiology researchers . As a solution for these challenges , we are building a semantic structure , using a suite of ontologies , which supports management of data and information at each step of the experimental lifecycle . This framework will enable researchers to leverage the large scale of glycoproteomics data to their benefit . In this paper , we focus on the design of these biological ontology schemas with an emphasis on relationships between biological concepts , on the use of novel approaches to populate these complex ontologies including integrating extremely large datasets ( 500MB ) as part of the instance base and on the evaluation of ontologies using OntoQA ( 38 ) metrics . The application of these ontologies in providing informatics solutions , for high throughput glycoproteomics experimental domain , is also discussed . We present our experience as a use case of developing two ontologies in one domain , to be part of a set of use cases , which are used in the development of an emergent framework for building and deploying biological ontologies ."
"1032","The two cultures : mashing up web 2.0 and the semantic web A common perception is that there are two competing visions for the future evolution of the Web : the Semantic Web and Web 2.0 . A closer look , though , reveals that the core technologies and concerns of these two approaches are complementary and that each field can and must draw from the other 's strengths . We believe that future web applications will retain the Web 2.0 focus on community and usability , while drawing on Semantic Web infrastructure to facilitate mashup-like information sharing . However , there are several open issues that must be addressed before such applications can become commonplace . In this paper , we outline a semantic weblogs scenario that illustrates the potential for combining Web 2.0 and Semantic Web technologies , while highlighting the unresolved issues that impede its realization . Nevertheless , we believe that the scenario can be realized in the short-term . We point to recent progress made in resolving each of the issues as well as future research directions for each of the communities ."
"1033","Using XForms to simplify Web programming The difficulty of developing and deploying commercial web applications increases as the number of technologies they use increases and as the interactions between these technologies become more complex . This paper describes a way to avoid this increasing complexity by re-examining the basic requirements of web applications . Our approach is to first separate client concerns from server concerns , and then to reduce the interaction between client and server to its most elemental : parameter passing . We define a simplified programming model for form-based web applications and we use XForms and a subset of J2EE as enabling technologies . We describe our implementation of an MVC-based application builder for this model , which automatically generates the code needed to marshal input and output data between clients and servers . This marshalling uses type checking and other forms of validation on both clients and servers . We also show how our programming model and application builder support the customization of web applications for different execution targets , including , for example , different client devices ."
"1034","A generic uiml vocabulary for device - and modality independent user interfaces We present in this poster our work on a User Interface Markup Language ( UIML ) vocabulary for the specification of device - and modality independent user interfaces . The work presented here is part of an application-oriented project . One of the results of the project is a prototype implementation of a generic platform for device independent multimodal mobile applications . The poster presents the requirements for a generic user interface description format and explains our approach on an integrated description of user interfaces for both graphical and voice modality . A basic overview of the vocabulary structure , its language elements and main features is presented ."
"1035","The webgraph framework I : compression techniques Studying web graphs is often difficult due to their large size . Recently , several proposals have been published about various techniques that allow tostore a web graph in memory in a limited space , exploiting the inner redundancies of the web . The WebGraph framework is a suite of codes , algorithms and tools that aims at making it easy to manipulate large web graphs . This papers presents the compression techniques used in WebGraph , which are centred around referentiation and intervalisation ( which in turn are dual to each other ) . WebGraph can compress the WebBase graph ( 118 Mnodes , 1 Glinks ) in as little as 3.08 bits per link , and its transposed version in as littleas 2.89 bits per link ."
"1036","Flash crowds and denial of service attacks : characterization and implications for CDNs and web sites The paper studies two types of events that often overload Web sites to a point when their services are degraded or disrupted entirely - flash events ( FEs ) and denial of service attacks ( DoS ) . The former are created by legitimate requests and the latter contain malicious requests whose goal is to subvert the normal operation of the site . We study the properties of both types of events with a special attention to characteristics that distinguish the two . Identifying these characteristics allows a formulation of a strategy for Web sites to quickly discard malicious requests . We also show that some content distribution networks ( CDNs ) may not provide the desired level of protection to Web sites against flash events . We therefore propose an enhancement to CDNs that offers better protection and use trace-driven simulations to study the effect of our enhancement on CDNs and Web sites ."
"1037","SecuBat : a web vulnerability scanner As the popularity of the web increases and web applications become tools of everyday use , the role of web security has been gaining importance as well . The last years have shown a significant increase in the number of web-based attacks . For example , there has been extensive press coverage of recent security incidences involving the loss of sensitive credit card information belonging to millions of customers . Many web application security vulnerabilities result from generic input validation problems . Examples of such vulnerabilities are SQL injection and Cross-Site Scripting ( XSS ) . Although the majority of web vulnerabilities are easy to understand and to avoid , many web developers are , unfortunately , not security-aware . As a result , there exist many web sites on the Internet that are vulnerable . This paper demonstrates how easy it is for attackers to automatically discover and exploit application-level vulnerabilities in a large number of web applications . To this end , we developed SecuBat , a generic and modular web vulnerability scanner that , similar to a port scanner , automatically analyzes web sites with the aim of finding exploitable SQL injection and XSS vulnerabilities . Using SecuBat , we were able to find many potentially vulnerable web sites . To verify the accuracy of SecuBat , we picked one hundred interesting web sites from the potential victim list for further analysis and confirmed exploitable flaws in the identified web pages . Among our victims were well-known global companies and a finance ministry . Of course , we notified the administrators of vulnerable sites about potential security problems . More than fifty responded to request additional information or to report that the security hole was closed ."
"1038","Hardening Web browsers against man-in-the-middle and eavesdropping attacks Existing Web browsers handle security errors in a manner that often confuses users . In particular , when a user visits a secure site whose certificate the browser can not verify , the browser typically allows the user to view and install the certificate and connect to the site despite the verification failure . However , few users understand the risk of man-in-the-middle attacks and the principles behind certificate-based authentication . We propose context-sensitive certificate verification ( CSCV ) , whereby the browser interrogates the user about the context in which a certificate verification error occurs . Considering the context , the browser then guides the user in handling and possibly overcoming the security error . We also propose specific password warnings ( SPW ) when users are about to send passwords in a form vulnerable to eavesdropping . We performed user studies to evaluate CSCV and SPW . Our results suggest that CSCV and SPW can greatly improve Web browsing security and are easy to use even without training . Moreover , CSCV had greater impact than did staged security training ."
"1039","Managing versions of web documents in a transaction-time web server This paper presents a transaction-time HTTP server , called TTApache that supports document versioning . A document often consists of a main file formatted in HTML or XML and several included files such as images and stylesheets . A change to any of the files associated with a document creates a new version of that document . To construct a document version history , snapshots of the document 's files are obtained over time . Transaction times are associated with each file version to record the version 's lifetime . The transaction time is the system time of the edit that created the version . Accounting for transaction time is essential to supporting audit queries that delve into past document versions and differential queries that pinpoint differences between two versions . TTApache performs automatic versioning when a document is read thereby removing the burden of versioning from document authors . Since some versions may be created but never read , TTApache distinguishes between known and assumed versions of a document . TTApache has a simple query language to retrieve desired versions . A browser can request a specific version , or the entire history of a document . Queries can also rewrite links and references to point to current or past versions . Over time , the version history of a document continually grows . To free space , some versions can be vacuumed . Vacuuming a version however changes the semantics of requests for that version . This paper presents several policies for vacuuming versions and strategies for accounting for vacuumed versions in queries ."
"1040","WebPod : persistent Web browsing sessions with pocketable storage devices We present WebPod , a portable system that enables mobile users to use the same persistent , personalized web browsing session on any Internet-enabled device . No matter what computer is being used , WebPod provides a consistent browsing session , maintaining all of a user 's plugins , bookmarks , browser web content , open browser windows , and browser configuration options and preferences . This is achieved by leveraging rapid improvements in capacity , cost , and size of portable storage devices . WebPod provides a virtualization and checkpoint\/restart mechanism that decouples the browsing environment from the host , enabling web browsing sessions to be suspended to portable storage , carried around , and resumed from the storage device on another computer . WebPod virtualization also isolates web browsing sessions from the host , protecting the browsing privacy of the user and preventing malicious web content from damaging the host . We have implemented a Linux WebPod prototype and demonstrate its ability to quickly suspend and resume web browsing sessions , enabling a seamless web browsing experience for mobile users as they move among computers ."
"1041","The volume and evolution of web page templates Web pages contain a combination of unique content and template material , which is present across multiple pages and used primarily for formatting , navigation , and branding . We study the nature , evolution , and prevalence of these templates on the web . As part of this work , we develop new randomized algorithms for template extraction that perform approximately twenty times faster than existing approaches with similar quality . Our results show that 40 -- 50 % of the content on the web is template content . Over the last eight years , the fraction of template content has doubled , and the growth shows no sign of abating . Text , links , and total HTML bytes within templates are all growing as a fraction of total content at a rate of between 6 and 8 % per year . We discuss the deleterious implications of this growth for information retrieval and ranking , classification , and link analysis ."
"1042","Type based service composition Service matchmaking and composition has recently drawn increasing attention in the research community . Most existing algorithms construct chains of services based on exact matches of input\/output types . However , this does not work when the available services only cover a part of the range of the input type . We present an algorithm that also allows partial matches and composes them using switches that decide on the required service at runtime based on the actual data type . We report experiments on randomly generated composition problems that show that using partial matches can decrease the failure rate of the integration algorithm using only complete matches by up to 7 times with no increase in the number of directory accesses required . This shows that composition with partial matches is an essential and useful element of web service composition ."
"1043","XL : an XML programming language for web service specification and composition We present an XML programming language specially designed for the implementation of Web services . XL is portable and fully compliant with W3C standards such as XQuery , XML Protocol , and XML Schema . One of the key features of XL is that it allows programmers to concentrate on the logic of their application . XL provides high-level and declarative constructs for actions which are typically carried out in the implementation of a Web service ; e.g. , logging , error handling , retry of actions , workload management , events , etc. . Issues such as performance tuning ( e.g. , caching , horizontal partitioning , etc. ) should be carried out automatically by an implementation of the language . This way , the productivity of the programmers , the ability of evolution of the programs , and the chances to achieve good performance are substantially enhanced ."
"1044","An evaluation of TCP splice benefits in web proxy servers This study is the first to evaluate the performance benefits of using the recently proposed TCP Splice kernel service in Web proxy servers . Previous studies show that splicing client and server TCP connections in the IP layer improves the throughput of proxy servers like firewalls and content routers by reducing the data transfer overheads . In a Web proxy server , data transfer overheads represent a relatively large fraction of the request processing overheads , in particular when content is not cacheable or the proxy cache is memory-based . The study is conducted with a socket-level implementation of TCP Splice . Compared to IP-level implementations , socket-level implementations make possible the splicing of connections with different TCP characteristics , and improve response times by reducing recovery delay after a packet loss . The experimental evaluation is focused on HTTP request types for which the proxy can fully exploit the TCP Splice service , which are the requests for non-cacheabl . content and SSL tunneling . The experimental testbed includes an emulated WAN environment and benchmark applications for HTTP\/1 .0 Web client , Web server , and Web proxy running on AIX RS\/6000 machines . Our experiments demonstrate that TCP Splice enables reductions in CPU utilization of 10-43 % of the CPU , depending on file sizes and request rates . Larger relative reductions are observed when tunneling SSL connections , in particular for small file transfers . Response times are also reduced by up to 1.8 sec ."
"1045","Semantic email This paper investigates how the vision of the Semantic Web can be carried overto the realm of email . We introduce a general notion of semantice mail , in which an email message consists of an RDF query or update coupled with corresponding explanatory text . Semantic email opens the door to a wide range of automated , email-mediated applications with formally guaranteed properties . In particular , this paper introduces a broad class of semantic email processes . For example consider the process of sending an email to a program committee asking who will attend the PC dinner automatically collecting the responses and tallying them up . We define bothlogical and decision-theoretic models where an email process ismodeled as a set of updates to a data set on which we specify goals via certain constraints or utilities . We then describe a set ofinference problems that arise while trying to satisfy these goals and analyze their computational tractability . In particular weshow that for the logical model it is possible to automatically infer which email responses are acceptable w.r.t. a set ofconstraints in polynomial time and for the decision-theoreticmodel it is possible to compute the optimal message-handling policy in polynomial time . Finally we discuss our publicly available implementation of semantic email and outline research challenges inthis realm ."
"1046","Supporting management reporting : a writable web case study The World-Wide Web was originally developed as a shared , writable , hypertext medium , a facility that is still widely needed . We have recently developed a Web-based management reporting system for a legal firm in an attempt to improve the efficiency and management of their overall business process . This paper shares our experiences in relating the firm 's specific writing and issue tracking tasks to existing Web , open hypermedia , and Semantic Web research , and describes why we chose to develop a new solution -- a set of open hypermedia components collectively called the Management Reporting System -- rather than employ an existing system ."
"1047","QoS computation and policing in dynamic web service selection The emerging Service-Oriented Computing ( SOC ) paradigm promises to enable businesses and organizations to collaborate in an unprecedented way by means of standard web services . To support rapid and dynamic composition of services in this paradigm , web services that meet requesters ' functional requirements must be able to be located and bounded dynamically from a large and constantly changing number of service providers based on their Quality of Service ( QoS ) . In order to enable quality-driven web service selection , we need an open , fair , dynamic and secure framework to evaluate the QoS of a vast number of web services . The fair computation and enforcing of QoS of web services should have minimal overhead but yet able to achieve sufficient trust by both service requesters and providers . In this paper , we presented our open , fair and dynamic QoS computation model for web services selection through implementation of and experimentation with a QoS registry in a hypothetical phone service provisioning market place application ."
"1048","Building a companion website in the semantic web A problem facing many textbook authors ( including one of the authors of this paper ) is the inevitable delay between new advances in the subject area and their incorporation in a new ( paper ) edition of the textbook . This means that some textbooks are quickly considered out of date , particularly in active technological areas such as the Web , even though the ideas presented in the textbook are still valid and important to the community . This paper describes our approach to building a companion website for the textbook Hypermedia and the Web : An Engineering Approach . We use Bloom 's taxonomy of educational objectives to critically evaluate a number of authoring and presentation techniques used in existing companion websites , and adapt these techniques to create our own companion website using Semantic Web technologies in order to overcome the identified weaknesses . Finally , we discuss a potential model of future companion websites , in the context of an e-publishing , e-commerce Semantic Web services scenario ."
"1049","Defeating script injection attacks with browser-enforced embedded policies Web sites that accept and display content such as wiki articles or comments typically filter the content to prevent injected script code from running in browsers that view the site . The diversity of browser rendering algorithms and the desire to allow rich content make filtering quite difficult , however , and attacks such as the Samy and Yamanner worms have exploited filtering weaknesses . This paper proposes a simple alternative mechanism for preventing script injection called Browser-Enforced Embedded Policies ( BEEP ) . The idea is that a web site can embed a policy in its pages that specifies which scripts are allowed to run . The browser , which knows exactly when it will run a script , can enforce this policy perfectly . We have added BEEP support to several browsers , and built tools to simplify adding policies to web applications . We found that supporting BEEP in browsers requires only small and localized modifications , modifying web applications requires minimal effort , and enforcing policies is generally lightweight ."
"1050","A framework for coordinated multi-modal browsing with multiple clients As users acquire or gain access to an increasingly diverse range of web access clients , web applications are adapting their user interfaces to support multiple modalities on multiple client types . User experiences can be enhanced by clients with differing capabilities combining to provide a distributed user interface to applications . Indeed , users will be frustrated if their interaction with applications is limited to one client at a time . This paper discusses the requirements for coordinating web interaction across an aggregation of clients . We present a framework for multi-device browsing that provides both coordinated navigation between web resources and coordinated interaction between variants , or representations , of those resources once instantiated in the clients . The framework protects the application from some of the complexities of client aggregation . We show how a small number of enhancements to the XForms and XML Events vocabularies can facilitate coordination between clients and provide an appropriate level of control to applications . We also describe a novel proxy which consolidates HTTP requests from aggregations of clients and reduces the burden that multi-client browsing places on the application ."
"1051","SEMPL : a semantic portal Semantic Web technology is intended for the retrieval , collection , and analysis of meaningful data with significant automation afforded by machine understandability of data ( 1 ) . As one illustration of semantic web technology in action , we present SEMPL , a semantic web portal for the Large Scale Distributed Information Systems lab ( LSDIS ) at the University of Georgia . SEMPL , which is powered by a state of the art commercial system , Semagix Freedom ( 7 ) , uses an ontology-driven approach to provide semantic browsing , linking , and contextual querying of content within the portal . By using the ontology based information integration technique , SEMPL can specify the context of a particular piece of research information , annotate web pages , and provide links to semantically related areas enabling a rich contextual retrieval of information ."
"1052","Xspect : bridging open hypermedia and XLink This paper evaluates the XLink format in comparison with other linking formats . The comparison is based on Xspect , an implementation of XLink . Xspect handles transformation between an open hypermedia format ( OHIF ) and XLink , and the paper discusses this isomorphic transformation and generalises it to include another open hypermedia format , FOHM . The Xspect system , based on XSLT and Javascript , provides users with an interface to browse and merge linkbases . Xspect supports navigational hypermedia in the form of links inserted on the fly into Web pages , as well as guided tours presented as SVG . Xspect has two implementations : one server-side and one running on the client . Both implementation provide the user with an interface for the creation of annotations . The main result of the paper is a critique of XLink . XLink is shown to be a format well suited for navigational hypermedia , but lacking in more advanced constructs . More problematic are the issues regarding large-scale use , such as evaluating validity and credibility of linkbases , and ensuring general support for a format as flexible as XLink ."
"1053","Sampling search-engine results We consider the problem of efficiently sampling Web search engine query results . In turn , using a small random sample instead of the full set of results leads to efficient approximate algorithms for several applications , such as : Determining the set of categories in a given taxonomy spanned by the search results ; Finding the range of metadata values associated to the result set in order to enable `` multi-faceted search ; '' Estimating the size of the result set ; Data mining associations to the query terms . We present and analyze an efficient algorithm for obtaining uniform random samples applicable to any search engine based on posting lists and document-at-a-time evaluation . ( To our knowledge , all popular Web search engines , e.g. Google , Inktomi , AltaVista , AllTheWeb , belong to this class . ) Furthermore , our algorithm can be modified to follow the modern object-oriented approach whereby posting lists are viewed as streams equipped with a next method , and the next method for Boolean and other complex queries is built from the next method for primitive terms . In our case we show how to construct a basic next ( p ) method that samples term posting lists with probability p , and show how to construct next ( p ) methods for Boolean operators ( AND , OR , WAND ) from primitive methods . Finally , we test the efficiency and quality of our approach on both synthetic and real-world data ."
"1054","Distributed web retrieval In the ocean of Web data , Web search engines are the primary way to access content . As the data is on the order of petabytes , current search engines are very large centralized systems based on replicated clusters . Web data , however , is always evolving . The number of Web sites continues to grow rapidly ( over 270 millions at the beginning of 2011 ) and there are currently more than 20 billion indexed pages . On the other hand , Internet users are above one billion and hundreds of million of queries are issued each day . In the near future , centralized systems are likely to become less effective against such a data-query load , thus suggesting the need of fully distributed search engines . Such engines need to maintain high quality answers , fast response time , high query throughput , high availability and scalability ; in spite of network latency and scattered data . In this tutorial we present the architecture of current search engines and we explore the main challenges behind the design of all the processes of a distributed Web retrieval system crawling , indexing , and query processing ."
"1055","Web page summarization using dynamic content Summarizing web pages have recently gained much attention from researchers . Until now two main types of approaches have been proposed for this task : content - and context-based methods . Both of them assume fixed content and characteristics of web documents without considering their dynamic nature . However the volatility of information published on the Internet argue for the implementation of more time-aware techniques . This paper proposes a new approach towards automatic web page description , which extends the concept of a web page by the temporal dimension . Our method provides a broader view on web document summarization and can complement the existing techniques ."
"1056","Event synchronization for interactive cyberdrama generation on the web : a distributed approach The digital generation of a story in which users have influence over the narrative is emerging as an exciting example of computer-based interactive entertainment . Interactive storytelling has existed in non digital versions for thousand of years , but with the advent of the Web the demand for enabling distributed cyberdrama generation is becoming increasingly common . To govern the complexity stemming from the distributed generation of complex plots , we have devised an event synchronization service that may be exploited to support the distribution of interactive storytelling activities over the Web . The main novelty of our approach is that the semantics of the cyberdrama is exploited to discard obsolete events . This brings to the positive result of speeding up the activity of drama generation , thus enabling an augmented interactivity among dispersed players ."
"1057","Network arts : exposing cultural reality In this article , we explore a new role for the computer in art as a reflector of popular culture . Moving away from the static audio-visual installations of other artistic endeavors and from the traditional role of the machine as a computational tool , we fuse art and the Internet to expose cultural connections people draw implicitly but rarely consider directly . We describe several art installations that use the World Wide Web as a reflection of cultural reality to highlight and explore the relations between ideas that compose the fabric of our every day lives ."
"1058","Huskysim : a simulation toolkit for application scheduling in computational grids Grid computing -- the assemblage of heterogeneous distributed clusters of computers viewed as a single virtual machine -- promises to serve as the next major paradigm in distributed computing . Since Grids are assemblages of ( usually ) autonomous systems ( autonomous clusters , supercomputers , or even single workstations ) scheduling can become a complex affair which must take into consideration not just the requirements ( and scheduling decisions ) made at the point of the job 's origin , but also the scheduling requirements ( and decisions ) made at remote points on the fabric , and in particular scheduling decisions made by a remote autonomous system onto which the local job has been scheduled . The current existing scheduling models range from static , where each of the programs is assigned once to a processor before execution of the program commences , to dynamic , where a program may be reassigned to different processors , or a hybrid approach , which combines characteristics of both techniques ( 1,4,5 ) . To address this issue , we have developed a JAVA based discrete event Grid simulator toolkit called HuskySim . The HuskySim toolkit provides core functionalities ( e.g. , compute objects , network objects , and scheduling objects ) that can be used to simulate a distributed computing environment . Furthermore , it can be used to predict the performance of various classes of Grid scheduling algorithms including : Static scheduling algorithms , Dynamic scheduling , Adaptive Scheduling . In our design , we adopted an object-oriented design , which allows an easy mapping and integration of simulation objects into the simulation program . This approach simplifies the simulation of multitasking , and distributed data processing model . Our model of multitasking processing is based on an interrupt driven mechanism . As shown in Figure 1 , the simulator works by relaying messages between the core engine and the simulation modules through the message handling sub-system . Once the architecture , the load distribution , and the scheduling algorithms are defined , the object registration subsystem sends a NEW OBJECT REQUEST MESSAGE to the object class libraries and builds a skeleton for the requested simulation experiment . Workloads traces can be generated using probabilistic models . The currently supported distributions are : Uniform , Poisson , Exponential , Normal , Erlang , and Power Tailed . It is also possible to use real world load traces . Moreover , we augmented the Simulator with a statistical module . Using the statistical module provided with the HuskySim , the core simulation engine can send messages to perform various type of analysis on the performance data including : variance reduction , regression , time series analysis , clustering , and data mining . In order to quantify the system performance , the simulator provides various performance metrics including : CPU utilization , disk utilization , application turnaround time , latency , make span , host to host bandwidth , jammed bandwidth , and TCP\/IP traffic data . These measurements are handled through the measurement sub-system . Furthermore , the HuskySim can be used to simulate the classes of algorithmic and parametric adaptive Grid schedulers . In which , the scheduling algorithm may not be fixed in advance . Simply , the scheduling algorithm is selected at run time based on the current workload on the Grid fabric in order to operate at near optimal level ."
"1059","Interpreting distributed ontologies Semantic Web is challenged by the URI meaning issues arising from putting ontologies in open and distributed environments . As a try to clarify some of the meaning issues , this paper proposes a new approach to interpreting distributed ontologies , it 's built on the top of local models semantics , and extends it to deal with the URI sharing by harmonizing the local models via agreement on vocabulary provenance . The commitment relationship is presented to allow the URI sharing between ontologies with richer semantics ."
"1060","Distributed location aware web crawling Distributed crawling has shown that it can overcome important limitations of the today 's crawling paradigm . However , the optimal benefits of this approach are usually limited to the sites hosting the crawler . In this work , we propose a location-aware method , called IPMicra , that utilizes an IP address hierarchy , and allows crawling of links in a near optimal location aware manner ."
"1061","Efficient web change monitoring with page digest The Internet and the World Wide Web have enabled a publishing explosion of useful online information , which has produced the unfortunate side effect of information overload : it is increasingly difficult for individuals to keep abreast of fresh information . In this paper we describe an approach for building a system for efficiently monitoring changes to Web documents . This paper has three main contributions . First , we present a coherent framework that captures different characteristics of Web documents . The system uses the Page Digest encoding to provide a comprehensive monitoring system for content , structure , and other interesting properties of Web documents . Second , the Page Digest encoding enables improved performance for individual page monitors through mechanisms such as short-circuit evaluation , linear time algorithms for document and structure similarity , and data size reduction . Finally , we develop a collection of sentinel grouping techniques based on the Page Digest encoding to reduce redundant processing in large-scale monitoring systems by grouping similar monitoring requests together . We examine how effective these techniques are over a wide range of parameters and have seen an order of magnitude speed up over existing Web-based information monitoring systems ."
"1062","Web page ranking using link attributes We present a variant of PageRank , WLRank , that considers different Web page attributes to give more weight to some links . Our evaluation shows that the precision of the answers can improve significantly ."
"1063","Are web pages characterized by color ? When human guess the content of a web page , not only the text on the page but also its appearance is an important factor . However , there have been few studies on the relationshipbetween the content and visual appearance of a web page . We investigating the tendencybetween them , especially web content and color use , we found a tendency to use color for some kinds of content pages . We think this result opens the way to estimating web content using color information ."
"1064","Filtering spam e-mail on a global scale In this paper we analyze a very large junk e-mail corpus which was generated by a hundred thousand volunteer users of the Hotmail e-mail service . We describe how the corpus is being collected , and analyze : the geographic origins of the e-mail who the e-mail is targeting and what the e-mail is selling ."
"1065","Adapting databases and WebDAV protocol The ability of the Web to share data regardless of geographical location raises a new issue called remote authoring . With the Internet and Web browsers being independent of hardware , it becomes possible to build Web-enabled database applications . Many approaches are provided to integrate databases into the Web environment , which use the Web 's protocol i.e. HTTP to transfer the data between clients and servers . However , those methods are affected by the HTTP shortfalls with regard to remote authoring . This paper introduces and discusses a new methodology for remote authoring of databases , which is based on the WebDAV protocol . It is a seamless and effective methodology for accessing and authoring databases , particularly in that it naturally benefits from the WebDAV advantages such as metadata and access control . These features establish a standard way of accessing database metadata , and increase the database security , while speeding up the database connection ."
"1066","Efficient pagerank approximation via graph aggregation We present a framework for approximating random-walk based probability distributions over Web pages using graph aggregation . We ( 1 ) partition the Web 's graph into classes of quasi-equivalent vertices , ( 2 ) project the page-based random walk to be approximated onto those classes , and ( 3 ) compute the stationary probability distribution of the resulting class-based random walk . From this distribution we can quickly reconstruct a distribution on pages . Inparticular , our framework can approximate the well-known PageRank distribution by setting the classes according to the set of pages on each Web host . We experimented on a Web-graph containing over 1.4 billion pages , and were able to produce a ranking that has Spearman rank-order correlation of 0.95 with respect to PageRank . A simplistic implementation of our method required less than half the running time of a highly optimized implementation of PageRank , implying that larger speedup factors are probably possible ."
"1067","Graph-based text database for knowledge discovery While we expect to discover knowledge in the texts available on the Web , such discovery usually requires many complex analysis steps , most of which require different text handling operations such as similar text search or text clustering . Drawing an analogy from the relational data model , we propose a text representation model that simplifies the steps . The model represents texts in a formal manner , Subject Graphs , described herein , provides text handling operations whose inputs and outputs are identical in form , i.e. a set of subject graphs . We develop a graph-based text database , which is based on the model , and an interactive knowledge discovery system . Trials of the system show that it allows the user to interactively and intuitively discover knowledge in Web pages by combining text handling operations defined on subject graphs in various orders ."
"1068","A diagrammatic inference system for the web We developed a diagrammatic inference system for the World Wide Web . Our system enables the creation of diagrams such that the information contained in them can be searched and inference can be performed on it . We developed an XMLSchema for bar , line , and pie charts . Based on it , we developed software that transforms a corresponding XML file into an SVG image , which in turn is rendered by the client as an image . Additionally , we developed a search engine which enables a user to find information explicitly contained in the XML file , and as such in the image . Furthermore , we developed an inference engine which enables a user to locate information that is implicitly contained in the image ."
"1069","An agent system reasoning about the web and the user The paper describes some innovations related to the ongoing work on the GSA prototype , an integrated information retrieval agent . In order to improve the original system effectiveness , we propose the GSA2 system , introducing a new internal architecture based on a message-passing framework and on an ontology description formalism ( WOLF , Web ontology Framework ) . GSA2 is conceived in order to describe and easily perform reasoning on `` facts about the web and the user '' . The most innovative aspect of the project is its customizable and flexible reasoning system , based on Answer Set Programming it plays the role of the central decision making module , and allows the Agent to take proactive decisions . The introduction of a logic language allows one to describe , program and plan behaviors of the Agent easily and quickly , and to experiment with a large variety of Information Retrieval strategies . Both the System Architecture and WOLF are general and reusable , and the result constitutes a good example of real implementation of agents based on logics ."
"1070","SLA based profit optimization in web systems With the rapid growth of eBusiness , the Web services are becoming a commodity . To reduce the management cost for the IT infrastructure , companies often outsource their IT services to third party service providers . Large service centers have been setup to provide services to many customers by sharing the IT resources . This leads to the efficient use of resources and a reduction of the operating cost . The service provider and their customers often negotiate utility based Service Level Agreements ( SLAs ) to determine the cost and penalty based on the achieved performance level . The system is based on a centralized controller which can control the request volumes at various servers and the scheduling policy at each server . The controller can also decide to turn ON or OFF servers depending on the system load . This paper designs a resource allocation scheduler for such web environments so as to maximize the profits associated with multiple class SLAs ."
"1071","Online feedback by tests and reporting for elearning and certification programs The evaluation of eLearning success is an indispensable business requirement of education programs : the easy registration of ` visits ' to eLearning websites is , however , not sufficient in most cases . Additional metrics from authenticated logins and reports of learning activity and success - as obtained from specific online tests ' are required . The aim is to document the acceptance , progress and return of investment ( ROI ) of eLearning programs , and set up additional training well tailored to the needs of a specific learning community . An example from a corporate certification program proves the applicability of the proposed processes ."
"1072","Hearsay : enabling audio browsing on hypertext content In this paper we present HearSay , a system for browsing hypertext Web documents via audio . The HearSay system is based on our novel approach to automatically creating audio browsable content from hypertext Web documents . It combines two key technologies : ( 1 ) automatic partitioning of Web documents through tightly coupled structural and semantic analysis , which transforms raw HTML documents into semantic structures so as to facilitate audio browsing ; and ( 2 ) VoiceXML , an already standardized technology which we adopt to represent voice dialogs automatically created from the XML output of partitioning . This paper describes the software components of HearSay and presents an initial system evaluation ."
"1073","A scheme of service discovery and control on ubiquitous devices We have developed a set of hardware and software components to realize ubiquitous computing environments , based on two keywords , simple '' ( easy to implement ) and `` open '' ( adopt widely publicized specifications ) . Then this set has been resulted into UBKit ( Ubiquity Building Toolkit ) . The Micro-Server an instance of UBKitenables existing consumer electronics to join in computer networks . In this paperwe propose a scheme for discovery and control of deviecs attached to micro-servers . ''"
"1074","HPG : a tool for presentation generation in WIS Web Information Systems ( WIS ) support the process of retrieving information from sources on the Web and of presenting them as a hypermedia presentation . Most WIS design methodologies focus on the engineering of the abstract navigation ( hyperlinks ) . The actual presentation generation is less supported . Hera is one of the few WIS methodologies that offer a tool for presentation generation ( HPG ) . The HPG transforms RDF data obtained as the result of a query into a Web presentation suited to the user ( in HTML or WML ) ."
"1075","Scheduling web requests in broadcast environments On-demand broadcast has been supported in the Internet to enhance system scalability . Unfortunately , most of existing on-demand scheduling algorithms did not consider the time constraints associated with web requests . This paper proposes a novel scheduling algorithm , called Slack Inverse Number of requests ( SIN ) , that takes into account the urgency and productivity of serving pending requests . Trace-driven experiments demonstrate that SIN significantly out performs existing algorithms over a wide range of workloads ."
"1076","Metadata co-development : a process resulting in metadata about technical assistance to educators Metadata development can be challenging because the vocabulary should be flexible and extensible , widely applicable , interoperable , and both machine and human readable . We describe how we engaged members of organizations in the field of technical assistance to educators in a process of metadata development , and the challenges we faced . The result was a an ontology for the communities of practice that is interoperable and can evolve it was then used to catalogue resources for dissemination via the Semantic Web ."
"1077","Gossip based streaming In this paper , we propose a novel multicast streaming protocol foroverlay networks , called Gossip Based Streaming ( GBS ) . In GBS , streaming contents are not come from a single upstream source , but delivered from several sources to a client . Though being similarto existing gossip protocols , the unique requirements forstreaming , such as continuous playback , are addressed in our design . Preliminary results show that GBS performs much better indynamic user environments ."
"1078","On the temporal dimension of search Web search is probably the single most important application on the Internet . The most famous search techniques are perhaps the PageRank and HITS algorithms . These algorithms are motivated by the observation that a hyperlink from a page to another is an implicit conveyance of authority to the target page . They exploit this social phenomenon to identify quality pages , e.g. , `` authority '' pages and `` hub '' pages . In this paper we argue that these algorithms miss an important dimension of the Web , the temporal dimension . The Web is not a static environment . It changes constantly . Quality pages in the past may not be quality pages now or in the future . These techniques favor older pages because these pages have many in-links accumulated over time . New pages , which may be of high quality , have few or no in-links and are left behind . Bringing new and quality pages to users is important because most users want the latest information . Research publication search has exactly the same problem . This paper studies the temporal dimension of search in the context of research publication search . We propose a number of methods deal with the problem . Our experimental results show that these methods are highly effective ."
"1079","A hybrid approach for searching in the semantic web This paper presents a search architecture that combines classical search techniques with spread activation techniques applied to a semantic model of a given domain . Given an ontology , weights are assigned to links based on certain properties of the ontology , so that they measure the strength of the relation . Spread activation techniques are used to find related concepts in the ontology given an initial set of concepts and corresponding initial activation values . These initial values are obtained from the results of classical search applied to the data associated with the concepts in the ontology . Two test cases were implemented , with very positive results . It was also observed that the proposed hybrid spread activation , combining the symbolic and the sub-symbolic approaches , achieved better results when compared to each of the approaches alone ."
"1080","An application server for the semantic web The Semantic Web relies on the complex interaction of several technologies involving ontologies . Therefore , sophisticated Semantic Web applications typically comprise more than one software module . Instead of coming up with proprietary solutions , developers should be able to rely on a generic infrastructure for application development in this context . We call such an infrastructure Application Server for the Semantic Web whose design and development are based on existing Application Servers . However , we apply and augment their underlying concepts for use in the Semantic Web and integrate semantic technology within the server itself . We provide a short overview of requirements and design issues of such a server and present our implementation and ongoing work KAON SERVER ."
"1081","Understanding user goals in web search Previous work on understanding user web search behavior has focused on how people search and what they are searching for , but not why they are searching . In this paper , we describe a framework for understanding the underlying goals of user searches , and our experience in using the framework to manually classify queries from a web search engine . Our analysis suggests that so-called navigational '' searches are less prevalent than generally believed while a previously unexplored `` resource-seeking '' goal may account for a large fraction of web searches . We also illustrate how this knowledge of user search goals might be used to improve future web search engines ."
"1082","The role of standards in creating community Participation in the web of communities requires a common language , a common technological structure and development of content that is relevant and captivating . This paper reports on a project that both conserves a rich regional cultural heritage and has structured the content developed during this conservation to be fluidly shared with both the domain and the broader communities . It also examines the varied degrees of acceptance within these communities ."
"1083","Flexible on-device service object replication with replets An increasingly large amount of Web applications employ service objects such as Servlets to generate dynamic and personalized content . Existing caching infrastructures are not well suited for caching such content in mobile environments because of disconnection and weak connection . One possible approach to this problem is to replicate Web-related application logic to client devices . The challenges to this approach are to deal with client devices that exhibit huge divergence in resource availabilities , to support applications that have different data sharing and coherency requirements , and to accommodate the same application under different deployment environments . The Replet system targets these challenges . It uses client , server and application capability and preference information ( CPI ) to direct the replication of service objects to client devices : from the selection of a device for replication and populating the device with client-specific data , to choosing an appropriate replica to serve a given request and maintaining the desired state consistency among replicas . The Replet system exploits on-device replication to enable client - , server - and application-specific cost metrics for replica invocation and synchronization . We have implemented a prototype in the context of Servlet-based Web applications . Our experiment and simulation results demonstrate the viability and significant benefits of CPI-driven on-device service object replication ."
"1084","A semantic approach for designing business protocols Business processes involve interactions among autonomous partners . We propose that these interactions be specified modularly as protocols . Protocols can be published , enabling implementors to independently develop components that respect published protocols and yet serve diverse interests . A variety of business protocols would be needed to capture subtle business needs . We propose that the same kinds of conceptual abstractions be developed for protocols as for information models . Specifically , we consider ( 1 ) refinement : a subprotocol may satisfy the requirements of a superprotocol , but support additional properties and ( 2 ) aggregation : a protocol may combine existing protocols . In support of the above , we develop a formal semantics for protocols , an operational characterization of them , and an algebra for protocol composition ."
"1085","Associative sources and agents for zero-input publishing This paper presents an associative agent that allows seamless navigation from one 's own personal space to third-party associative sources , as well as the personal spaces of other users . The agent provides users with access to a dynamically growing list of information sources , all of which follow a common associative sources API that we have defined . The agent also allows users act as sources themselves and take part in peer-to peer knowledge sharing ."
"1086","Testbed for information extraction from deep web Search results generated by searchable databases are served dynamically and far larger than the static documents on the Web . These results pages have been referred to as the Deep Web . We need to extract the target data in results pages to integrate them on different searchable databases . We propose a test bed for information extraction from search results . We chose 100 databases randomly from 114,540 pages with search forms . Therefore , these databases have a good variety . We selected 51 databases which include URLs in a results pageand manually identify target information to be extracted . We also suggest evaluation measures for comparing extraction methods and methods for extending the target data ."
"1087","On a web browsing support system with 3d visualization Existing commercial Web browsers provide various utilities and functions , e.g. , Web bookmarks and a browsing history list . Since the bookmark and history functions only the title and URL of the Web page , users who can not remember the contents of each Web page have difficulty retracing their steps . In this paper , we propose a bookmark system based on a 3D interface . Additionally , our system offers three main functions a 3D browsing history function , a marker function , and a look-ahead loading function . These functions enable users to browse Web pages more effectively ."
"1088","Automatically generating metadata for digital photographs with geographic coordinates Given location information on digital photographs , we can automatically generate an abundance of photo-related metadata using off-the-shelf and web-based data sources . These metadata can serve as additional memory cues and filters when browsing a personal or global collection of photos ."
"1089","Distributed community crawling The massive distribution of the crawling task can lead to inefficient exploration of the same portion of the Web . We propose a technique to guide crawlers exploration based on the notion of Web communities . Thest ability properties of the method can be used as an implicit coordination mechanism to increase the efficiency of the crawling task ."
"1090","Fine-grained , structured configuration management for web projects Researchers in Web engineering have regularly noted that existing Web application development environments provide little support for managing the evolution of Web applications . Key limitations of Web development environments include line-oriented change models that inadequately represent Web document semantics and in ability to model changes to link structure or the set of objects making up the Webapplication . Developers may find it difficult to grasp how theoverall structure of the Web application has changed over time and may respond by using ad hoc solutions that lead to problems of maintain ability , quality and reliability . Web applications are software artifacts , and as such , can benefit from advanced version control and software configuration management ( SCM ) technologies from software engineering . We have modified an integrated development environment to manage the evolution and maintenance of Web applications . The resulting environment is distinguished by itsfine-grained version control framework , fine-grained Web contentchange management , and product versioning configuration management , in which a Web project can be organized at the logical level and itsstructure and components are versioned in a fine-grained manner aswell . This paper describes the motivation for this environment as well as its user interfaces , features , and implementation ."
"1091","Time-based contextualized-news browser ( t-cnb ) We propose a new way of browsing contextualized-news articles . Our prototype browser system is called a Time-based Contextualized-News Browser ( T-CNB ) . The T-CNB concurrently and automatically presents a series of related pages for one news source while browsing the user-specified page . It extracts the past related pages from a user-specified news articles on the web . The related pages outline the progress of user-specified news articles . We call the related pages ` contextual pages ' . Using the T-CNB , a user only needs to specify one news article on the web . The user then automatically receives past related news articles , which provide a wider understanding of the topic . The T-CNB automatically generates and presents contextualized news articles ."
"1092","Constraint SVG We believe it is important for web graphic standards such as SVG to support user interaction and diagrams that can adapt their layout and appearance to their viewing context so as to take into account viewing device charateristics and the viewer 's requirements . Previously we suggested that adding expression-based attributes to SVG and using one-way constraints to evaluate these dynamically would considerably improve SVG 's support for adaptive layout and user interaction . We describe a minimal backward compatible extension to SVG 1.1 , called Constraint SVG ( CSVG ) , that provides such expression-based attributes and its implementation on top of Batik . CSVG also provides another significant extension to SVG 1.1 : it allows the author to define new custom elements using XSLT ."
"1093","Using semantic web approach in augmented audio reality system for museum visitors In this paper , we describe our work in progress on the reasoning module of ec ( h ) o , an augmented audio-reality interface for museum visitors utilizing spatialized soundscapes and a semantic web approach to information . We used ontologies to describe the semantics of sound objects and represent user model . A rule-based system for selecting sound object uses semantic description of objects , visitor 's interaction history and heuristics for continuity of the dialogue between user and the system ."
"1094","Enhancing the SCORM metadata model Nowadays , the leading e-learning platforms are converging towards standardization . This paper presents an extension to the SCORM , today 's most well acclaimed e-learning standard , enabling the modelling of course related entities that surround learning objects and content aggregations , therefore increasing the standard 's modelling scope and allowing for gains in efficiency in knowledge dissemination . A prototype is being implemented and tested on VIANET , an original e-learning platform with extensible support for the SCORM . content aggregations ."
"1095","Web engineering with the visual software circuit board The Visual Software Circuit Board ( VSCB ) platform supports a component based development methodology towards the development of software systems . The circuit board design techniques and methodologies have evolved for electronic device and component engineering for decades . The circuit board approach , now applied for software systems and applications , makes the component based development process easy to visualize and comprehend . This paper describes the VSCB based design methodology with a specific focus on usage of VSCB for web application engineering ."
"1096","Best bets : thousands of queries in search of a client A number of applications require selecting targets for specific contents on the basis of criteria defined by the contents providers rather than selecting documents in response to user queries , as in ordinary information retrieval . We present a class of retrieval systems , called Best Bets , that generalize Information Filtering and encompass a variety of applications including editorial suggestions , promotional campaigns and targeted advertising , such as Google AdWords ™ . We developed techniques for implementing Best Bets systems addressing performance issues for large scale deployment as efficient query search , incremental updates and dynamic ranking ."
"1097","Adaptive query routing in peer web search An unstructured peer network application was proposed to address the query forwarding problem of distributed search engines and scalability limitations of centralized search engines . Here we present novel techniques to improve local adaptive routing , showing they perform significantly better than a simple learning scheme driven by query response interactions among neighbors . We validate prototypes of our peer network application via simulations with 500 model users based on actual Web crawls . We finally compare the quality of the results with those obtained by centralized search engines , suggesting that our application can draw advantages from the context and coverage of the peer collective ."
"1098","A quality framework for web site quality : user satisfaction and quality assurance Web site developers need to use of standards and best practices to ensure that Web sites are functional , accessible and interoperable . However many Web sites fail to achieve such goals . This short paper describes how a Web site quality assessment method ( E-Qual ) might be used in conjunction with a quality assurance framework ( QA Focus ) to provide a rounded view of Web site quality that takes account of end user and developer perspectives ."
"1099","The semantic webscape : a view of the semantic web It has been a few years since the semantic Web was initiated by W3C , but its status has not been quantitatively measured . It is crucial to understand the status at this early stage , for researchers , developers and administrators to gain insight into what will come in this field . The objective of our work is to quantitatively measure and present the status of the semantic Web . We conduct a longitudinal study on the semantic Web pages to track trends in the use of semantic markup languages . This paper presents early results of this study with two historical data sets from October 2003 and October 2004 . Our results show that while it is very early stage of semantic Web adoption , its growth outpaces that of the entire Web for the period . Also , RDF ( Resource Description Framework ) has dominated among semantic markup languages , taking about 98 % of all semantic pages on the Web . It has been used in a variety of metadata annotation applications . This study shows that the most popular application is RSS ( RDF Site Summary ) for syndicating news and blogs , which takes more than 60 % of all semantic Web pages . It also shows that the use of OWL ( Web Ontology Language ) which was recommended by W3C in early 2004 has been increased 900 % for the period ."
"1100","Design and implementation of a feedback controller for slowdown differentiation on internet servers Proportional slowdown differentiation ( PSD ) aims to maintain slowdown ratios between different classes of clients according to their pre-specified differentiation parameters . In this paper , we design a feedback controller to allocate processing rate on Internet servers for PSD . In this approach , the processing rate of a class is adjusted by an integral feedback controller according to the difference between the target slowdown ratio and the achieved one . The initial rate class is estimated based on predicted workload using queueing theory . We implement the feedback controller in an Apache Web server . The experimental results under various environments demonstrate the controller 's effectiveness and robustness ."
"1101","MemoSpace : a visualization tool for web navigation A central aspect of reducing orientation problems in web navigation concerns the design of adequate navigation aids . Visualization of users ' navigation path in form of a temporal-spatial template can function as external memory of users ' search history , thereby supporting the user to find previously visited sites , getting an overview of the search process and moreover , provide structure for the complex WorldWideWeb ( WWW ) environment . This paper presents an application for dynamic 2 and 3 dimensional visualizations of users ' navigation paths , called MemoSpace . In an explorative study , users behavior and subjective evaluation of a MemoSpace application was examined ."
"1102","XSLT by example XQBE ( XQuery By Example , ( 1 ) ) , a visual dialect of XQuery , uses hierarchical structures to express transformations between XML documents . XSLT , the standard transformation language for XML , is increasingly popular among programmers and Web developers for separating the application and presentation layers of Web applications . However , its syntax and its rule-based execution paradigm are rather intricate , and the number of XSLT experts is limited ; the availability of easier `` dialects '' could be extremely valuable and may contribute to the adoption of XML for developing data-centered Web applications and services . With this motivation in mind , we adapted XQBE to serve as a visual interface for expressing XML-to-XML transformations and generate the XSLT code that performs such transformations ."
"1103","Soundness proof of Z semantics of OWL using institutions The correctness of the Z semantics of OWL is the theoretical foundation of using software engineering techniques to verify Web ontologies . As OWL and Z are based on different logical systems , we use institutions to represent their underlying logical systems and use institution morphisms to prove the correctness of the Z semantics for OWL DL ."
"1104","Automatic generation of web portals using artificial ants We present in this work a new model ( named AntTree ) based on artificial ants for document hierarchical clustering . This model is inspired from the self-assembly behavior of real ants . We have simulated this behavior to build a hierarchical tree-structured partitioning of a set of documents , according to the similarities between these documents . We have successfully compared our results to those obtained by ascending hierarchical clustering ."
"1105","Automatic generation of link collections and their visualization In this paper , we describe a method of generating link collections in a user-specified category by comprehensively collecting existing link collections and analyzing their hyperlink references . Moreover , we propose a visualization method for a bird 's - eye view of the generated link collections . Our methods are effective in grasping intuitively the trend of significant sites and keywords in a category ."
"1106","An environment for collaborative content acquisition and editing by coordinated ubiquitous devices Digital content is not only stored by servers on the Internet , but also on various embedded devices belonging to ubiquitous networks . In this paper , we propose a content processing mechanism for use in an environment enabling collaborative acquisition of embedded digital content in real-world situations . We have developed a network management device that makes it possible to acquire embedded content using coordinated ubiquitous devices . The management device actively configures a network that includes content-providing devices and browsing devices to permit sharing of various items with digital content . We also developed a Functional web mechanism for processing embedded web content in the real-world without a keyboard . This mechanism adds various functions to conventional web content . These functions are activated by messages from a Field in a content processing device . We construct a practical prototype system , which is simple enough for children to use , that we called the `` Virtual Insect Catcher '' . Through a test with 48 children , we demonstrated that this system can be used to acquire embedded web content , retrieve related content from the Internet , and then create new web content . We will also describe the proposed mechanism and the system testing ."
"1107","Middleware services for web service compositions WS - \* specifications cover a variety of issues ranging from security and reliability to transaction support in web services . However , these specifications do not address web service compositions . On the other hand , BPEL as the future standard web service composition language allows the specification of the functional part of the composition as a business process but it fails short in expressing non-functional properties such as security , reliability and persistence . In this paper , we propose an approach for the transparent integration of technical concerns in web service compositions . Our approach is driven by the analogy between web services and software components and is inspired from server-side component models such as Enterprise Java Beans . The main components of our framework are the process container , the middleware services and the deployment descriptor ."
"1108","Applying NavOptim to minimize navigational effort A major factor in the effectiveness of the interaction which users have with Web applications is the ease with which they can locate information and functionality which they are seeking . Effective design is however complicated by the multiple design purposes and diverse users which Web applications typically support . In this paper we describe a navigational design method aimed at optimising designs through minimizing navigational entropy . The approach uses a theoretical navigational depth for the various information and service components to moderate a nested hierarchical clustering of the content ."
"1109","Adaptive filtering of advertisements on web pages We present a browser extension to dynamically learn to filter unwanted images ( such as advertisements or flashy graphics ) based on minimal user feedback . To do so , we apply the weighted majority algorithm using pieces of the Uniform Resource Locators of such images as predictors . Experimental results tend to confirm that the accuracy of the predictions converges quickly to very high levels ."
"1110","Can link analysis tell us about web traffic ? In this paper we measure correlation between link analysis characteristics for Web pages such as in - and out-degree , PageRank and RBS with those obtained from real Web traffic analysis . Measurements made on real data from the Polish Web show that PageRank is observably but not strongly correlated with actual visits made by Web users to Web pages and that our RBS algorithm ( 2 ) is more correlated with traffic data than PageRank in some cases ."
"1111","The classroom sentinel : supporting data-driven decision-making in the classroom Whereas schools typically record mounds of data regarding student performance , attendance , and other behaviors over the course of a school year , rarely is that data consulted and used to inform day-to-day instructional practice in the classroom . As teachers come under increasing pressure to ensure success for all of their students , we are attempting to provide tools to help teachers make sense of what is happening in their classrooms and take appropriate proactive and\/or remedial action . One such tool is a Web service we 've dubbed the Classroom Sentinel . The Classroom Sentinel mines electronic gradebook and other student information system data sources to detect critical teaching and learning patterns and bring those patterns to the attention of the teacher in the form of timely alerts . In this paper , we introduce the notion of classroom patterns , present some examples , and describe a framework for alert generation and delivery ."
"1112","Finding the search engine that works for you A search engine evaluation model that considers over seventy performance and feature parameters is presented . The design of a web-based system that allows the user to tailor the model to his\/her own preference , and to evaluate search engines of interest , is introduced . The results presented to the user identify the most suitable search engine that suits his\/her needs ."
"1113","Wireless SOAP : optimizations for mobile wireless web services We propose a set of optimization techniques , collectively called Wireless SOAP ( WSOAP ) , to compress SOAP messages transmitted across a wireless link . The Name Space Equivalency technique rests on the observation that exact recovery of compressed messages is not required at the receiver ; an equivalent form suffices . The WSDL Aware Encoding technique obtains further savings by utilizing knowledge of the underlying WSDL by means of an offline protocol we define . We summarize the design , implementation and performance of our Wireless SOAP prototype , and show that Wireless SOAP can reduce message sizes by 3x-12x compared to SOAP ."
"1114","Processing link structures and linkbases on the web Hyperlinks are an essential feature of the World Wide Web , highly responsible for its success . XLink improves on HTML 's linking capabilities in several ways . In particular , links after XLink can be `` out-of-line '' ( i.e. , not defined at a link source ) and collected in ( possibly several ) linkbases , which considerably ease building complex link structures . Modeling of link structures and processing of linkbases under the Web 's `` open world linking '' are aspects neglected by XLink . Adding a notion of `` interface '' to XLink , as suggested in this work , considerably improves modeling of link structures . When a link structure is traversed , the relevant linkbase ( s ) might become ambiguous . We suggest three linkbase management modes governing the binding of a linkbase to a document to resolve this ambiguity ."
"1115","Automated synthesis of executable web service compositions from BPEL4WS processes We propose a technique for the automated synthesis of new composite web services . Given a set of abstract bpel4ws descriptions of component services , and a composition requirement , we automatically generate a concrete bpel4ws process that , when executed , interacts with the components and satisfies the requirement . We implement the proposed approach exploiting efficient representation techniques , and we show its scalability over case studies taken from a real world application and over a parameterized domain ."
"1116","Boosting SVM classifiers by ensemble By far , the support vector machines ( SVM ) achieve the state-of-the-art performance for the text classification ( TC ) tasks . Due to the complexity of the TC problems , it becomes a challenge to systematically develop classifiers with better performance . We try to attack this problem by ensemble methods , which are often used for boosting weak classifiers , such as decision tree , neural networks , etc. , and whether they are effective for strong classifiers is not clear ."
"1117","BackRank : an alternative for PageRank ? This paper proposes to extend a previous work , The Effect of the Back Button in a Random Walk : Application for PageRank ( 5 ) . We introduce an enhanced version of the PageRank algorithm using a realistic model for the Back button , thus improving the random surfer model . We show that in the special case where the history is bound to an unique page ( you can not use the Back button twice in a row ) , we can produce an algorithm that does not need much more resources than a standard PageRank . This algorithm , BackRank , can converge up to 30 % faster than a standard PageRank and suppress most of the drawbacks induced by the existence of pages without links ."
"1118","Adaptive page ranking with neural networks Recent developments in the area of neural networks provided new models which are capable of processing general types of graph structures . Neural networks are well-known for their generalization capabilities . This paper explores the idea of applying a novel neural network model to a web graph to compute an adaptive ranking of pages . Some early experimental results indicate that the new neural network models generalize exceptionally well when trained on a relatively small number of pages ."
"1119","Multispace information visualization framework for the intercomparison of data sets retrieved from web services We introduce a new visualization framework for the intercomparison of more than one data set retrieved from Web services . In our framework , we use more than one visualization space simultaneously , each of which visualizes a single data set retrieved from the Web service . For this purpose , we provide a new 3D component for accessing Web services , and provide a 3D space component , in which data set retrieved from the Web service is visualized . Moreover , our framework provides users with various operations applicable to these space components , i.e. , union , intersection , set-difference , cross-product , selection , projection , and joins ."
"1120","Diversified SCM standard for the Japanese retail industry In this paper , we present the concept of a diversified SCM ( supply chain management ) standard and distributed hub architecture which were used in B2B experiments for the Japanese retail industry . The conventional concept of B2B standards develops a single ideal set of business transactions to be supported . In contrast , our concept allows a wide range of diverse business transaction patterns necessary for industry supply chains . An industry develops a standard SCM model that partitions the whole supply chain into several transaction segments , each of which provides alternative business transaction patterns . For B2B collaboration , companies must agree on a collaboration configuration , which chooses the transaction alternatives from each segment . To support the development of a B2B system that executes an agreed collaboration , we introduce an SOA ( service oriented architecture ) based pattern called a distributed hub architecture . As a hub of B2B collaboration , it includes a complete set of services that can process every possible business transaction included in a standard SCM model . However , it does not function as a centralized service that coordinates participants . Instead , it is deployed on every participant and executes the assigned part of the supply chain collaboratively with other distributed hubs . Based on this concept , we analyzed actual business transactions in the Japanese retail industry and developed a standard SCM model , which represents more than a thousand possible transaction patterns . Based on the model , we developed an experimental system for the Japanese retail industry . The demonstration experiment involved major players in the industry including one of the largest general merchandise stores , one of the largest wholesalers , and major manufacturers in Japan ."
"1121","Persistence in web based collaborations We outline work on web based support for group creativity . We focus on a study of the effect persistence of participants ' musical contributions has on their mutual engagement ."
"1122","Multichannel publication of interactive media documents in a news environment Multichannel publication of multimedia presentations poses a significant challenge on the generic description of the presentation content and the system necessary to convert these descriptions into final-form presentations . We present a solution based on the XiMPF document model and a component based system architecture ."
"1123","Personal TV viewing by using live chat as metadata We propose a new TV viewing method by personalizing TV programs with live chat information on the Web . It enables a new way of viewing TV content from different perspectives reflecting viewers ' viewpoints ."
"1124","Analyzing online discussion for marketing intelligence We present a system that gathers and analyzes online discussion as it relates to consumer products . Weblogs and online message boards provide forums that record the voice of the public . Woven into this discussion is a wide range of opinion and commentary about consumer products . Given its volume , format and content , the appropriate approach to understanding this data is large-scale web and text data mining . By using a wide variety of state-of-the-art techniques including crawling , wrapping , text classification and computational linguistics , online discussion is gathered and annotated within a framework that provides for interactive analysis that yields marketing intelligence for our customers ."
"1125","Multi-step media adaptation : implementation of a knowledge-based engine Continuing changes in the domains of consumer devices and multimedia formats demand for a new approach to media adaptation . The publication of customized content on a device requires an automatic adaptation engine that takes into account the specifications of both the device and the material to be published . These specifications can be expressed using a single domain ontology that describes the concepts of the media adaptation domain . In this document , we provide insight into the implementation of an adaptation engine that exploits this domain knowledge . We explain how this engine , through the use of description matching and Semantic Web Services , composes a chain of adaptation services which will alter the original content to the needs of the target device ."
"1126","Building reactive web applications The Adaptive Web is a new research area addressing the personalization of the Web experience for each user . In this paper we propose a new high-level model for the specification of Web applications that take into account the manner users interact with the application for supplying appropriate contents or gathering profile data . We therefore consider entire processes ( rather than single properties ) as smallest information units , allowing for automatic restructuring of application components . For this purpose , a high-level Event-Condition-Action ( ECA ) paradigm is proposed , which enables capturing arbitrary ( and timed ) clicking behaviors ."
"1127","Mining directed social network from message board In the paper , we present an approach to mining a directed social network from a message board on the Internet where vertices denote individuals and directed links denote the flow of influence . The influence is measured based on propagating terms among individuals via messages . The distance with respect to contextual similarity between individuals is acquired since the influence indicates the degree of their shared interest represented as terms ."
"1128","Hera presentation generator Semantic Web Information Systems ( SWIS ) are Web Information Systems that use Semantic Web technologies . Hera is a model-driven design methodology for SWIS . In Hera , models are represented in RDFS and model instances in RDF . The Hera Presentation Generator ( HPG ) is an integrated development environment that supports the presentation generation layer of the Hera methodology . The HPG is based on a pipeline of data transformations driven by different Hera models ."
"1129","Enhancing the privacy of web-based communication A profiling adversary is an adversary whose goal is to classify a population of users into categories according to messages they exchange . This adversary models the most common privacy threat against web based communication . We propose a new encryption scheme , called stealth encryption , that protects users from profiling attacks by concealing the semantic content of plaintext while preserving its grammatical structure and other non-semantic linguistic features , such as word frequency distribution . Given English plaintext , stealth encryption produces ciphertext that can not efficiently be distinguished from normal English text ( our techniques apply to other languages as well ) ."
"1130","Efficient structural joins with on-the-fly indexing Previous work on structural joins mostly focuses on maintaining offline indexes on disks . Most of them also require the elements in both sets to be sorted . In this paper , we study an on-the-fly , in-memory indexing approach to structural joins . There is no need to sort the elements or maintain indexes on disks . We identify the similarity between the structural join problem and the stabbing query problem , and extend a main memory-based indexing technique for stabbing queries to structural joins ."
"1131","The language observatory project ( LOP ) The first part of the paper provides a brief description of the Language Observatory Project ( LOP ) and highlights the major technical difficulties to be challenged . The latter part gives how we responded to these difficulties by adopting UbiCrawler as a data collecting engine for the project . An interactive collaboration between the two groups is producing quite satisfactory results ."
"1132","WCAG formalization with W3C standards Web accessibility consists on a set of checkpoints which are rather expensive to evaluate or to spot . However , using W3C technologies , this cost can be clearly minimized . This article presents a W3C formalized rule-set version for automatable checkpoints from WCAG 1.0 ."
"1133","XHTML meta data profiles In this paper , we describe XHTML Meta Data Profiles ( XMDP ) which use XHTML to define a simple profile format which is both human and machine readable . XMDP can be used to extend XHTML by defining new link relationships , meta data properties\/values , and class name semantics . XMDP has already been used to extend semantic XHTML to represent social networks , document licensing , voting , and tagging ."
"1134","A more precise model for web retrieval Most research works on web retrieval latency are object-level based , which we think is insufficient and sometimes inaccurate . In this paper , we propose a fine grained operation-level Web Retrieval Dependency Model ( WRDM ) to provide more precise capture of web retrieval process . Our model reveals some new factors in web retrieval which can not be seen at object level but are very important to studies in the web retrieval area ."
"1135","Cyclone : an encyclopedic web search site We propose a Web search site called `` Cyclone '' , in which a user can retrieve encyclopedic term descriptions on the Web . Cyclone searches the Web for headwords and page fragments describing the headwords . High-quality page fragments are selected as term descriptions and are classified into domains . The number of current headwords is over 700,000 ."
"1136","An agent system for ontology sharing on WWW Semantic Web Services ( SWS ) , a new generation WWW technology , will facilitate the automation of Web service tasks , including automated Web service discovery , execution , composition and mediation by using XML based metadata and ontology . There have been several efforts to build knowledge representation languages for Web Services . However , only few attempts have so far been made to develop applications based on SWS . Especially , front-end agent systems for users are one of the urgent research areas . The purpose of this paper is to introduce our new integrated front-end agent system for ontology management and SWS management ."
"1137","Testing google interfaces modified for the blind We present the results of a research project focus on improving the usability of web search tools for blind users who interact via screen reader and voice synthesizer . In the first stage of our study , we proposed eight specific guidelines for simplifying this interaction with search engines . Next , we evaluated these criteria by applying them to Google UIs , re-implementing the simple search and the result page . Finally , we prepared the environment for a remote test with 12 totally blind users . The results highlight how Google interfaces could be improved in order to simplify interaction for the blind ."
"1138","Relationship between web links and trade We report on observations on Web characterization studies that suggest that the amount of Web links among sites under different country-code top-level domains is related to the amount of trade between the corresponding countries ."
"1139","Graphical representation of RDF queries In this poster we discuss a graphical notation for representing queries for semistructured data . We try to strike a balance between expressiveness of the query language and simplicity and understandability of the graphical notation . We present the primitives of the notation by means of examples ."
"1140","Facetnet : a framework for analyzing communities and their evolutions in dynamic networks We discover communities from social network data , and analyze the community evolution . These communities are inherent characteristics of human interaction in online social networks , as well as paper citation networks . Also , communities may evolve over time , due to changes to individuals ' roles and social status in the network as well as changes to individuals ' research interests . We present an innovative algorithm that deviates from the traditional two-step approach to analyze community evolutions . In the traditional approach , communities are first detected for each time slice , and then compared to determine correspondences . We argue that this approach is inappropriate in applications with noisy data . In this paper , we propose FacetNet for analyzing communities and their evolutions through a robust unified process . In this novel framework , communities not only generate evolutions , they also are regularized by the temporal smoothness of evolutions . As a result , this framework will discover communities that jointly maximize the fit to the observed data and the temporal evolution . Our approach relies on formulating the problem in terms of non-negative matrix factorization , where communities and their evolutions are factorized in a unified way . Then we develop an iterative algorithm , with proven low time complexity , which is guaranteed to converge to an optimal solution . We perform extensive experimental studies , on both synthetic datasets and real datasets , to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods ."
"1141","Learning multiple graphs for document recommendations The Web offers rich relational data with different semantics . In this paper , we address the problem of document recommendation in a digital library , where the documents in question are networked by citations and are associated with other entities by various relations . Due to the sparsity of a single graph and noise in graph construction , we propose a new method for combining multiple graphs to measure document similarities , where different factorization strategies are used based on the nature of different graphs . In particular , the new method seeks a single low-dimensional embedding of documents that captures their relative similarities in a latent space . Based on the obtained embedding , a new recommendation framework is developed using semi-supervised learning on graphs . In addition , we address the scalability issue and propose an incremental algorithm . The new incremental method significantly improves the efficiency by calculating the embedding for new incoming documents only . The new batch and incremental methods are evaluated on two real world datasets prepared from CiteSeer . Experiments demonstrate significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method ."
"1142","Hybrid multicasting in large-scale service networks The importance of service composition has been widely recognized in the Internet research community due to its high flexibility in allowing development of customized applications . So far little attention has been paid to composite services ' runtime performance-related aspects , which are of great importance to wide-area applications . Service composition in the wide area actually creates a new type of routing problem which we call QoS service routing . We study this problem in large networks ( e.g. , the Web ) and provide distributed and scalable routing solutions with various optimization goals . Most importantly , we propose ways to reduce redundancies of data delivery and service execution through explorations of different types of multicast ( service multicast and data multicast ) in one-to-many application scenarios ."
"1143","Extending the compatibility notion for abstract WS-BPEL processes WS-BPEL defines a standard for executable processes . Executable processes are business processes which can be automated through an IT infrastructure . The WS-BPEL specification also introduces the concept of abstract processes : In contrast to their executable siblings , abstract processes are not executable and can have parts where business logic is disguised . Nevertheless , the WS-BPEL specification introduces a notion of compatibility between such an under-specified abstract process and a fully specified executable one . Basically , this compatibility notion defines a set of syntactical rules that can be augmented or restricted by profiles . So far , there exist two of such profiles : the Abstract Process Profile for Observable Behavior and the Abstract Process Profile for Templates . None of these profiles defines a concept of behavioral equivalence . Therefore , both profiles are too strict with respect to the rules they impose when deciding whether an executable process is compatible to an abstract one . In this paper , we propose a novel profile that extends the existing Abstract Process Profile for Observable Behavior by defining a behavioral relationship . We also show that our novel profile allows for more flexibility when deciding whether an executable and an abstract process are compatible ."
"1144","Test case prioritization for regression testing of service-oriented business applications Regression testing assures the quality of modified service-oriented business applications against unintended changes . However , a typical regression test suite is large in size . Earlier execution of those test cases that may detect failures is attractive . Many existing prioritization techniques order test cases according to their respective coverage of program statements in a previous version of the application . On the other hand , industrial service-oriented business applications are typically written in orchestration languages such as WS-BPEL and integrated with workflow steps and web services via XPath and WSDL . Faults in these artifacts may cause the application to extract wrong data from messages , leading to failures in service compositions . Surprisingly , current regression testing research hardly considers these artifacts . We propose a multilevel coverage model to capture the business process , XPath , and WSDL from the perspective of regression testing . We develop a family of test case prioritization techniques atop the model . Empirical results show that our techniques can achieve significantly higher rates of fault detection than existing techniques ."
"1145","Parallel crawlers In this paper we study how we can design an effective parallel crawler . As the size of the Web grows , it becomes imperative to parallelize a crawling process , in order to finish downloading pages in a reasonable amount of time . We first propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling . Based on this understanding , we then propose metrics to evaluate a parallel crawler , and compare the proposed architectures using 40 million pages collected from the Web . Our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture ."
"1146","Querying for meta knowledge The Semantic Web is based on accessing and reusing RDF data from many different sources , which one may assign different levels of authority and credibility . Existing Semantic Web query languages , like SPARQL , have targeted the retrieval , combination and reuse of facts , but have so far ignored all aspects of meta knowledge , such as origins , authorship , recency or certainty of data , to name but a few . In this paper , we present an original , generic , formalized and implemented approach for managing many dimensions of meta knowledge , like source , authorship , certainty and others . The approach re-uses existing RDF modeling possibilities in order to represent meta knowledge . Then , it extends SPARQL query processing in such a way that given a SPARQL query for data , one may request meta knowledge without modifying the original query . Thus , our approach achieves highly flexible and automatically coordinated querying for data and meta knowledge , while completely separating the two areas of concern ."
"1147","Efficient similarity joins for near duplicate detection With the increasing amount of data and the need to integrate data from multiple data sources , a challenging issue is to find near duplicate records efficiently . In this paper , we focus on efficient algorithms to find pairs of records such that their similarities are above a given threshold . Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records . We propose new filtering techniques by exploiting the ordering information ; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency . Experimental results show that our proposed algorithms can achieve up to 2.6 x - 5x speed-up over previous algorithms on several real datasets and provide alternative solutions to the near duplicate Web page detection problem ."
"1148","Race : finding and ranking compact connected trees for keyword proximity search over xml documents In this paper , we study the problem of keyword proximity search over XML documents and leverage the efficiency and effectiveness . We take the disjunctive semantics among input keywords into consideration and identify meaningful compact connected trees as the answers of keyword proximity queries . We introduce the notions of Compact Lowest Common Ancestor ( CLCA ) and Maximal CLCA ( MCLCA ) and propose Compact Connected Trees ( CCTrees ) and Maximal CCTrees ( MCCTrees ) to efficiently and effectively answer keyword queries . We propose a novel ranking mechanism , RACE , to Rank compAct Connected trEes , by taking into consideration both the structural similarity and the textual similarity . Our extensive experimental study shows that our method achieves both high search efficiency and effectiveness , and outperforms existing approaches significantly ."
"1149","Sailer : an effective search engine for unified retrieval of heterogeneous xml and web documents This paper studies the problem of unified ranked retrieval of heterogeneous XML documents and Web data . We propose an effective search engine called Sailer to adaptively and versatilely answer keyword queries over the heterogenous data . We model the Web pages and XML documents as graphs . We propose the concept of pivotal trees to effectively answer keyword queries and present an effective method to identify the top-k pivotal trees with the highest ranks from the graphs . Moreover , we propose effective indexes to facilitate the effective unified ranked retrieval . We have conducted an extensive experimental study using real datasets , and the experimental results show that Sailer achieves both high search efficiency and accuracy , and outperforms the existing approaches significantly ."
"1150","Web accessibility : a broader view Web accessibility is an important goal . However , most approaches to its attainment are based on unrealistic economic models in which Web content developers are required to spend too much for which they receive too little . We believe this situation is due , in part , to the overly narrow definitions given both to those who stand to benefit from enhanced access to the Web and what is meant by this enhanced access . In this paper , we take a broader view , discussing a complementary approach that costs developers less and provides greater advantages to a larger community of users . While we have quite specific aims in our technical work , we hope it can also serve as an example of how the technical conversation regarding Web accessibility can move beyond the narrow confines of limited adaptations for small populations ."
"1151","Extraction and mining of an academic social network This paper addresses several key issues in extraction and mining of an academic social network : 1 ) extraction of a researcher social network from the existing Web ; 2 ) integration of the publications from existing digital libraries ; 3 ) expertise search on a given topic ; and 4 ) association search between researchers . We developed a social network system , called ArnetMiner , based on proposed methods to the above problems . In total , 448,470 researcher profiles and 981,599 publications were extracted\/integrated after the system having been in operation for two years . The paper describes the architecture and main features of the system . It also briefly presents the experimental results of the proposed methods ."
"1152","A unified framework for name disambiguation Name ambiguity problem has been a challenging issue for a long history . In this paper , we intend to make a thorough investigation of the whole problem . Specifically , we formalize the name disambiguation problem in a unified framework . The framework can incorporate both attribute and relationship into a probabilistic model . We explore a dynamic approach for automatically estimating the person number K and employ an adaptive distance measure to estimate the distance between objects . Experimental results show that our proposed framework can significantly outperform the baseline method ."
"1153","Statistical properties of community structure in large social and information networks A large body of work has been devoted to identifying community structure in networks . A community is often though of as a set of nodes that has more connections between its members than to the remainder of the network . In this paper , we characterize as a function of size the statistical and structural properties of such sets of nodes . We define the network community profile plot , which characterizes the `` best '' possible community - according to the conductance measure - over a wide range of size scales , and we study over 70 large sparse real-world networks taken from a wide range of application domains . Our results suggest a significantly more refined picture of community structure in large real-world networks than has been appreciated previously . Our most striking finding is that in nearly every network dataset we examined , we observe tight but almost trivial communities at very small scales , and at larger size scales , the best possible communities gradually `` blend in '' with the rest of the network and thus become less `` community-like . '' This behavior is not explained , even at a qualitative level , by any of the commonly-used network generation models . Moreover , this behavior is exactly the opposite of what one would expect based on experience with and intuition from expander graphs , from graphs that are well-embeddable in a low-dimensional structure , and from small social networks that have served as testbeds of community detection algorithms . We have found , however , that a generative model , in which new edges are added via an iterative `` forest fire '' burning process , is able to produce graphs exhibiting a network community structure similar to our observations ."
"1154","Scholarly publishing and argument in hyperspace The World Wide Web is opening up access to documents and data for scholars . However it has not yet impacted on one of the primary activities in research : assessing new findings in the light of current knowledge and debating it with colleagues . The ClaiMaker system uses a directed graph model with similarities to hypertext , in which new ideas are published as nodes , which other contributors can build on or challenge in a variety of ways by linking to them . Nodes and links have semantic structure to facilitate the provision of specialist services for interrogating and visualizing the emerging network . By way of example , this paper is grounded in a ClaiMaker model to illustrate how new claims can be described in this structured way ."
"1155","A foundation for tool based mobility support for visually impaired web users Users make journeys through the Web . Web travel encompasses the tasks of orientation and navigation , the environment and the purpose of the journey . The ease of travel , its mobility , varies from page to page and site to site . For visually impaired users , in particular , mobility is reduced ; the objects that support travel are inaccessible or missing altogether . Web development tools need to include support to increase mobility . We present a framework for finding and classifying travel objects within Web pages . The evaluation carried out has shown that this framework supports a systematic and consistent method for assessing travel upon the Web . We propose that such a framework can provide the foundation for a semi-automated tool for the support of travel upon the Web ."
"1156","Lock-free consistency control for web 2.0 applications Online collaboration and sharing is the central theme of many web-based services that create the so-called Web 2.0 phenomena . Using the Internet as a computing platform , many Web 2.0 applications set up mirror sites to provide large-scale availability and to achieve load balance . However , in the age of Web 2.0 , where every user is also a writer and publisher , the deployment of mirror sites makes consistency maintenance a Web scale problem . Traditional concurrency control methods ( e.g. two phase lock , serialization , etc. ) are not up to the task for several reasons . First , large network latency between mirror sites will make two phase locking a throughput bottleneck . Second , locking will block a large portion of concurrent operations , which makes it impossible to provide large-scale availability . On the other hand , most Web 2.0 operations do not need strict serializability - it is not the intention of a user who is correcting a typo in a shared document to block another who is adding a comment , as long as consistency can still be achieved . Thus , in order to enable maximal online collaboration and sharing , we need a lock-free mechanism that can maintain consistency among mirror sites on the Web . In this paper , we propose a flexible and efficient method to achieve consistency maintenance in the Web 2.0 world . Our experiments show its good performance improvement compared with existing methods based on distributed lock ."
"1157","Using web structure for classifying and describing web pages The structure of the web is increasingly being used to improve organization , search , and analysis of information on the web . For example , Google uses the text in citing documents ( documents that link to the target document ) for search . We analyze the relative utility of document text , and the text in citing documents near the citation , for classification and description . Results show that the text in citing documents , when available , often has greater discriminative and descriptive power than the text in the target document itself . The combination of evidence from a document and citing documents can improve on either information source alone . Moreover , by ranking words and phrases in the citing documents according to expected entropy loss , we are able to accurately name clusters of web pages , even with very few positive examples . Our results confirm , quantify , and extend previous research using web structure in these areas , introducing new methods for classification and description of pages ."
"1158","A system for principled matchmaking in an electronic marketplace More and more resources are becoming available on the Web , and there is a growing need for infrastructures that , based on advertised descriptions , are able to semantically match demands with supplies . We formalize general properties a matchmaker should have , then we present a matchmaking facilitator , compliant with desired properties . The system embeds a NeoClassic reasoner , whose structural subsumption algorithm has been modified to allow match categorization into potential and partial , and ranking of matches within categories . Experiments carried out show the good correspondence between users and system rankings ."
"1159","Dynamic cost-per-action mechanisms and applications to online advertising We study the Cost-Per-Action or Cost-Per-Acquisition ( CPA ) charging scheme in online advertising . In this scheme , instead of paying per click , the advertisers pay only when a user takes a specific action ( e.g. fills out a form ) or completes a transaction on their websites . We focus on designing efficient and incentive compatible mechanisms that use this charging scheme . We describe a mechanism based on a sampling-based learning algorithm that under suitable assumptions is asymptotically individually rational , asymptotically Bayesian incentive compatible and asymptotically ex-ante efficient . In particular , we demonstrate our mechanism for the case where the utility functions of the advertisers are independent and identically-distributed random variables as well as the case where they evolve like independent reflected Brownian motions ."
"1160","E-learning personalization based on itineraries and long-term navigational behavior In this paper we describe a practical framework for studying then a navigational behavior of the users of an e-learning environment integrated in a virtual campus . The students navigate through the web based virtual campus interacting with learning resources which are structured following the SCORM e-learning standard . Our main goal is to design a usage mining tool for analyzing such user navigational behavior and for extracting relevant information that can be used to validate several aspects related to virtual campus design and usability but also to determine the optimal scheduling for each course depending on user profile . We intend to extend these quencing capabilities of the SCORM standard to include the concept of recommended itinerary , by combining teachers expertise with learned experience acquired by system usage analysis ."
"1161","A proposal for an owl rules language Although the OWLWeb Ontology Language adds considerable expressive power to the Semantic Web it does have expressive limitations , particularly with respect to what can be said about properties . Wepresent ORL ( OWL Rules Language ) , a Horn clause rules extension to OWL that overcomes many of these limitations . ORL extends OWL in a syntactically and semantically coherent manner : the basic syntax for ORL rules is an extension of the abstract syntax for OWL DL and OWLLite ; ORL rules are given formal meaning via an extension of the OWLDL model-theoretic semantics ; ORL rules are given an XML syntax basedon the OWL XML presentation syntax ; and a mapping from ORL rules to RDF graphs is given based on the OWL RDF\/XML exchange syntax . Wediscuss the expressive power of ORL , showing that the ontology consistency problem is undecidable , provide several examples of ORLusage , and discuss how reasoning support for ORL might be provided ."
"1162","Description logic programs : combining logic programs with description logic We show how to interoperate , semantically and inferentially , between the leading Semantic Web approaches to rules ( RuleML Logic Programs ) and ontologies ( OWL\/DAML + OIL Description Logic ) via analyzing their expressive intersection . To do so , we define a new intermediate knowledge representation ( KR ) contained within this intersection : Description Logic Programs ( DLP ) , and the closely related Description Horn Logic ( DHL ) which is an expressive fragment of first-order logic ( FOL ) . DLP provides a significant degree of expressiveness , substantially greater than the RDF-Schema fragment of Description Logic . We show how to perform DLP-fusion : the bidirectional translation of premises and inferences ( including typical kinds of queries ) from the DLP fragment of DL to LP , and vice versa from the DLP fragment of LP to DL . In particular , this translation enables one to `` build rules on top of ontologies '' : it enables the rule KR to have access to DL ontological definitions for vocabulary primitives ( e.g. , predicates and individual constants ) used by the rules . Conversely , the DLP-fusion technique likewise enables one to `` build ontologies on top of rules '' : it enables ontological definitions to be supplemented by rules , or imported into DL from rules . It also enables available efficient LP inferencing algorithms\/implementations to be exploited for reasoning over large-scale DL ontologies ."
"1163","Learning to rank relational objects and its application to web search Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects . The technology has been successfully applied to web search , and is becoming one of the key machineries for building search engines . Existing approaches to learning to rank , however , did not consider the cases in which there exists relationship between the objects to be ranked , despite of the fact that such situations are very common in practice . For example , in web search , given a query certain relationships usually exist among the the retrieved documents , e.g. , URL hierarchy , similarity , etc. , and sometimes it is necessary to utilize the information in ranking of the documents . This paper addresses the issue and formulates it as a novel learning problem , referred to as , ` learning to rank relational objects ' . In the new learning task , the ranking model is defined as a function of not only the contents ( features ) of objects but also the relations between objects . The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined . It formalizes the learning task as an optimization problem in the setting . The paper then proposes a new method to perform the optimization task , particularly an implementation based on SVM . Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks ( Pseudo Relevance Feedback and Topic Distillation ) in web search , indicating that the proposed method can indeed make effective use of relation information and content information in ranking ."
"1164","Towards practical genre classification of web documents Classification of documents by genre is typically done either using linguistic analysis or term frequency based techniques . The former provides better classification accuracy than the latter but at the cost of two orders of magnitude more computation time . While term frequency analysis requires much less computational resources than linguistic analysis , it returns poor classification accuracy when the genres are not sufficiently distinct . A method that removes or approximates the expensive portions of linguistic analysis is presented . The accuracy and computation time of this method then compared with both linguistic analysis and term frequency analysis . The results in this paper show that this method can significantly reduce the computation of both time of linguistic analysis and term frequency analysis , while retaining an accuracy that is higher than that of term frequency analysis ."
"1165","Ranking the web frontier The celebrated PageRank algorithm has proved to be a very effective paradigm for ranking results of web search algorithms . In this paper we refine this basic paradigm to take into account several evolving prominent features of the web , and propose several algorithmic innovations . First , we analyze features of the rapidly growing `` frontier '' of the web , namely the part of the web that crawlers are unable to cover for one reason or another . We analyze the effect of these pages and find it to be significant . We suggest ways to improve the quality of ranking by modeling the growing presence of `` link rot '' on the web as more sites and pages fall out of maintenance . Finally we suggest new methods of ranking that are motivated by the hierarchical structure of the web , are more efficient than PageRank , and may be more resistant to direct manipulation ."
"1166","Foundations for service ontologies : aligning OWL-S to dolce Clarity in semantics and a rich formalization of this semantics are important requirements for ontologies designed to be deployed in large-scale , open , distributed systems such as the envisioned Semantic Web This is especially important for the description of Web Services , which should enable complex tasks involving multiple agents . As one of the first initiatives of the Semantic Webcommunity for describing Web Services , OWL-S attracts a lot of interest even though it is still under development . We identify problematic aspects of OWL-S and suggest enhancements through alignment to a foundational ontology . Another contribution of ourwork is the Core Ontology of Services that tries to fill the epistemological gap between the foundational ontology and OWL-S . It can be reused to align other Web Service description languages as well . Finally , we demonstrate the applicability of our work byaligning OWL-S ' standard example called CongoBuy ."
"1167","Statistical analysis of the social network and discussion threads in slashdot We analyze the social network emerging from the user comment activity on the website Slashdot . The network presents common features of traditional social networks such as a giant component , small average path length and high clustering , but differs from them showing moderate reciprocity and neutral assortativity by degree . Using Kolmogorov-Smirnov statistical tests , we show that the degree distributions are better explained by log-normal instead of power-law distributions . We also study the structure of discussion threads using an intuitive radial tree representation . Threads show strong heterogeneity and self-similarity throughout the different nesting levels of a conversation . We use these results to propose a simple measure to evaluate the degree of controversy provoked by a post ."
"1168","A dynamic bayesian network click model for web search ranking As with any application of machine learning , web search ranking requires labeled data . The labels usually come in the form of relevance assessments made by editors . Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels . The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant . In this paper , we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs . Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance ."
"1169","Quicklink selection for navigational query results Quicklinks for a website are navigational shortcuts displayed below the website homepage on a search results page , and that let the users directly jump to selected points inside the website . Since the real-estate on a search results page is constrained and valuable , picking the best set of quicklinks to maximize the benefits for a majority of the users becomes an important problem for search engines . Using user browsing trails obtained from browser toolbars , and a simple probabilistic model , we formulate the quicklink selection problem as a combinatorial optimizaton problem . We first demonstrate the hardness of the objective , and then propose an algorithm that is provably within a factor of 1-1 \/ e of the optimal . We also propose a different algorithm that works on trees and that can find the optimal solution ; unlike the previous algorithm , this algorithm can incorporate natural constraints on the set of chosen quicklinks . The efficacy of our methods is demonstrated via empirical results on both a manually labeled set of websites and a set for which quicklink click-through rates for several webpages were obtained from a real-world search engine ."
"1170","Model based engineering of learning situations for adaptive web based educational systems In this paper , we propose an approach for the engineering of web based educational applications . The applications that we focus require advanced functionality for regulating and tutoring learners ' activities ( dynamics of learning ) . Our approach aims at proposing models , not only to describe details of such learning situations , but also to characterize the constraints that the Learning Management System exploiting such situations must satisfy in this sense , this approach also contributes to the specification of the Adaptive Web Based Educational System ( AWBES ) fitted to a particular learning situation . Moreover , this approach for the engineering of learning situations conforms to current software engineering research works ."
"1171","A hybrid phish detection approach by identity discovery and keywords retrieval Phishing is a significant security threat to the Internet , which causes tremendous economic loss every year . In this paper , we proposed a novel hybrid phish detection method based on information extraction ( IE ) and information retrieval ( IR ) techniques . The identity-based component of our method detects phishing webpages by directly discovering the inconsistency between their identity and the identity they are imitating . The keywords-retrieval component utilizes IR algorithms exploiting the power of search engines to identify phish . Our method requires no training data , no prior knowledge of phishing signatures and specific implementations , and thus is able to adapt quickly to constantly appearing new phishing patterns . Comprehensive experiments over a diverse spectrum of data sources with 11449 pages show that both components have a low false positive rate and the stacked approach achieves a true positive rate of 90.06 % with a false positive rate of 1.95 % ."
"1172","Web people search : results of the first evaluation and the plan for the second This paper presents the motivation , resources and results for the first Web People Search task , which was organized as part of the SemEval-2007 evaluation exercise . Also , we will describe a survey and proposal for a new task , `` attribute extraction '' , which is planned for inclusion in the second evaluation , planned for autumn , 2008 ."
"1173","Evaluating similarity measures for emergent semantics of social tagging Social bookmarking systems are becoming increasingly important data sources for bootstrapping and maintaining Semantic Web applications . Their emergent information structures have become known as folksonomies . A key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies , and which measures are best suited for applications such as community detection , navigation support , semantic search , user profiling and ontology learning . Here we build an evaluation framework to compare various general folksonomy-based similarity measures , which are derived from several established information-theoretic , statistical , and practical measures . Our framework deals generally and symmetrically with users , tags , and resources . For evaluation purposes we focus on similarity between tags and between resources and consider different methods to aggregate annotations across users . After comparing the ability of several tag similarity measures to predict user-created tag relations , we provide an external grounding by user-validated semantic proxies based on WordNet and the Open Directory Project . We also investigate the issue of scalability . We find that mutual information with distributional micro-aggregation across users yields the highest accuracy , but is not scalable ; per-user projection with collaborative aggregation provides the best scalable approach via incremental computations . The results are consistent across resource and tag similarity ."
"1174","A comparative analysis of web and peer-to-peer traffic Peer-to-Peer ( P2P ) applications continue to grow in popularity , and have reportedly overtaken Web applications as the single largest contributor to Internet traffic . Using traces collected from a large edge network , we conduct an extensive analysis of P2P traffic , compare P2P traffic with Web traffic , and discuss the implications of increased P2P traffic . In addition to studying the aggregate P2P traffic , we also analyze and compare the two main constituents of P2P traffic in our data , namely BitTorrent and Gnutella . The results presented in the paper may be used for generating synthetic workloads , gaining insights into the functioning of P2P applications , and developing network management strategies . For example , our results suggest that new models are necessary for Internet traffic . As a first step , we present flow-level distributional models for Web and P2P traffic that may be used in network simulation and emulation experiments ."
"1175","Less talk , more rock : automated organization of community-contributed collections of concert videos We describe a system for synchronization and organization of user-contributed content from live music events . We start with a set of short video clips taken at a single event by multiple contributors , who were using a varied set of capture devices . Using audio fingerprints , we synchronize these clips such that overlapping clips can be displayed simultaneously . Furthermore , we use the timing and link structure generated by the synchronization algorithm to improve the findability and representation of the event content , including identifying key moments of interest and descriptive text for important captured segments of the show . We also identify the preferred audio track when multiple clips overlap . We thus create a much improved representation of the event that builds on the automatic content match . Our work demonstrates important principles in the use of content analysis techniques for social media content on the Web , and applies those principles in the domain of live music capture ."
"1176","PodCred : a framework for analyzing podcast preference The PodCred framework is a framework for assessing the credibility and quality of podcasts published on the internet . It consists of a series of indicators designed to support prediction of listener preference of one podcast over another , given that both carry comparable informational content . The indicators are grouped into four categories pertaining to the Podcast Content , the Podcaster , the Podcast Context or the Technical Execution of the podcast . We adopt the term `` cred '' as a designation encompassing both credibility ( comprising trustworthiness and expertise ) and qualitative acceptability to listeners . Our podcast analysis framework is inspired by work on credibility in blogs , another medium dominated by user generated content . The PodCred framework is derived from a review of the literature on credibility for other media , a survey of prescriptive standards for podcasting , and a detailed data analysis of award winning podcasts . The paper concludes with a discussion of future work in which the framework will be applied ."
"1177","How people use the web on mobile devices This paper describes a series of user studies on how people use the Web via mobile devices . The data primarily comes from contextual inquiries with 47 participants between 2004 and 2007 , and is complemented with a phone log analysis of 577 panelists in 2007 . We report four key contextual factors in using the Web on mobile devices and propose mobile Web activity taxonomy . The framework contains three user activity categories identical to previous stationary Web studies : information seeking , communication , and transaction , and a new category : personal space extension . The new category refers to the practice that people put their content on the Web for personal access , therefore extending their personal information space ."
"1178","Behavioral classification on the click graph A bipartite query-URL graph , where an edge indicates that a document was clicked for a query , is a useful construct for finding groups of related queries and URLs . Here we use this behavior graph for classification . We choose a click graph sampled from two weeks of image search activity , and the task of `` adult '' filtering : identifying content in the graph that is inappropriate for minors . We show how to perform classification using random walks on this graph , and two methods for estimating classifier parameters ."
"1179","Using the wisdom of the crowds for keyword generation In the sponsored search model , search engines are paid by businesses that are interested in displaying ads for their site alongside the search results . Businesses bid for keywords , and their ad is displayed when the keyword is queried to the search engine . An important problem in this process is ` keyword generation ' : given a business that is interested in launching a campaign , suggest keywords that are related to that campaign . We address this problem by making use of the query logs of the search engine . We identify queries related to a campaign by exploiting the associations between queries and URLs as they are captured by the user 's clicks . These queries form good keyword suggestions since they capture the `` wisdom of the crowd '' as to what is related to a site . We formulate the problem as a semi-supervised learning problem , and propose algorithms within the Markov Random Field model . We perform experiments with real query logs , and we demonstrate that our algorithms scale to large query logs and produce meaningful results ."
"1180","How opinions are received by online communities : a case study on amazon.com helpfulness votes There are many on-line settings in which users publicly express opinions . A number of these offer mechanisms for other users to evaluate these opinions ; a canonical example is Amazon.com , where reviews come with annotations like `` 26 of 32 people found the following review helpful . '' Opinion evaluation appears in many off-line settings as well , including market research and political campaigns . Reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself : rather than asking , `` What did Y think of X ? '' , we are asking , `` What did Z think of Y 's opinion of X ? '' Here we develop a framework for analyzing and modeling opinion evaluation , using a large-scale collection of Amazon book reviews as a dataset . We find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product . As part of our approach , we develop novel methods that take advantage of the phenomenon of review `` plagiarism '' to control for the effects of text in opinion evaluation , and we provide a simple and natural mathematical model consistent with our findings . Our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology , and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from ifferent countries ."
"1181","CaTTS : calendar types and constraints for Web applications Data referring to cultural calendars such as the widespread Gregorian dates but also dates after the Chinese , Hebrew , or Islamic calendars as well as data referring to professional calendars like fiscal years or teaching terms are omnipresent on the Web . Formalisms such as XML Schema have acknowledged this by offering a rather extensive set of Gregorian dates and times as basic data types . This article introduces into CaTTS , the ( u ) C ( \/ u ) alendar ( u ) a ( \/ u ) nd ( u ) T ( \/ u ) ime ( u ) T ( \/ u ) ype ( u ) S ( \/ u ) ystem . CaTTS goes far beyond predefined date and time types after the Gregorian calendar as supported by XML Schema . CaTTS first gives rise to declaratively specify more or less complex cultural or professional calendars including specificities such as leap seconds , leap years , and time zones . CaTTS further offers a tool for the static type checking ( of data typed after calendar ( s ) defined in CaTTS ) . CaTTS finally offers a language for declaratively expressing and a solver for efficiently solving temporal constraints ( referring to calendar ( s ) expressed in CaTTS ) . CaTTS complements data modeling and reasoning methods designed for generic Semantic Web applications such as RDF or OWL with methods specific to the particular application domain of calendars and time ."
"1182","A clustering method for news articles retrieval system Organizing the results of a search facilitates the user in overviewing the information returned . We regard the clustering task as the tasks of making labels for a list of items and we focus on news articles and propose a clustering method that uses named entity extraction ."
"1183","Image annotation using clickthrough data Automatic image annotation using supervised learning is performed by concept classifiers trained on labeled example images . This work proposes the use of clickthrough data collected from search logs as a source for the automatic generation of concept training data , thus avoiding the expensive manual annotation effort . We investigate and evaluate this approach using a collection of 97,628 photographic images . The results indicate that the contribution of search log based training data is positive ; in particular , the combination of manual and automatically generated training data outperforms the use of manual data alone . It is therefore possible to use clickthrough data to perform large-scale image annotation with little manual annotation effort or , depending on performance , using only the automatically generated training data . The datasets used as well as an extensive presentation of the experimental results can be accessed at http:\/\/olympus.ee.auth.gr\/~diou\/civr2009\/ ."
"1184","Using static analysis for Ajax intrusion detection We present a static control-flow analysis for JavaScript programs running in a web browser . Our analysis tackles numerous challenges posed by modern web applications including asynchronous communication , frameworks , and dynamic code generation . We use our analysis to extract a model of expected client behavior as seen from the server , and build an intrusion-prevention proxy for the server : the proxy intercepts client requests and disables those that do not meet the expected behavior . We insert random asynchronous requests to foil mimicry attacks . Finally , we evaluate our technique against several real applications and show that it protects against an attack in a widely-used web application ."
"1185","Mining multilingual topics from wikipedia In this paper , we try to leverage a large-scale and multilingual knowledge base , Wikipedia , to help effectively analyze and organize Web information written in different languages . Based on the observation that one Wikipedia concept may be described by articles in different languages , we adapt existing topic modeling algorithm for mining multilingual topics from this knowledge base . The extracted ` universal ' topics have multiple types of representations , with each type corresponding to one language . Accordingly , new documents of different languages can be represented in a space using a group of universal topics , which makes various multilingual Web applications feasible ."
"1186","The web of things The Web , similar to other successful man made systems is continuously evolving . With the miniaturization and increased performance of computing devices which are also being embedded in common physical objects , it is natural that the Web evolved to also include these - therefore the Web of Things . This tutorial provides an overview of the system vertical structure by identifying the relevant components , illustrating their functionality and showing existing tools and systems . The aim is to show how small devices can be connected to the Web at various levels of abstraction and transform them into `` first-class '' Web residents ."
"1187","Supervised rank aggregation This paper is concerned with rank aggregation , the task of combining the ranking results of individual rankers at meta-search . Previously , rank aggregation was performed mainly by means of unsupervised learning . To further enhance ranking accuracies , we propose employing supervised learning to perform the task , using labeled data . We refer to the approach as Supervised Rank Aggregation . We set up a general framework for conducting Supervised Rank Aggregation , in which learning is formalized an optimization which minimizes disagreements between ranking results and the labeled data . As case study , we focus on Markov Chain based rank aggregation in this paper . The optimization for Markov Chain based methods is not a convex optimization problem , however , and thus is hard to solve . We prove that we can transform the optimization problem into that of Semidefinite Programming and solve it efficiently . Experimental results on meta-searches show that Supervised Rank Aggregation can significantly outperform existing unsupervised methods ."
"1188","Evaluating strategies for similarity search on the web Finding pages on the Web that are similar to a query page ( Related Pages ) is an important component of modern search engines . A variety of strategies have been proposed for answering Related Pages queries , but comparative evaluation by user studies is expensive , especially when large strategy spaces must be searched ( e.g. , when tuning parameters ) . We present a technique for automatically evaluating strategies using Web hierarchies , such as Open Directory , in place of user feedback . We apply this evaluation methodology to a mix of document representation strategies , including the use of text , anchor-text , and links . We discuss the relative advantages and disadvantages of the various approaches examined . Finally , we describe how to efficiently construct a similarity index out of our chosen strategies , and provide sample results from our index ."
"1189","Inferring private information using social network data On-line social networks , such as Facebook , are increasingly utilized by many users . These networks allow people to publish details about themselves and connect to their friends . Some of the information revealed inside these networks is private and it is possible that corporations could use learning algorithms on the released data to predict undisclosed private information . In this paper , we explore how to launch inference attacks using released social networking data to predict undisclosed private information about individuals . We then explore the effectiveness of possible sanitization techniques that can be used to combat such inference attacks under different scenarios ."
"1190","To join or not to join : the illusion of privacy in social networks with mixed public and private user profiles In order to address privacy concerns , many social media websites allow users to hide their personal profiles from the public . In this work , we show how an adversary can exploit an online social network with a mixture of public and private user profiles to predict the private attributes of users . We map this problem to a relational classification problem and we propose practical models that use friendship and group membership information ( which is often not hidden ) to infer sensitive attributes . The key novel idea is that in addition to friendship links , groups can be carriers of significant information . We show that on several well-known social media sites , we can easily and accurately recover the information of private-profile users . To the best of our knowledge , this is the first work that uses link-based and group-based classification to study privacy implications in social networks with mixed public and private user profiles ."
"1191","How to make a semantic web browser Two important architectural choices underlie the success of the Web : numerous , independently operated servers speak a common protocol , and a single type of client the Web browser provides point-and-click access to the content and services on these decentralized servers . However , because HTML marries content and presentation into a single representation , end users are often stuck with inappropriate choices made by the Web site designer of how to work with and view the content . RDF metadata on the Semantic Web does not have this limitation : users can gain direct access to information and control over how it is presented . This principle forms the basis for our Semantic Web browser an end user application that automatically locates metadata and assembles point-and-click interfaces from a combination of relevant information , ontological specifications , and presentation knowledge , all described in RDF and retrieved dynamically from the Semantic Web . Because data and services are accessed directly through a standalone client and not through a central point of access ( e.g. , a portal ) , new content and services can be consumed as soon as they become available . In this way we take advantage of an important sociological force that encourages the production of new Semantic Web content while remaining faithful to the decentralized nature of the Web ."
"1192","Semantic web support for the business-to-business e-commerce lifecycle If an e-services approach to electronic commerce is to become widespread , standardisation of ontologies , message content and message protocols will be necessary . In this paper , we present a lifecycle of a business-to-business e-commerce interaction , and show how the Semantic Web can support a service description language that can be used throughout this lifecycle . By using DAML , we develop a service description language sufficiently expressive and flexible to be used not only in advertisements , but also in matchmaking queries , negotiation proposals and agreements . We also identify which operations must be carried out on this description language if the B2B lifecycle is to be fully supported . We do not propose specific standard protocols , but instead argue that our operators are able to support a wide variety of interaction protocols , and so will be fundamental irrespective of which protocols are finally adopted ."
"1193","A probabilistic approach to automated bidding in alternative auctions This paper presents an approach to develop bidding agents that participate in multiple alternative auctions , with the goal of obtaining an item at the lowest price . The approach consists of a prediction method and a planning algorithm . The prediction method exploits the history of past auctions in order to build probability functions capturing the belief that a bid of a given price may win a given auction . The planning algorithm computes the lowest price , such that by sequentially bidding in a subset of the relevant auctions , the agent can obtain the item at that price with an acceptable probability . The approach addresses the case where the auctions are for substitutable items with different values . Experimental results are reported , showing that the approach increases the payoff of their users and the welfare of the market ."
"1194","Analysis of multimedia workloads with implications for internet streaming In this paper , we study the media workload collected from a large number of commercial Web sites hosted by a major ISP and that collected from a large group of home users connected to the Internet via a well-known cable company . Some of our key findings are : ( 1 ) Surprisingly , the majority of media contents are still delivered via downloading from Web servers . ( 2 ) A substantial percentage of media downloading connections are aborted before completion due to the long waiting time . ( 3 ) A hybrid approach , pseudo streaming , is used by clients to imitate real streaming . ( 4 ) The mismatch between the downloading rate and the client playback speed in pseudo streaming is common , which either causes frequent playback delays to the clients , or unnecessary traffic to the Internet . ( 5 ) Compared with streaming , downloading and pseudo streaming are neither bandwidth efficient nor performance effective . To address this problem , we propose the design of AutoStream , an innovative system that can provide additional previewing and streaming services automatically for media objects hosted on standard Web sites in server farms at the client 's will ."
"1195","DOM-based content extraction of HTML documents Web pages often contain clutter ( such as pop-up ads , unnecessary images and extraneous links ) around the body of an article that distracts a user from actual content . Extraction of `` useful and relevant '' content from web pages has many applications , including cell phone and PDA browsing , speech rendering for the visually impaired , and text summarization . Most approaches to removing clutter or making content more readable involve changing font size or removing HTML and data components such as images , which takes away from a webpage 's inherent look and feel . Unlike `` Content Reformatting '' , which aims to reproduce the entire webpage in a more convenient form , our solution directly addresses `` Content Extraction '' . We have developed a framework that employs easily extensible set of techniques that incorporate advantages of previous work on content extraction . Our key insight is to work with the DOM trees , rather than with raw HTML markup . We have implemented our approach in a publicly available Web proxy to extract content from HTML web pages ."
"1196","All your contacts are belong to us : automated identity theft attacks on social networks Social networking sites have been increasingly gaining popularity . Well-known sites such as Facebook have been reporting growth rates as high as 3 % per week . Many social networking sites have millions of registered users who use these sites to share photographs , contact long-lost friends , establish new business contacts and to keep in touch . In this paper , we investigate how easy it would be for a potential attacker to launch automated crawling and identity theft attacks against a number of popular social networking sites in order to gain access to a large volume of personal user information . The first attack we present is the automated identity theft of existing user profiles and sending of friend requests to the contacts of the cloned victim . The hope , from the attacker 's point of view , is that the contacted users simply trust and accept the friend request . By establishing a friendship relationship with the contacts of a victim , the attacker is able to access the sensitive personal information provided by them . In the second , more advanced attack we present , we show that it is effective and feasible to launch an automated , cross-site profile cloning attack . In this attack , we are able to automatically create a forged profile in a network where the victim is not registered yet and contact the victim 's friends who are registered on both networks . Our experimental results with real users show that the automated attacks we present are effective and feasible in practice ."
"1197","Investigating web services on the world wide web Searching for Web service access points is no longer attached to service registries as Web search engines have become a new major source for discovering Web services . In this work , we conduct a thorough analytical investigation on the plurality of Web service interfaces that exist on the Web today . Using our Web Service Crawler Engine ( WSCE ) , we collect metadata service information on retrieved interfaces through accessible UBRs , service portals and search engines . We use this data to determine Web service statistics and distribution based on object sizes , types of technologies employed , and the number of functioning services . This statistical data can be used to help determine the current status of Web services . We determine an intriguing result that 63 % of the available Web services on the Web are considered to be active . We further use our findings to provide insights on improving the service retrieval process ."
"1198","An event-condition-action language for XML XML repositories are now a widespread means for storing and exchanging information on the Web . As these repositories become increasingly used in dynamic applications such as e-commerce , there is a rapidly growing need for a mechanism to incorporate reactive functionality in an XML setting . Event-condition-action ( ECA ) rules are a technology from active databases and are a natural method for supporting suchfunctionality . ECA rules can be used for activities such as automatically enforcing document constraints , maintaining repository statistics , and facilitating publish\/subscribe applications . An important question associated with the use of a ECA rules is how to statically predict their run-time behavior . In this paper , we define a language for ECA rules on XML repositories . We then investigate methods for analyzing the behavior of a set of ECA rules , a task which has added complexity in this XML setting compared with conventional active databases ."
"1199","Near real time information mining in multilingual news This paper presents a near real-time multilingual news monitoring and analysis system that forms the backbone of our research work . The system integrates technologies to address the problems related to information extraction and analysis of open source intelligence on the World Wide Web . By chaining together different techniques in text mining , automated machine learning and statistical analysis , we can automatically determine who , where and , to a certain extent , what is being reported in news articles ."
"1200","Opinion integration through semi-supervised topic modeling Web 2.0 technology has enabled more and more people to freely express their opinions on the Web , making the Web an extremely valuable source for mining user opinions about all kinds of topics . In this paper we study how to automatically integrate opinions expressed in a well-written expert review with lots of opinions scattering in various sources such as blogspaces and forums . We formally define this new integration problem and propose to use semi-supervised topic models to solve the problem in a principled way . Experiments on integrating opinions about two quite different topics ( a product and a political figure ) show that the proposed method is effective for both topics and can generate useful aligned integrated opinion summaries . The proposed method is quite general . It can be used to integrate a well written review with opinions in an arbitrary text collection about any topic to potentially support many interesting applications in multiple domains ."
"1201","Improved techniques for result caching in web search engines Query processing is a major cost factor in operating large web search engines . In this paper , we study query result caching , one of the main techniques used to optimize query processing performance . Our first contribution is a study of result caching as a weighted caching problem . Most previous work has focused on optimizing cache hit ratios , but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered . We describe and evaluate several algorithms for weighted result caching , and study the impact of Zipf-based query distributions on result caching . Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods , substantially narrowing the existing performance gap to the theoretically optimal ( clairvoyant ) method . Finally , using the same approach , we also obtain performance gains for the related problem of inverted list caching ."
"1202","Learning deterministic regular expressions for the inference of schemas from XML data Inferring an appropriate DTD or XML Schema Definition ( XSD ) for a given collection of XML documents essentially reduces to learning deterministic regular expressions from sets of positive example words . Unfortunately , there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only , as we will show . The regular expressions occurring in practical DTDs and XSDs , however , are such that every alphabet symbol occurs only a small number of times . As such , in practice it suffices to learn the subclass of regular expressions in which each alphabet symbol occurs at most k times , for some small k. We refer to such expressions as k-occurrence regular expressions ( k-OREs for short ) . Motivated by this observation , we provide a probabilistic algorithm that learns k-OREs for increasing values of k , and selects the one that best describes the sample based on a Minimum Description Length argument . The effectiveness of the method is empirically validated both on real world and synthetic data . Furthermore , the method is shown to be conservative over the simpler classes of expressions considered in previous work ."
"1203","Pagerank for product image search In this paper , we cast the image-ranking problem into the task of identifying `` authority '' nodes on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of images . Through an iterative procedure based on the PageRank computation , a numerical weight is assigned to each image ; this measures its relative importance to the other images being considered . The incorporation of visual signals in this process differs from the majority of large-scale commercial-search engines in use today . Commercial search-engines often solely rely on the text clues of the pages in which images are embedded to rank images , and often entirely ignore the content of the images themselves as a ranking signal . To quantify the performance of our approach in a real-world system , we conducted a series of experiments based on the task of retrieving images for 2000 of the most popular products queries . Our experimental results show significant improvement , in terms of user satisfaction and relevancy , in comparison to the most recent Google Image Search results ."
"1204","Estimating web site readability using content extraction Nowadays , information is primarily searched on the WWW . From a user perspective , the readability is an important criterion for measuring the accessibility and thereby the quality of an information . We show that modern content extraction algorithms help to estimate the readability of a web document quite accurate ."
"1205","Visualising student tracking data to support instructors in web-based distance education This paper presents a novel approach of using web log data generated by course management systems ( CMS ) to help instructors become aware of what is happening in distance learning classes . Specifically , techniques from Information Visualization are used to graphically render complex , multidimensional student tracking data collected by CMS . A system , called CourseVis , illustrates the proposed approach . Graphical representations from the use of CourseVis to visualise data from a java on-line distance course ran with WebCT are presented . Findings from the evaluation of CourseVis are presented , and it is argued that CourseVis can help teachers become aware of some social , behavioural , and cognitive aspects related to distance learners . Using graphical representations of student tracking data , instructors can identify tendencies in their classes , or quickly discover individuals that need special attention ."
"1206","Online expansion of rare queries for sponsored search Sponsored search systems are tasked with matching queries to relevant advertisements . The current state-of-the-art matching algorithms expand the user 's query using a variety of external resources , such as Web search results . While these expansion-based algorithms are highly effective , they are largely inefficient and can not be applied in real-time . In practice , such algorithms are applied offline to popular queries , with the results of the expensive operations cached for fast access at query time . In this paper , we describe an efficient and effective approach for matching ads against rare queries that were not processed offline . The approach builds an expanded query representation by leveraging offline processing done for related popular queries . Our experimental results show that our approach significantly improves the effectiveness of advertising on rare queries with only a negligible increase in computational cost ."
"1207","Privacy preserving frequency capping in internet banner advertising We describe an optimize-and-dispatch approach for delivering pay-per-impression advertisements in online advertising . The platform provider for an advertising network commits to showing advertisers ' banner ads while capping the number of advertising message shown to a unique user as the user transitions through the network . The traditional approach for enforcing frequency caps has been to use cross-site cookies to track users . However,cross-site cookies and other tracking mechanisms can infringe on the user privacy . In this paper , we propose a novel linear programming approach that decides when to show an ad to the user based solely on the page currently viewed by the users . We show that the frequency caps are fulfilled in expectation . We show the efficacy of that approach using simulation results ."
"1208","Ontology-based learning content repurposing This paper investigates basic research issues that need to be addressed for developing an architecture that enables repurposing of learning objects in a flexible way . Currently , there are a number of Learning Object Content Models ( e.g. the SCORM Content Aggregation Model ) that define learning objects and their components in a more or less precise way . However , these models do not allow repurposing of fine-grained components ( sentences , images ) . We developed an ontology-based solution for content repurposing . The ontology is a solid basis for an architecture that will enable on-the-fly access to learning object components and that will facilitate repurposing these components ."
"1209","Towards intent-driven bidterm suggestion In online advertising , pervasive in commercial search engines , advertisers typically bid on few terms , and the scarcity of data makes ad matching difficult . Suggesting additional bidterms can significantly improve ad clickability and conversion rates . In this paper , we present a large-scale bidterm suggestion system that models an advertiser 's intent and finds new bidterms consistent with that intent . Preliminary experiments show that our system significantly increases the coverage of a state of the art production system used at Yahoo while maintaining comparable precision ."
"1210","Efficient interactive fuzzy keyword search Traditional information systems return answers after a user submits a complete query . Users often feel `` left in the dark '' when they have limited knowledge about the underlying data , and have to use a try-and-see approach for finding information . A recent trend of supporting autocomplete in these systems is a first step towards solving this problem . In this paper , we study a new information-access paradigm , called `` interactive , fuzzy search , '' in which the system searches the underlying data `` on the fly '' as the user types in query keywords . It extends autocomplete interfaces by ( 1 ) allowing keywords to appear in multiple attributes ( in an arbitrary order ) of the underlying data ; and ( 2 ) finding relevant records that have keywords matching query keywords approximately . This framework allows users to explore data as they type , even in the presence of minor errors . We study research challenges in this framework for large amounts of data . Since each keystroke of the user could invoke a query on the backend , we need efficient algorithms to process each query within milliseconds . We develop various incremental-search algorithms using previously computed and cached results in order to achieve an interactive speed . We have deployed several real prototypes using these techniques . One of them has been deployed to support interactive search on the UC Irvine people directory , which has been used regularly and well received by users due to its friendly interface and high efficiency ."
"1211","Educanext : a framework for sharing live educational resources with isabel EducaNext is an educational mediator created within the UNIVERSAL IST Project which supports both , the exchange of reusable educational materials based on open standards , as well as the collaboration of educators over the network in the realization of educational activities . The Isabel CSCW application is a group collaboration tool for the Internet supporting audience interconnection over the network , such as distributed classrooms , conferences or meetings . This paper describes the conclusions and feedback obtained from the integration of Isabel into EducaNext , it 's use for the realization of collaborative educational activities involving distributed classrooms , lectures or workshops , as well as the general conclusions obtained about the integration of synchronous collaboration applications into educational mediators ."
"1212","StatSnowball : a statistical approach to extracting entity relationships Traditional relation extraction methods require pre-specified relations and relation-specific human-tagged examples . Bootstrapping systems significantly reduce the number of training examples , but they usually apply heuristic-based methods to combine a set of strict hard rules , which limit the ability to generalize and thus generate a low recall . Furthermore , existing bootstrapping methods do not perform open information extraction ( Open IE ) , which can identify various types of relations without requiring pre-specifications . In this paper , we propose a statistical extraction framework called Statistical Snowball ( StatSnowball ) , which is a bootstrapping system and can perform both traditional relation extraction and Open IE . StatSnowball uses the discriminative Markov logic networks ( MLNs ) and softens hard rules by learning their weights in a maximum likelihood estimate sense . MLN is a general model , and can be configured to perform different levels of relation extraction . In StatSnwoball , pattern selection is performed by solving an l1-norm penalized maximum likelihood estimation , which enjoys well-founded theories and efficient solvers . We extensively evaluate the performance of StatSnowball in different configurations on both a small but fully labeled data set and large-scale Web data . Empirical results show that StatSnowball can achieve a significantly higher recall without sacrificing the high precision during iterations with a small number of seeds , and the joint inference of MLN can improve the performance . Finally , StatSnowball is efficient and we have developed a working entity relation search engine called Renlifang based on it ."
"1213","Modeling anchor text and classifying queries to enhance web document retrieval Several types of queries are widely used on the World Wide Web and the expected retrieval method can vary depending on the query type . We propose a method for classifying queries into informational and navigational types . Because terms in navigational queries often appear in anchor text for links to other pages , we analyze the distribution of query terms in anchor texts on the Web for query classification purposes . While content-based retrieval is effective for informational queries , anchor-based retrieval is effective for navigational queries . Our retrieval system combines the results obtained with the content-based and anchor-based retrieval methods , in which the weight for each retrieval result is determined automatically depending on the result of the query classification . We also propose a method for improving anchor-based retrieval . Our retrieval method , which computes the probability that a document is retrieved in response to the given query , identifies synonyms of query terms in the anchor texts on the Web and uses these synonyms for smoothing purposes in the probability estimation . We use the NTCIR test collections and show the effectiveness of individual methods and the entire Web retrieval system experimentally ."
"1214","Large scale multi-label classification via metalabeler The explosion of online content has made the management of such content non-trivial . Web-related tasks such as web page categorization , news filtering , query categorization , tag recommendation , etc. often involve the construction of multi-label categorization systems on a large scale . Existing multi-label classification methods either do not scale or have unsatisfactory performance . In this work , we propose MetaLabeler to automatically determine the relevant set of labels for each instance without intensive human involvement or expensive cross-validation . Extensive experiments conducted on benchmark data show that the MetaLabeler tends to outperform existing methods . Moreover , MetaLabeler scales to millions of multi-labeled instances and can be deployed easily . This enables us to apply the MetaLabeler to a large scale query categorization problem in Yahoo ! , yielding a significant improvement in performance ."
"1215","General auction mechanism for search advertising In sponsored search , a number of advertising slots is available on a search results page , and have to be allocated among a set of advertisers competing to display an ad on the page . This gives rise to a bipartite matching market that is typically cleared by the way of an automated auction . Several auction mechanisms have been proposed , with variants of the Generalized Second Price ( GSP ) being widely used in practice . There is a rich body of work on bipartite matching markets that builds upon the stable marriage model of Gale and Shapley and the assignment model of Shapley and Shubik . This line of research offers deep insights into the structure of stable outcomes in such markets and their incentive properties . In this paper , we model advertising auctions in terms of an assignment model with linear utilities , extended with bidder and item specific maximum and minimum prices . Auction mechanisms like the commonly used GSP or the well-known Vickrey-Clarke-Groves ( VCG ) can be interpreted as simply computing a bidder-optimal stable matching in this model , for a suitably defined set of bidder preferences , but our model includes much richer bidders and preferences . We prove that in our model the existence of a stable matching is guaranteed , and under a non-degeneracy assumption a bidder-optimal stable matching exists as well . We give an algorithm to find such matching in polynomial time , and use it to design truthful mechanism that generalizes GSP , is truthful for profit-maximizing bidders , correctly implements features like bidder-specific minimum prices and position-specific bids , and works for rich mixtures of bidders and preferences . Our main technical contributions are the existence of bidder-optimal matchings and strategyproofness of the resulting mechanism , and are proved by induction on the progress of the matching algorithm ."
"1216","Performing grouping and aggregate functions in XML queries Since more and more business data are represented in XML format , there is a compelling need of supporting analytical operations in XML queries . Particularly , the latest version of XQuery proposed by W3C , XQuery 1.1 , introduces a new construct to explicitly express grouping operation in FLWOR expression . Existing works in XML query processing mainly focus on physically matching query structure over XML document . Given the explicit grouping operation in a query , how to efficiently compute grouping and aggregate functions over XML document is not well studied yet . In this paper , we extend our previous XML query processing algorithm , VERT , to efficiently perform grouping and aggregate function in queries . The main technique of our approach is introducing relational tables to index values . Query pattern matching and aggregation computing are both conducted with table indices . We also propose two semantic optimizations to further improve the query performance . Finally we present experimental results to validate the efficiency of our approach , over other existing approaches ."
"1217","Dissemination of heterogeneous xml data A lot of recent research has focused on the content-based dissemination of XML data . However , due to the heterogeneous data schemas used by different data publishers even for data in the same domain , an important challenge is how to efficiently and effectively disseminate relevant data to subscribers whose subscriptions might be specified based on schemas that are different from those used by the data publishers . This paper examines the options to resolve this schema heterogeneity problem in XML data dissemination , and proposes a novel paradigm that is based on data rewriting . Our experimental results demonstrate the effectiveness of the data rewriting paradigm and identifies the tradeoffs of the various approaches"
"1218","Feature weighting in content based recommendation system using social network analysis We propose a hybridization of collaborative filtering and content based recommendation system . Attributes used for content based recommendations are assigned weights depending on their importance to users . The weight values are estimated from a set of linear regression equations obtained from a social network graph which captures human judgment about similarity of items ."
"1219","A measurement-driven analysis of information propagation in the flickr social network Online social networking sites like MySpace , Facebook , and Flickr have become a popular way to share and disseminate content . Their massive popularity has led to viral marketing techniques that attempt to spread content , products , and ideas on these sites . However , there is little data publicly available on viral propagation in the real world and few studies have characterized how information spreads over current online social networks . In this paper , we collect and analyze large-scale traces of information dissemination in the Flickr social network . Our analysis , based on crawls of the favorite markings of 2.5 million users on 11 million photos , aims at answering three key questions : ( a ) how widely does information propagate in the social network ? ( b ) how quickly does information propagate ? and ( c ) what is the role of word-of-mouth exchanges between friends in the overall propagation of information in the network ? Contrary to viral marketing `` intuition , '' we find that ( a ) even popular photos do not spread widely throughout the network , ( b ) even popular photos spread slowly through the network , and ( c ) information exchanged between friends is likely to account for over 50 of all favorite-markings , but with a significant delay at each hop ."
"1220","Learning to recognize reliable users and content in social media with coupled mutual reinforcement Community Question Answering ( CQA ) has emerged as a popular forum for users to pose questions for other users to answer . Over the last few years , CQA portals such as Naver and Yahoo ! Answers have exploded in popularity , and now provide a viable alternative to general purpose Web search . At the same time , the answers to past questions submitted in CQA sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering . Unfortunately , the quality of the submitted questions and answers varies widely - increasingly so that a large fraction of the content is not usable for answering queries . Previous approaches for retrieving relevant and high quality content have been proposed , but they require large amounts of manually labeled data -- which limits the applicability of the supervised approaches to new sites and domains . In this paper we address this problem by developing a semi-supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation , that requires relatively few labeled examples to initialize the training process . Results of a large scale evaluation demonstrate that our methods are more effective than previous approaches for finding high-quality answers , questions , and users . More importantly , our quality estimation significantly improves the accuracy of search over CQA archives over the state-of-the-art methods ."
"1221","Recommending questions using the mdl-based tree cut model The paper is concerned with the problem of question recommendation . Specifically , given a question as query , we are to retrieve and rank other questions according to their likelihood of being good recommendations of the queried question . A good recommendation provides alternative aspects around users ' interest . We tackle the problem of question recommendation in two steps : first represent questions as graphs of topic terms , and then rank recommendations on the basis of the graphs . We formalize both steps as the tree-cutting problems and then employ the MDL ( Minimum Description Length ) for selecting the best cuts . Experiments have been conducted with the real questions posted at Yahoo ! Answers . The questions are about two domains , ` travel ' and ` computers & internet ' . Experimental results indicate that the use of the MDL-based tree cut model can significantly outperform the baseline methods of word-based VSM or phrase-based VSM . The results also show that the use of the MDL-based tree cut model is essential to our approach ."
"1222","Anycast-aware transport for content delivery networks Anycast-based content delivery networks ( CDNs ) have many properties that make them ideal for the large scale distribution of content on the Internet . However , because routing changes can result in a change of the endpoint that terminates the TCP session , TCP session disruption remains a concern for anycast CDNs , especially for large file downloads . In this paper we demonstrate that this problem does not require any complex solutions . In particular , we present the design of a simple , yet efficient , mechanism to handle session disruptions due to endpoint changes . With our mechanism , a client can continue the download of the content from the point at which it was before the endpoint change . Furthermore , CDN servers purge the TCP connection state quickly to handle frequent switching with low system overhead . We demonstrate experimentally the effectiveness of our proposed mechanism and show that more complex mechanisms are not required . Specifically , we find that our mechanism maintains high download throughput even with a reasonably high rate of endpoint switching , which is attractive for load balancing scenarios . Moreover , our results show that edge servers can purge TCP connection state after a single timeout-triggered retransmission without any tangible impact on ongoing connections . Besides improving server performance , this behavior improves the resiliency of the CDN to certain denial of service attacks ."
"1223","On deep annotation The success of the Semantic Web crucially depends on the easy creation , integration and use of semantic data . For this purpose , we consider an integration scenario that defies core assumptions of current metadata construction methods . We describe a framework of metadata creation when web pages are generated from a database and the database owner is cooperatively participating in the Semantic Web . This leads us to the definition of ontology mapping rules by manual semantic annotation and the usage of the mapping rules and of web services for semantic queries . In order to create metadata , the framework combines the presentation layer with the data description layer -- in contrast to `` conventional '' annotation , which remains at the presentation layer . Therefore , we refer to the framework as deep annotation 1 . We consider deep annotation as particularly valid because , ( i ) , web pages generated from databases outnumber static web pages , ( ii ) , annotation of web pages may be a very intuitive way to create semantic data from a database and , ( iii ) , data from databases should not be materialized as RDF files , it should remain where it can be handled most efficiently -- in its databases ."
"1224","Improving pseudo-relevance feedback in web information retrieval using web page segmentation In contrast to traditional document retrieval , a web page as a whole is not a good information unit to search because it often contains multiple topics and a lot of irrelevant information from navigation , decoration , and interaction part of the page . In this paper , we propose a VIsion-based Page Segmentation ( VIPS ) algorithm to detect the semantic content structure in a web page . Compared with simple DOM based segmentation method , our page segmentation scheme utilizes useful visual cues to obtain a better partition of a page at the semantic level . By using our VIPS algorithm to assist the selection of query expansion terms in pseudo-relevance feedback in web information retrieval , we achieve 27 % performance improvement on Web Track dataset ."
"1225","SemRank : ranking complex relationship search results on the semantic web While the idea that querying mechanisms for complex relationships ( otherwise known as Semantic Associations ) should be integral to Semantic Web search technologies has recently gained some ground , the issue of how search results will be ranked remains largely unaddressed . Since it is expected that the number of relationships between entities in a knowledge base will be much larger than the number of entities themselves , the likelihood that Semantic Association searches would result in an overwhelming number of results for users is increased , therefore elevating the need for appropriate ranking schemes . Furthermore , it is unlikely that ranking schemes for ranking entities ( documents , resources , etc. ) may be applied to complex structures such as Semantic Associations . In this paper , we present an approach that ranks results based on how predictable a result might be for users . It is based on a relevance model SemRank , which is a rich blend of semantic and information-theoretic techniques with heuristics that supports the novel idea of modulative searches , where users may vary their search modes to effect changes in the ordering of results depending on their need . We also present the infrastructure used in the SSARK system to support the computation of SemRank values for resulting Semantic Associations and their ordering ."
"1226","Predicting clicks : estimating the click-through rate for new ads Search engine advertising has become a significant element of the Web browsing experience . Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad . This ranking has a strong impact on the revenue the search engine receives from the ads . Further , showing the user an ad that they prefer to click on improves user satisfaction . For these reasons , it is important to be able to accurately estimate the click-through rate of ads in the system . For ads that have been displayed repeatedly , this is empirically measurable , but for new ads , other means must be used . We show that we can use features of ads , terms , and advertisers to learn a model that accurately predicts the click-though rate for new ads . We also show that using our model improves the convergence and performance of an advertising system . As a result , our model increases both revenue and user satisfaction ."
"1227","Privacy diffusion on the web : a longitudinal perspective For the last few years we have been studying the diffusion of private information for users as they visit various Web sites triggering data gathering aggregation by third parties . This paper reports on our longitudinal study consisting of multiple snapshots of our examination of such diffusion over four years . We examine the various technical ways by which third-party aggregators acquire data and the depth of user-related information acquired . We study techniques for protecting privacy diffusion as well as limitations of such techniques . We introduce the concept of secondary privacy damage . Our results show increasing aggregation of user-related data by a steadily decreasing number of entities . A handful of companies are able to track users ' movement across almost all of the popular Web sites . Virtually all the protection techniques have significant limitations highlighting the seriousness of the problem and the need for alternate solutions ."
"1228","A multimodal interaction manager for device independent mobile applications This poster presents an overview of the work on an interaction manager of a platform for multimodal applications in 2.5 G and 3G mobile phone networks and WLAN environments . The poster describes the requirements for the interaction manager ( IM ) , its tasks and the resulting structure . We examine the W3C 's definition of an interaction manager and compare it to our implementation , which accomplishes some additional tasks ."
"1229","Characterizing insecure javascript practices on the web JavaScript is an interpreted programming language most often used for enhancing webpage interactivity and functionality . It has powerful capabilities to interact with webpage documents and browser windows , however , it has also opened the door for many browser-based security attacks . Insecure engineering practices of using JavaScript may not directly lead to security breaches , but they can create new attack vectors and greatly increase the risks of browser-based attacks . In this paper , we present the first measurement study on insecure practices of using JavaScript on the Web . Our focus is on the insecure practices of JavaScript inclusion and dynamic generation , and we examine their severity and nature on 6,805 unique websites . Our measurement results reveal that insecure JavaScript practices are common at various websites : ( 1 ) at least 66.4 % of the measured websites manifest the insecure practices of including JavaScript files from external domains into the top-level documents of their webpages ; ( 2 ) over 44.4 % of the measured websites use the dangerous eval ( ) function to dynamically generate and execute JavaScript code on their webpages ; and ( 3 ) in JavaScript dynamic generation , using the document . write ( ) method and the innerHTML property is much more popular than using the relatively secure technique of creating script elements via DOM methods . Our analysis indicates that safe alternatives to these insecure practices exist in common cases and ought to be adopted by website developers and administrators for reducing potential security risks ."
"1230","Personalized web exploration with task models Personalized Web search has emerged as one of the hottest topics for both the Web industry and academic researchers . However , the majority of studies on personalized search focused on a rather simple type of search , which leaves an important research topic - the personalization in exploratory searches - as an under-studied area . In this paper , we present a study of personalization in task-based information exploration using a system called TaskSieve . TaskSieve is a Web search system that utilizes a relevance feedback based profile , called a `` task model '' , for personalization . Its innovations include flexible and user controlled integration of queries and task models , task-infused text snippet generation , and on-screen visualization of task models . Through an empirical study using human subjects conducting task-based exploration searches , we demonstrate that TaskSieve pushes significantly more relevant documents to the top of search result lists as compared to a traditional search system . TaskSieve helps users select significantly more accurate information for their tasks , allows the users to do so with higher productivity , and is viewed more favorably by subjects under several usability related characteristics ."
"1231","GigaHash : scalable minimal perfect hashing for billions of urls A minimal perfect function maps a static set of n keys on to the range of integers -LCB- 0,1,2 , ... , n - 1 -RCB- . We present a scalable high performance algorithm based on random graphs for constructing minimal perfect hash functions ( MPHFs ) . For a set of n keys , our algorithm outputs a description of h in expected time O ( n ) . The evaluation of h ( x ) requires three memory accesses for any key x and the description of h takes up 0.89 n bytes ( 7.13 n bits ) . This is the best ( most space efficient ) known result to date . Using a simple heuristic and Huffman coding , the space requirement is further reduced to 0.79 n bytes ( 6.86 n bits ) . We present a high performance architecture that is easy to parallelize and scales well to very large data sets encountered in internet search applications . Experimental results on a one billion URL dataset obtained from Live Search crawl data , show that the proposed algorithm ( a ) finds an MPHF for one billion URLs in less than 4 minutes , and ( b ) requires only 6.86 bits\/key for the description of h."
"1232","What is Twitter , a social network or a news media ? Twitter , a microblogging service less than three years old , commands more than 41 million users as of July 2009 and is growing fast . Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets . The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing . We have crawled the entire Twitter site and obtained 41.7 million user profiles , 1.47 billion social relations , 4,262 trending topics , and 106 million tweets . In its follower-following topology analysis we have found a non-power-law follower distribution , a short effective diameter , and low reciprocity , which all mark a deviation from known characteristics of human social networks ( 28 ) . In order to identify influentials on Twitter , we have ranked users by the number of followers and by PageRank and found two rankings to be similar . Ranking by retweets differs from the previous two rankings , indicating a gap in influence inferred from the number of followers and that from the popularity of one 's tweets . We have analyzed the tweets of top trending topics and reported on their temporal behavior and user participation . We have classified the trending topics based on the active period and the tweets and show that the majority ( over 85 % ) of topics are headline news or persistent news in nature . A closer look at retweets reveals that any retweeted tweet is to reach an average of 1,000 users no matter what the number of followers is of the original tweet . Once retweeted , a tweet gets retweeted almost instantly on next hops , signifying fast diffusion of information after the 1st retweet . To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it ."
"1233","Predicting positive and negative links in online social networks We study online social networks in which relationships can be either positive ( indicating relations such as friendship ) or negative ( indicating relations such as opposition or antagonism ) . Such a mix of positive and negative links arise in a variety of online settings ; we study datasets from Epinions , Slashdot and Wikipedia . We find that the signs of links in the underlying social networks can be predicted with high accuracy , using models that generalize across this diverse range of sites . These models provide insight into some of the fundamental principles that drive the formation of signed links in networks , shedding light on theories of balance and status from social psychology ; they also suggest social computing applications by which the attitude of one user toward another can be estimated from evidence provided by their relationships with other members of the surrounding social network ."
"1234","Using landing pages for sponsored search ad selection We explore the use of the landing page content in sponsored search ad selection . Specifically , we compare the use of the ad 's intrinsic content to augmenting the ad with the whole , or parts , of the landing page . We explore two types of extractive summarization techniques to select useful regions from the landing pages : out-of-context and in-context methods . Out-of-context methods select salient regions from the landing page by analyzing the content alone , without taking into account the ad associated with the landing page . In-context methods use the ad context ( including its title , creative , and bid phrases ) to help identify regions of the landing page that should be used by the ad selection engine . In addition , we introduce a simple yet effective unsupervised algorithm to enrich the ad context to further improve the ad selection . Experimental evaluation confirms that the use of landing pages can significantly improve the quality of ad selection . We also find that our extractive summarization techniques reduce the size of landing pages substantially , while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page ."
"1235","An axiomatic approach for result diversification Understanding user intent is key to designing an effective ranking system in a search engine . In the absence of any explicit knowledge of user intent , search engines want to diversify results to improve user satisfaction . In such a setting , the probability ranking principle-based approach of presenting the most relevant results on top can be sub-optimal , and hence the search engine would like to trade-off relevance for diversity in the results . In analogy to prior work on ranking and clustering systems , we use the axiomatic approach to characterize and design diversification systems . We develop a set of natural axioms that a diversification system is expected to satisfy , and show that no diversification function can satisfy all the axioms simultaneously . We illustrate the use of the axiomatic framework by providing three example diversification objectives that satisfy different subsets of the axioms . We also uncover a rich link to the facility dispersion problem that results in algorithms for a number of diversification objectives . Finally , we propose an evaluation methodology to characterize the objectives and the underlying axioms . We conduct a large scale evaluation of our objectives based on two data sets : a data set derived from the Wikipedia disambiguation pages and a product database ."
"1236","Search engines and their public interfaces : which apis are the most synchronized ? Researchers of commercial search engines often collect datausing the application programming interface ( API ) or by `` scraping '' results from the web user interface ( WUI ) , butanecdotal evidence suggests the interfaces produce differentresults . We provide the first in-depth quantitative analysisof the results produced by the Google , MSN and Yahoo APIand WUI interfaces . After submitting a variety of queriesto the interfaces for 5 months , we found significant discrepanciesin several categories . Our findings suggest that theAPI indexes are not older , but they are probably smaller for Google and Yahoo . Researchers may use our findings tobetter understand the differences between the interfaces andchoose the best API for their particular types of queries ."
"1237","Using graphics processors for high performance IR query processing Web search engines are facing formidable performance challenges due to data sizes and query loads . The major engines have to process tens of thousands of queries per second over tens of billions of documents . To deal with this heavy workload , such engines employ massively parallel systems consisting of thousands of machines . The significant cost of operating these systems has motivated a lot of recent research into more efficient query processing mechanisms . We investigate a new way to build such high performance IR systems using graphical processing units ( GPUs ) . GPUs were originally designed to accelerate computer graphics applications through massive on-chip parallelism . Recently a number of researchers have studied how to use GPUs for other problem domains such as databases and scientific computing . Our contribution here is to design a basic system architecture for GPU-based high-performance IR , to develop suitable algorithms for subtasks such as inverted list compression , list intersection , and top - $ k $ scoring , and to show how to achieve highly efficient query processing on GPU-based systems . Our experimental results for a prototype GPU-based system on $ 25.2 $ million web pages indicate that significant gains in query processing performance can be obtained ."
"1238","Debugging standard document formats We present a tool for helping XML schema designers to obtain a high quality level for their specifications . The tool allows one to analyze relations between classes of XML documents and formally prove them . For instance , the tool can be used to check forward and backward compatibilities of recommendations . When such a relation does not hold , the tool allows one to identify the reasons and reports detailed counter-examples that exemplify the problem . For this purpose , the tool relies on recent advances in logic-based automated theorem proving techniques that allow for efficient reasoning on very large sets of XML documents . We believe this tool can be of great value for standardization bodies that define specifications using various XML type definition languages ( such as W3C specifications ) , and are concerned with quality assurance for their normative recommendations ."
"1239","2lip : the step towards the web3d The World Wide Web allows users to create and publish a variety of resources , including multimedia ones . Most of the contemporary best practices for designing web interfaces , however , do not take into account the 3D techniques . In this paper we present a novel approach for designing interactive web applications -- 2-Layer Interface Paradigm ( 2LIP ) . The background layer of the 2LIP-type user interface is a 3D scene , which a user can not directly interact with . The foreground layer is HTML content . Only taking an action on this content ( e.g. pressing a hyperlink , scrolling a page ) can affect the 3D scene . We introduce a reference implementation of 2LIP : Copernicus - The Virtual 3D Encyclopedia , which shows one of the potential paths of the evolution of Wikipedia towards Web 3.0 . Based on the evaluation of Copernicus we prove that designing web interfaces according to 2LIP provides users a better browsing experience , without harming the interaction ."
"1240","A contextual-bandit approach to personalized news article recommendation Personalized web services strive to adapt their services ( advertisements , news articles , etc. ) to individual users by making use of both content and user information . Despite a few recent advances , this problem remains challenging for at least two reasons . First , web service is featured with dynamically changing pools of content , rendering traditional collaborative filtering methods inapplicable . Second , the scale of most web services of practical interest calls for solutions that are both fast in learning and computation . In this work , we model personalized recommendation of news articles as a contextual bandit problem , a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles , while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks . The contributions of this work are three-fold . First , we propose a new , general contextual bandit algorithm that is computationally efficient and well motivated from learning theory . Second , we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic . Finally , using this offline evaluation method , we successfully applied our new algorithm to a Yahoo ! Front Page Today Module dataset containing over 33 million events . Results showed a 12.5 % click lift compared to a standard context-free bandit algorithm , and the advantage becomes even greater when data gets more scarce ."
"1241","Personalized recommendation on dynamic content using predictive bilinear models In Web-based services of dynamic content ( such as news articles ) , recommender systems face the difficulty of timely identifying new items of high-quality and providing recommendations for new users . We propose a feature-based machine learning approach to personalized recommendation that is capable of handling the cold-start issue effectively . We maintain profiles of content of interest , in which temporal characteristics of the content , e.g. popularity and freshness , are updated in real-time manner . We also maintain profiles of users including demographic information and a summary of user activities within Yahoo ! properties . Based on all features in user and content profiles , we develop predictive bilinear regression models to provide accurate personalized recommendations of new items for both existing and new users . This approach results in an offline model with light computational overhead compared with other recommender systems that require online re-training . The proposed framework is general and flexible for other personalized tasks . The superior performance of our approach is verified on a large-scale data set collected from the Today-Module on Yahoo ! Front Page , with comparison against six competitive approaches ."
"1242","Limiting the spread of misinformation in social networks In this work , we study the notion of competing campaigns in a social network and address the problem of influence limitation where a `` bad '' campaign starts propagating from a certain node in the network and use the notion of limiting campaigns to counteract the effect of misinformation . The problem can be summarized as identifying a subset of individuals that need to be convinced to adopt the competing ( or `` good '' ) campaign so as to minimize the number of people that adopt the `` bad '' campaign at the end of both propagation processes . We show that this optimization problem is NP-hard and provide approximation guarantees for a greedy solution for various definitions of this problem by proving that they are submodular . We experimentally compare the performance of the greedy method to various heuristics . The experiments reveal that in most cases inexpensive heuristics such as degree centrality compare well with the greedy approach . We also study the influence limitation problem in the presence of missing data where the current states of nodes in the network are only known with a certain probability and show that prediction in this setting is a supermodular problem . We propose a prediction algorithm that is based on generating random spanning trees and evaluate the performance of this approach . The experiments reveal that using the prediction algorithm , we are able to tolerate about 90 % missing data before the performance of the algorithm starts degrading and even with large amounts of missing data the performance degrades only to 75 % of the performance that would be achieved with complete data ."
"1243","Measurement-calibrated graph models for social network experiments Access to realistic , complex graph datasets is critical to research on social networking systems and applications . Simulations on graph data provide critical evaluation of new systems and applications ranging from community detection to spam filtering and social web search . Due to the high time and resource costs of gathering real graph datasets through direct measurements , researchers are anonymizing and sharing a small number of valuable datasets with the community . However , performing experiments using shared real datasets faces three key disadvantages : concerns that graphs can be de-anonymized to reveal private information , increasing costs of distributing large datasets , and that a small number of available social graphs limits the statistical confidence in the results . The use of measurement-calibrated graph models is an attractive alternative to sharing datasets . Researchers can `` fit '' a graph model to a real social graph , extract a set of model parameters , and use them to generate multiple synthetic graphs statistically similar to the original graph . While numerous graph models have been proposed , it is unclear if they can produce synthetic graphs that accurately match the properties of the original graphs . In this paper , we explore the feasibility of measurement-calibrated synthetic graphs using six popular graph models and a variety of real social graphs gathered from the Facebook social network ranging from 30,000 to 3 million edges . We find that two models consistently produce synthetic graphs with common graph metric values similar to those of the original graphs . However , only one produces high fidelity results in our application-level benchmarks . While this shows that graph models can produce realistic synthetic graphs , it also highlights the fact that current graph metrics remain incomplete , and some applications expose graph properties that do not map to existing metrics ."
"1244","Fine-grained privilege separation for web applications We present a programming model for building web applications with security properties that can be confidently verified during a security review . In our model , applications are divided into isolated , privilege-separated components , enabling rich security policies to be enforced in a way that can be checked by reviewers . In our model , the web framework enforces privilege separation and isolation of web applications by requiring the use of an object-capability language and providing interfaces that expose limited , explicitly-specified privileges to application components . This approach restricts what each component of the application can do and quarantines buggy or compromised code . It also provides a way to more safely integrate third-party , less-trusted code into a web application . We have implemented a prototype of this model based upon the Java Servlet framework and used it to build a webmail application . Our experience with this example suggests that the approach is viable and helpful at establishing reviewable application-specific security properties ."
"1245","Enhancing diversity , coverage and balance for summarization through structure learning Document summarization plays an increasingly important role with the exponential growth of documents on the Web . Many supervised and unsupervised approaches have been proposed to generate summaries from documents . However , these approaches seldom simultaneously consider summary diversity , coverage , and balance issues which to a large extent determine the quality of summaries . In this paper , we consider extract-based summarization emphasizing the following three requirements : 1 ) diversity in summarization , which seeks to reduce redundancy among sentences in the summary ; 2 ) sufficient coverage , which focuses on avoiding the loss of the document 's main information when generating the summary ; and 3 ) balance , which demands that different aspects of the document need to have about the same relative importance in the summary . We formulate the extract-based summarization problem as learning a mapping from a set of sentences of a given document to a subset of the sentences that satisfies the above three requirements . The mapping is learned by incorporating several constraints in a structure learning framework , and we explore the graph structure of the output variables and employ structural SVM for solving the resulted optimization problem . Experiments on the DUC2001 data sets demonstrate significant performance improvements in terms of F1 and ROUGE metrics ."
"1246","Web-scale classification with naive bayes Traditional Naive Bayes Classifier performs miserably on web-scale taxonomies . In this paper , we investigate the reasons behind such bad performance . We discover that the low performance are not completely caused by the intrinsic limitations of Naive Bayes , but mainly comes from two largely ignored problems : contradiction pair problem and discriminative evidence cancelation problem . We propose modifications that can alleviate the two problems while preserving the advantages of Naive Bayes . The experimental results show our modified Naive Bayes can significantly improve the performance on real web-scale taxonomies ."
"1247","An economic model of the worldwide web We believe that much novel insight into the worldwide web can be obtained from taking into account the important fact that it is created , used , and run by selfish optimizing agents : users , document authors , and search engines . On-going theoretical and experimental analysis of a simple abstract model of www creation and search based on user utilities illustrates this point : We find that efficiency is higher when the utilities are more clustered , and that power-law statistics of document degrees emerge very naturally in this context . More importantly , our work sets up many more elaborate questions , related , e.g. , to www search algorithms seen as author incentives , to search engine spam , and to search engine quality and competition ."